<!doctype html>
<html>
<head>
<!--     <meta charset="utf-8"> -->
<!-- <title>Redirecting to https://bob.github.io/repo/</title> -->
<!--     <meta http-equiv="refresh" content="0; URL=https://computethresholds.github.io/"> -->
<!--     <link rel="canonical" href="https://computethresholds.github.io/"> -->

  <title>On the Limitations of Compute Thresholds as a Governance Strategy</title> 
  <!-- Twitter Card data -->
  <meta name="twitter:card" value="summary">
  <meta name="twitter:title" content="On the Limitations of Compute Thresholds as a Governance Strategy">
  <!-- <meta name="twitter:description" content="What do pruned deep neural networks forget?"> -->
  <meta name="twitter:url" content="https://computethresholds.github.io/">
  <meta name="twitter:image" content="https://cdn.glitch.com/02868eea-fe84-443e-964a-8f04885fa5fa%2Faccuracy_distribution_updated.png?v=1574118491306">
  <meta name="twitter:site" content="@CohereForAI" />
  
  <meta property="og:image:width" content="1920" />
  <meta property="og:image:height" content="1080" />
  <meta property="og:title" content="On the Limitations of Compute Thresholds as a Governance Strategy" />
  <meta property="og:type" content="article" />
  <!-- <meta property="og:description" content="What do pruned deep neural networks forget?" /> -->
  <meta property="og:image" content="https://cdn.glitch.com/02868eea-fe84-443e-964a-8f04885fa5fa%2Faccuracy_distribution.png?v=1574118354833" />
  <meta property="og:url" content="https://computethresholds.github.io/" />
  <!-- <meta property="og:site_name" content="Deep Neural Network Pruning"> -->
  <meta property="og:locale" content="en_US">
  
  
  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="On the Limitations of Compute Thresholds as a Governance Strategy: Measuring the Disparate Impact of Model Pruning">
  <meta name="citation_fulltext_html_url" content="https://computethresholds.github.io/">
   <!-- Update paper link  -->
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/2407.05694">
  <meta name="citation_fulltext_world_readable" content="">
  <meta name="citation_author" content="Hooker, Sara">
  <meta name="citation_author_institution" content="Cohere For AI">
  <!-- Update publication date -->
  <meta name="citation_publication_date" content="2024/08/13">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152824096-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-152824096-1');
</script>

  <!--  https://schema.org/Article -->
  <meta property="description" itemprop="description" content="On the Limitations of Compute Thresholds as a Governance Strategy.">
  <meta property="article:author" content="Sara Hooker">
  <meta property="article:url" content="https://computethresholds.github.io//" />
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400" rel="stylesheet">
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
  <style>
     body {
      font-family: "Roboto", "Helvetica", sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      font-size: 12px;
    }
    html {
      margin: 0;
      padding: 0;
      height: 100%;
    }
    b {
      color: rgba(0, 0, 0, 0.911);
      font-size: 18px;
      font-weight:bold;
    }
    table td {
      font-size: 12px;
      text-align: center;
      outline: 1px solid white;
      padding: 0;
      margin: 0;
    }
    table.inner td {
      padding: 0;
      margin: 0;
      border: 0;
      width: 25%;
    }
    .footer-row {
      height: 15px;
    }
    table.inner tr {
      border: 0;
    }
    table.inner th {
      padding: 8px;
    }
    table th {
      font-size: 11px;
    }
    table {
      border-collapse: collapse;
      border-spacing: 0;
    }
    thead, tbody { display: block; }
    .rotated {
      transform: rotate(90deg);
      transform-origin: left bottom 0;
      margin-top: -111px;
      font-weight: bold;
      font-size: 1.2em;
      padding: 8px;
    }
    #headers {
      z-index: 1000;
      background-color: white;
      height: 65px;
      vertical-align: middle;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }
    #headers span {
      background-color: white;
      display: inline-block;
      line-height: 65px;
      font-size: 1.2em;
      font-weight: bold;
      text-align: center;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .cover {
      background: #1e283a;
    }
    .cover-container {
      padding-top: 10px;
      padding-bottom: 60px;
    }
    .descriptions_, .description_ {
      padding-top: 20px;
    }
    .cover-container, .descriptions_, .description_ {
      padding-right: 5px;
      padding-left: 5px;
      margin-right: auto;
      margin-left: auto;   
    }
  
    
  
    @media (min-width: 415px) {
      authors .authors-affiliations,
       .base-grid, .imgs-container
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        width: 500px;
      }
      .column_portfolio_  .column_portfolio_final .column_portfolio figcaption, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        padding: 0;
        padding-top: 4px;
        word-wrap: break-word;
        word-break: break-word;
      }
    }
    @media (min-width: 768px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_  .column_portfolio .column_portfoliofinal {
        width: 650px;
      }
    }
    @media (min-width: 992px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal  {
        width: 770px;
      }
    }
    @media (min-width: 1200px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        width: 970px;
      }
    }
    .cover h1 {
      font-family: "Roboto", "Gotham A", "Gotham B";
      letter-spacing: 0.05em;
      font-size: 63px;
      font-weight: 700;
      margin-bottom: 0.5em;
      text-transform: uppercase;
    }
    .cover h3 {
      font-size: 30px;
      letter-spacing: 0.05em;
      font-weight: 500;
    }
    .descriptions_ h3 {
      color: #313b4e;
      opacity: .8;
    }
    
    .descriptions_ p {
      color: #313b4e;
      opacity: .8;
      font-size: 16px;
    }
    .cover {
      color: #ddd;
    }
    
    .authors {
      margin-top: -40px;
      overflow: hidden;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    font-size: 1.5rem;
    line-height: 1.8em;
    padding: 1.5rem 0;
    min-height: 1.8em;
    }
    
    .subtitle {
      margin-top: -20px;
    }
    .icons {
      margin-top: 30px;
      padding-left: 4px;
    }
    .icons a {
      display: inline-block;
      font-size: 16px;
      color: #ccc;
      text-decoration: none;
    }
    .paper-icon {
      display: inline-block;
    }
    .paper-icon a {
      line-height: 35px;
      vertical-align: top;
    }
    .paper-icon:hover a {
      cursor: pointer;
      text-decoration: underline;
    }
    .description_ p {
      width: 100%;
      font-size: 16px;
    }
    .description_ img {
      vertical-align: middle;
      width: 100%;
    }
    .imgs-container {
      display: table-row;
    }
    .img-container {
      color: #62779c;
      text-align: center;
      font-weight: bold;
      font-size: 14px;
      padding-right: 6px;
      display: table-cell;
      width: 33%;
    }
    #headers.fixed-header {
      position: fixed;
      top: 0;
    }
    #table-container.fixed-header {
      margin-top: 106px;
    }
    .image-label {
      font-size: 15px;
      text-align: left;
      padding-bottom: 4px;
      padding-top: 6px;
      padding-left: 2px;
      font-weight: normal;
    }
    .img-times-selector-container {
      margin-left: -80px;
      margin-top: -45px;
      font-size: 18px;
      font-weight: bold;
      text-align: center;
     }
    .img-times-selector {
      width: 175px;
    }
    #table {
      margin-top: 0px;
      width: 100%;
    }
    
* {
  box-sizing: border-box;
}
/* Center website */
.row {
  margin: 8px -16px;
}
/* Add padding BETWEEN each column (if you want) */
.row,
.row > .column_portfolio {
  padding: 3px;
}
/* Create three equal columns that floats next to each other */
.column_portfolio {
  float: left;
  width: 33.33%;
  display: none; /* Hide columns by default */
}
    
.column_portfolio_  .column_portfolio .column_portfoliofinal figcaption {
      padding: 4px 8px;
     word-wrap: break-all;
      word-break: break-all;
  }
    
/* Create three equal columns that floats next to each other */
.column_portfolio_ {
  float: left;
  width: 25.00%;
  display: none; /* Hide columns by default */
}
    
.column_portfoliofinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_portfoliofinalfinal {
  float: left;
  width: 25.00%;
  display: none; /* Hide columns by default */
}
.column_header {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_header_ {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_headerfinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_headerfinalfinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
  
    
.column_two_fig {
  float: left;
  width: 50.00%;
  display: none; /* Hide columns by default */
}
/* Clear floats after rows */
.row:after {
  content: "";
  display: table;
  clear: both;
}
/* Content */
.content {
  background-color: white;
  padding: 10px;
  width: 80%;
  margin-left: auto;
  margin-right: auto;
}
	  
.content_reduced {
  background-color: white;
  padding: 10px;
  width: 70%;
  margin-left: auto;
  margin-right: auto;
}
    
.content_reduced_slightly {
  background-color: white;
  padding: 2px;
  width: 50%;
  margin-left: auto;
  margin-right: auto;
}
    
.content_resized {
  background-color: white;
  padding: 2px;
  width: 50%;
  margin-left: auto;
  margin-right: auto;
}
/* The "show" class is added to the filtered elements */
.show {
  display: block;
}
/* Style the buttons */
.btn {
  border: none;
  border-radius: 4px;
  outline: none;
  padding: 12px 16px;
  font-size: 14px;
  background-color:#599bb3;
  color:#ffffff;
  background:linear-gradient(to bottom, #599bb3 5%, #408c99 100%);
  text-shadow:0px 1px 0px #3d768a;
  margin-right: auto;
  margin-left: auto;  
   margin-bottom:5px;
  cursor:pointer;
}
/* Add a grey background color on mouse-over */
.btn:hover {
  background-color: #ddd;
}
/* Add a dark background color to the active button */
.btn.active_1, .btn.active_2, .btn.active_3, .btn.active_4, .btn:target
{ background:linear-gradient(to right, #666 3%, #666  100%);
  color: white;
  cursor:none;
}
    
figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 14px;
   font-weight: bold;
  line-height: 1.5em;
}
    
figcaption a {
  color: rgba(0, 0, 0, 0.6);
}
/* figcaption b,
figcaption .strong_, {
  font-weight: bold;
  font-size: 14px;
  color: #180A3E;
} */
    
</style>
</head>
<body>
  <div id="scroll-container">
    <div class="cover">
      <div class="cover-container">
        <div class="icons">
          <div class="paper-icon">
            <!-- Update Paper Link -->
            <a href="https://arxiv.org/abs/2407.05694">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fpaper_icon.png?v=1572561063939" style="width: 100px"/><br>Paper
            </a>
          </div>
          <!-- <div class="paper-icon" style="margin-left: 20px">
            <a href="https://github.com/google-research/google-research/tree/master/pruning_identified_exemplars">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fcode_icon.png?v=1572562103868" style="width: 100px"/><br>Code
            </a>
          </div>     -->
        </div>
        <div class="title"><h2>On the Limitations of Compute Thresholds as a Governance Strategy</h2></div>
        <div class="authors">Sara Hooker</div>
      <div class="institutions"></div>
       </div>
    </div>
      <div class="descriptions_">
	<h3>The Uncertain Relationship Between Compute and Risk</h3>
        </div>

         <div class="description_">
          
          <p> <q>Well Babbage what are you dreaming about?</q> to which I replied, <q>I am thinking that all these tables might be calculated by machinery.</q> <cite><i>Charles Babbage</i></cite> </p>

          <p> Many inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube 
            was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, 
            noticed that a chocolate bar melted in his pocket whenever he was close to a radar set. This innocuous discovery 
            resulted in the patent for the first microwave <dt-cite key="inbook"></dt-cite>. In a similar vein, deep neural networks only began 
            to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally 
            introduced in the 1970s as a specialized accelerator for video games and for developing graphics for movies and 
            animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use 
            case – to train deep neural networks <dt-cite key="Chellapilla2006,hooker2021,OH20041311kyoung,Payne2005"></dt-cite>. 
            GPUs had one critical advantage over CPUs - they were far better at parallelizing matrix 
            multiples <dt-cite key="BRODTKORB20134,DettmersGPU"></dt-cite>, a mathemetical operation which dominates the definition of deep 
            neural network layers <dt-cite key="fawzi2022discovering,davies2024"></dt-cite>. This higher number of floating operation points 
            per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of 
            deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever 
            deeper networks in 2011 <dt-cite key="inproceedings2011"></dt-cite>, 2012 <dt-cite key="Krizhevsky2012"></dt-cite> and 
            2015 <dt-cite key="szegedy2014going"></dt-cite>. A striking example of this jump in compute is a comparison of the now famous 
            2012 Google paper which used 16,000 CPU cores to classify cats <dt-cite key="le2012building"></dt-cite> to a paper published a 
            mere year later that solved the same task with only two CPU cores and four GPUs <dt-cite key="coates13"></dt-cite>. </p>
            
           <p> This would ignite a rush for compute which has led to a bigger-is-better race in the number of model parameters 
            over the last decade <dt-cite key="2016Canziani,strubell2019energy,rae2021scaling,raffel2020exploring,bommasani2021opportunities,bender_gebru_2021"></dt-cite>. 
            The computer scientist Ken Thompson famously said <q>When in doubt, use brute force.</q>  
            This was formalized as the “bitter lesson” by Rich Sutton who posited that computer science history tells us that 
            throwing more compute at a problem has consistently outperformed all attempts to leverage human knowledge of a domain 
            to teach a model <dt-cite key="SilverBittrLesson"></dt-cite>. In a punch to the ego of every computer scientist out there, what Sutton is 
            saying is that symbolic methods that codify human knowledge have not worked as well as letting a model learn patterns 
            for itself coupled with ever-vaster amounts of compute.  </p>
	   
             <p> <b>Is Sutton right?</b> Certainly, he is correct that scaling has been a widely favored formula because 
                it has provided persuasive gains in overall performance – size is the most de-risked tool we have to unlock 
                new gains. As the computer scientist Michael Jordan quipped <q>Today we can’t think without holding a 
                    piece of metal.</q> Increasing compute also conveniently fits into the cadence of quarterly industry 
                    planning, it is less risky to propose training a bigger model than it is to propose an alternative 
                    optimization technique. However, relying on compute alone misses a critical shift that is underway in the 
                    relationship between compute and performance. It is not always the case that bigger models result in better 
                    performance. The bitter lesson doesn't explain why Falcon 180B <dt-cite key="almazrouei2023falconseriesopenlanguage"></dt-cite> is 
                    easily outperformed by far smaller open weights models such as Llama-3 8B <dt-cite key="llama3modelcard"></dt-cite>, 
                    Command R 35B <dt-cite key="cohere_c4ai_command_r_plus"></dt-cite>, Gemma 27B <dt-cite key="gemma_2024"></dt-cite>. It also doesn't explain why 
                    Aya 23 8B <dt-cite key="aryabumi2024aya"></dt-cite> easily outperforms BLOOM 176 B <dt-cite key="workshop2023bloom176bparameteropenaccessmultilingual"></dt-cite> 
                    despite having only 4.5% of the parameters.   </p>

             <!-- Add Figure 3 from Paper here -->

                 <!-- <div class="content_reduced_slightly">
     <img src="https://cdn.glitch.com/f1ebd1ee-d1ac-4538-8ad5-0034e332e4ae%2Fsynaptic_pruning_image.png?v=1574277111414" alt="abstract_1" style="width:100%">
      <div class="figcaption">
         <strong_>Synaptic pruning removes redundant neurons and strengthens connections that are most useful for the environment. (Figure courtesy of Seeman, 1999)</strong_><br>
       </div>
             </div>   -->
    
		 <p> These are not isolated examples, but rather indicative of an overall trend where there is no guarantee 
            larger models consistently outperform smaller models. Figure \ref{fig:above_13_b} plots the scores of models 
            submitted to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>
            over the last two years. Here, we plot <i>large models</i> with more than 13 billion parameters whose leaderboard 
            score is less than the top performing <i>small model</i> with less than 13 billion parameters. 
            We observe that over time, more and more large models have been submitted that are outperformed by the best 
            small model daily submission. To understand why this is the case, we must understand what key variables have been 
            driving gains in performance over the last decade. In an era where there are diminishing returns for the amount 
            of compute available <dt-cite key="lohn2022ai,2020Thompson"></dt-cite>, optimization and architecture breakthroughs define the rate 
            of return for a given unit of compute. <b>It is this rate of return which is most critical to the pace of
                progress and to the level of risk incurred by additional compute</b>.</p>

        </div>

        <div class="description_">
            <h4> A shift in the relationship between compute and performance </h4>

            <p> <q>The world has changed less since Jesus Christ than it has in the last 30 years.</q> <cite><i>Charles Peguy, 1913</i></cite> </p>

            <p> In complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. 
                Throughout the 20th century doctors recommended removing tonsils in response to any swelling or infection, 
                but research has recently shown the removal may lead to higher incidence of throat cancer <dt-cite key="liang2023"></dt-cite>. 
                Early televised drug prevention advertisements in the 2000s led to increased drug use <dt-cite key="Terry-McElrath2011"></dt-cite>. 
                In a similar vein, the belief that more compute equates with more risk belies a far more complex picture that 
                requires re-examining the relationship between performance and compute. A key limitation of simply throwing 
                more scale at a task is that the relationship between additional compute and generalization remains poorly 
                understood. A growing body of research suggests that the relationship between compute and performance is far more 
                complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier. </p>

            <p> <b>Data quality reduces reliance on compute.</b> Models trained on better data do not require as much compute. 
                A large body of work has emerged which shows that efforts to better curate training corpus, including 
                de-duping <dt-cite key="taylor2022galactica, kocetkov2022stack"></dt-cite>, data pruning <dt-cite key="marion2023more,ayadata2024,sorscher2023neural,albalak2024survey,tirumala2023d4,chimoto2024critical"></dt-cite> 
                or data prioritization <dt-cite key="boubdir2023prompts,thakkar2023selfinfluence"></dt-cite> can compensate for more weights. 
                This suggests that the number of learnable parameters is not definitively the constraint on improving performance; 
                investments in better data quality mitigate the need for more weights <dt-cite key="ayadata2024,penedo2023refinedweb,raffel2020exploring,lee2022deduplicating"></dt-cite>. 
                If the size of a training dataset can be reduced without impacting performance <dt-cite key="marion2023more"></dt-cite>, 
                training time is reduced. This directly impacts the number of training FLOP and means less compute is needed. </p>

            <p> <b>Optimization breakthroughs compensate for compute.</b> Progress over the last few years has been as 
                much due to optimization improvements as it has been due to compute. This includes extending pre-training 
                with instruction finetuning to teach models instruction following <dt-cite key="singh2024aya"></dt-cite>, model distillation 
                using synthetic data from larger more performant "teachers" to train highly capable, smaller 
                "students" <dt-cite key="gemmateam2024gemma,aryabumi2024aya"></dt-cite>, chain-of-thought reasoning <dt-cite key="wei2023chainofthought,hsieh2023distilling"></dt-cite>, 
                increased context-length <dt-cite key="xiong2023effective"></dt-cite>, enabled tool-use <dt-cite key="qin2023toolllm,wang2023voyager"></dt-cite>, 
                retrieval augmented generation <dt-cite key="pozzobon2023goodtriever,NEURIPS2020_6b493230"></dt-cite>, and preference training to align 
                models with human feedback <dt-cite key="dang2024rlhfspeaklanguagesunlocking,ahmadian2024basics,ouyang2022LLMRLHF,bai2022constitutional,lee2023rlaif,tunstall2023zephyr,khalifa2021distributional,rafailov2023DPO,azar2023IPO"></dt-cite>. 
                All these techniques compensate for the need for weights or expensive prolonged training <dt-cite key="ho2024algorithmicprogresslanguagemodels"></dt-cite>. 
                All things equal, these have been shown to dramatically improve model performance relative to a model trained 
                without these optimization tricks given the same level of compute <dt-cite key="davidson2023ai,hernandez2020,erdil2023algorithmic,METR_undated,liu2024sophia"></dt-cite>. 
                In Figure \ref{fig:13b_models}, we plot the best daily 13B or smaller model submitted to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a> over time. 
                In a mere span of 2 years, the best-performing daily scores from small model went from an average of 38.59% across to an average of 77.15% across 2024 submissions.
                The takeaway is clear -- smaller models with the same amount of capacity are becoming more and more performant. </p>

            <p> <b>Architecture plays a significant role in determining scalability</b> The introduction of a new architecture
                design can fundamentally change the relationship between compute and 
                performance <dt-cite key="tay2022scaling,Sevilla_2022,ho2024algorithmic"></dt-cite> and render any compute threshold that is 
                set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of 
                architectures like convolutional neural networks (CNNs) for vision <dt-cite key="inproceedings2011,Krizhevsky2012,szegedy2014going"></dt-cite> and 
                Transformers for language modeling <dt-cite key="vaswani2023attention"></dt-cite>. </p>

            <p> While deep neural networks represent a huge step forward in performance for a given level of compute, what is 
                often missed is that our architectures also represent the ceiling in what is achievable through scaling. 
                While progress has revolved around deep neural networks for the last decade, there is much to suggest that the 
                next significant gain in efficiency will require an entirely different architecture. Deep neural networks remain 
                very inefficient as an algorithm. Our typical training regimes require that all examples are shown the same 
                number of times during the training <dt-cite key="xue2023adaptive"></dt-cite>. All modern networks are trained based upon 
                minimization of average error <dt-cite key="Goodfellow-et-al-2016"></dt-cite>. This means that learning rare artifacts requires 
                far more training time or capacity due to the diluted signal of infrequent attributes relative to the most 
                frequent patterns in the dataset <dt-cite key="Achille2017CriticalLP, jiang2020exploring, Mangalam2019DoDN, 2020fartash,frankle2020,pmlr-v70-arpit17a"></dt-cite>. 
                Small models are already good at learning the most frequent features, and most easy features and common patterns are 
                learned early on training with much harder rare features learned in later stages <dt-cite key="agarwal2020estimating,paul2021deep,Mangalam2019DoDN,siddiqui2022metadata,abbe2021staircasepropertyhierarchicalstructure"></dt-cite>. 
                When we radically scale the size of a model, we show the most gains in performance on are rare and underrepresented 
                attributes in the dataset -- the long-tail <dt-cite key="hooker2019compressed,hooker2020characterising"></dt-cite>. 
                Put differently, scaling is being used to inefficiently learn a very small fraction of the overall training 
                dataset. Our reliance on global updates also results in catastrophic forgetting, where performance deteriorates 
                on the original task because the new information interferes with previously learned behavior <dt-cite key="Mcclelland1995,pozzobon2023goodtriever"></dt-cite>. 
                All this suggests that our current architecture choices are probably not final and key disruptions lie ahead. 
                This is likely to radically change any scaling relationships, in the same way it has done in the last decade. 
                For example, it is unlikely any prediction of how compute scales based upon architectures before deep neural networks holds 
                true post-2012 after the introduction of convolutional neural networks.</p>
  </div>
           
    <!-- Uncomment below to create image container with key findings -->

           <!-- <div class="imgs-container">
        
            <div class="descriptions_">
              <p> The primary findings of our work can be summarized as follows: </p>

          <div class="img-container">1. Pruning would be better described as "selective brain damage." Pruning has a non-uniform impact across classes; a fraction of classes are disproportionately and systematically impacted by the introduction of sparsity.</div>
           <div class="img-container">2. The examples most impacted by pruning, which we term <i>Pruning Identified Exemplars</i> (PIEs), are more challenging for both pruned and non-pruned models to classify.</div>
          <div class="img-container">3. Pruning significantly reduces robustness to image corruptions and natural adversarial images.</div>
      </div>
     </div> -->
   
   <div class="descriptions_">
  <h3>Avoiding a FLOP FLOP</h3>
     </div>

<div class="description_">
  <p> <q>Any statistical relationship will break down when used for policy purposes.</q> <cite><i>Jon Danielsson</i></cite> </p>

   <p> <i>Are FLOP a reliable proxy for overall compute?</i> Even if the relationship between compute and generalization 
    were stable – there are difficulties operationalizing FLOP as a metric.  FLOP <dt-cite key="Goldberg1991"></dt-cite> refers 
    to <i>floating-point operations</i>, and has a fairly straightforward definition: sum up all the math operations in 
    floating point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers 
    were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields 
    that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. 
    This is because all these areas are dominated by simple primitive mathematical operations – for example, FLOP tend to be 
    closely associated with the size of models because deep neural network layers are dominated by a single 
    operation -- matrix multiplies -- which can be decomposed into a set of floating point operations <dt-cite key="fawzi2022discovering,davies2024"></dt-cite>. </p>
   
    <p> <b>We first begin by noting there are some reasons FLOP are attractive as a policy measure.</b> The primary one is 
        that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts 
        don’t change across hardware – the number of mathematical operations is the same no matter what hardware you train a 
        model on. In a world where hardware is increasingly heterogeneous <dt-cite key="hooker2021"></dt-cite> and it is hard to replicate the 
        exact training setting due to a lack of software portability <dt-cite key="NEURIPS2023_42c40aff"></dt-cite>, it is attractive to use a 
        metric that doesn’t depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could 
        occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of 
        new hardware generations <dt-cite key="epoch2023trendsinmachinelearninghardware"></dt-cite>, as well as engineering investments in 
        training infrastructure <dt-cite key="yoo2022scalable,lepikhin2020gshard"></dt-cite>, mean that over time much larger models will be 
        trained using the same number of devices. FLOP is also a metric which could potentially be inferred by cloud providers. 
        Given most machine learning workloads are run by a few key cloud providers, this may make administering such a measure 
        effectively easier <dt-cite key="heim2024governing"></dt-cite>.  </p>

    <p> A key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but 
        FLOP doesn’t say anything about end performance of a model --- only about the number of operations applied to the data. 
        For example, if you compare two models trained for the same number of FLOP but one has had safety alignment during 
        post-training <dt-cite key="aakanksha2024multilingualalignmentprismaligning,bai2022constitutional"></dt-cite> and the other has 
        none – these two models will still be accorded the same level of risk according to number of FLOP but one will present 
        a far lower risk to society because of safety alignment. </p>

    <p> Another key hurdle governance which adopts compute threshold will have to overcome is the lack of clear guidance 
        in all the policy to-date about how FLOP will actually be measured in practice. This ambiguity risks FLOP as a 
        metric being irrelevant or at the very least easy to manipulate. Developing principled standards for measuring any 
        metric of interest is essential for ensuring that safety measures are applied in a proportionate and appropriate way. 
        In the followings Section, we specify some of the key ways in which it is easy to manipulate FLOP if it is left 
        underspecified as a metric.  </p>
</div>

<div class="description_">
    <h4> Challenges of using FLOP as a metric </h4>

    <p> <q>If you cannot measure it, you cannot improve it.</q> <cite><i>Lord Kelvin</i></cite> </p>

    <p> <b>Training FLOP doesn't account for post-training leaps in performance</b> Applying scrutiny and regulation based 
        upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. 
        This can be grouped under <q>inference-time compute</q> and can result in large performance gains that dramatically 
        increase the risk profile of a model.  The limited work to-date which has evaluated a subset 
        of <q>inference-time compute</q> improvements estimates these can impart gains between 5x and 20x of base level
        post-training performance <dt-cite key="davidson2023ai"></dt-cite>.<q>inference-time compute</q> includes best-of-n sampling 
        techniques <dt-cite key="geminiteam2024gemini"></dt-cite>, chain-of-thought reasoning <dt-cite key="wei2023chainofthought,hsieh2023distilling,wang2023selfconsistency"></dt-cite> 
        and model distillation using synthetic data  <dt-cite key="aryabumi2024aya,shimabucoro2024llmseellmdo,ustun2024aya, geminiteam2024gemini"></dt-cite>. 
        All these techniques require more compute at test-time because of the need to perform more forward passes of the 
        model to generate additional samples. However, these are not reflected in training time costs and indeed 
        can often <i>reduce</i> the compute needed during training. For example, smaller, more performant models are often 
        trained on smaller amounts of synthetic data from a highly performant teacher <dt-cite key="epoch2023tradingoffcomputeintrainingandinference,huang2022large"></dt-cite>. 
        These improvements dramatically improve performance but are currently completely ignored by compute thresholds 
        since they don't contribute to <i>training</i> FLOP. </p>

    <p> Increasing the context-length <dt-cite key="xiong2023effective"></dt-cite> and retrieval augmented 
        systems <dt-cite key="lee2024longcontext,pozzobon2023goodtriever,NEURIPS2020_6b493230"></dt-cite> are additional examples of 
        introducing additional computational overhead at test-time by increasing the number of tokens to process. 
        Retrieval augmented models (RAG) have become a mainstay of state-of-art models yet are often introduced after training. 
        Most RAG systems are critical for keeping models up-to-date with knowledge yet contribute minimal or no FLOP. 
        Retrieval augmented models are particularly good at supplementing models with search capabilities or external 
        knowledge, which can enhances risks which depend on up-to-date knowledge such as biorisk and cybersecurity threats. </p>

    <p> Additionally increasing the context length often requires minimal FLOP but can dramatically increase performance 
        of a model. Entire books can be passed in at test time dramatically improving model performance on specialized 
        tasks (Gemini has 2M context window) <dt-cite key="xiong2023effective"></dt-cite>. This can make the number of FLOP irrelevant if 
        sensitive biological data can be passed at inference time in a long-context window. </p>

    <p> <b>Difficulty Tracking FLOP across model lifecycle.</b> Increasingly, training a model falls into distinct stages 
        that all confer different properties. For example, unsupervised pre-training dominates compute costs because the 
        volume of data is typically in the trillions of tokens <dt-cite key="epoch2023trendsinthedollartrainingcostofmachinelearningsystems,heim2023palm"></dt-cite>. 
        Following this, there is instruction finetuning, which confers the model the ability to follow 
        instructions <dt-cite key="ayadata2024"></dt-cite> and then preference training <dt-cite key="aakanksha2024multilingualalignmentprismaligning,ahmadian2024basics,bai2022constitutional,ouyang2022LLMRLHF,lee2023rlaif,tunstall2023zephyr,khalifa2021distributional,rafailov2023DPO,azar2023IPO"></dt-cite>, 
        which aligns model performance with human values. Between each of these steps models are often released 
        publicly <dt-cite key="ustun2024aya,touvron2023llama,aryabumi2024aya"></dt-cite>, meaning that developers can take a model from a 
        different developer and continue optimizing. The models with the most downloads on platforms like HuggingFace are 
        base models which are most conducive for continued pre-training. As sharing of models at different stages of the 
        life-cycle becomes more common, so will difficulties in tallying FLOP across the entire model life-cycle. 
        Furthermore, it may simply be infeasible to trace federated, decentralized training of models where hardware often 
        belongs to many different participants and training is conducted in a privacy-preserving manner <dt-cite key="donyehiya2023cold,borzunov2023petals,yuan2023decentralizedtrainingfoundationmodels,qin2024federatedfullparametertuningbillionsized"></dt-cite>. </p>

    <p> <b>How to handle Mixture of Experts (MoEs) and classic ensembling?</b> 
        MoEs <dt-cite key="zadouri2023pushing,shazeer2018meshtensorflow,riquelme2021scaling,du2022glam,fedus2022switch,tan2024scattered"></dt-cite> 
        are examples of adaptive compute -- where examples are routed to different parts of a model. This type of architecture 
        can often provide powerful efficiency gains, as despite a much larger overall architecture, only a subset of weights 
        are activated for a given example. Current policy frameworks do clearly not specify how to handle Mixture of 
        Experts (MoEs), which constitute some of the most highly performant systems currently deployed, such as 
        Mixtral <dt-cite key="jiang2024mixtral"></dt-cite> and the Gemini family of models <dt-cite key="geminiteam2024gemini"></dt-cite>. However, this raises 
        important questions – should the compute for each expert be counted towards total FLOP, or only the FLOP used to train 
        the subset of experts that are active at inference time? Given final performance depends on all experts in an 
        MoE, a recommendation should be to include all FLOP in the final consideration, but this is currently under-specified. 
        It also raises the question of how to treat new \emph{hybrid techniques} which train several specialized experts and then 
        both average parameters and utilize routing <dt-cite key="sukhbaatar2024branchtrainmix"></dt-cite>. </p>

    <p>Classical <i>simple ensembling techniques</i> dominate production systems in the real 
        world <dt-cite key="ko2023fairensemble,li2024agents"></dt-cite> and have been shown to heavily outperform a single model. 
        Unlike MoEs which are jointly optimized or trained using a router, classic ensembles are often only combined at 
        inference time using simple averaging of weights. Given the ensemble is never trained together, it is unclear whether 
        FLOP should reflect the compute of the single final model or the sum of all the training compute across models that
         were averaged. If it only reflects the FLOP of the final model, this may underestimate risk given ensembling is known 
         to improve performance. </p>

    <p> <b>FLOP only accounts for a single model, but does not capture risk of the overall system.</b>  
        The emphasis on compute thresholds as an indicator of risk also implies that risk is the property of a single model 
        rather than the system in which it is deployed. In the real-world, impact and risk are rarely attributable to a 
        single model but are a facet of the entire system a model sits in and the way it interacts with its 
        environment <dt-cite key="compound-ai-blog,NIPS2015_86df7dcf,jatho2023concretesafetymlproblems,raji2020closingaiaccountabilitygap"></dt-cite>. 
        Many real-world production systems are made up of cascading models where the final output is produced as a results of 
        inputs being processed by multiple algorithms in sequence <dt-cite key="paleyes2022,FrontierModelForum,NIPS2015_86df7dcf,shankar2022operationalizing"></dt-cite>. 
        There has yet to be guidance on whether the FLOP threshold is specific to a single model or whether all models that 
        constitute an end-to-end system contribute to the final tally. This has significant implications for model 
        providers – a cascade system is often made up of models which are not individually very powerful or risky – yet the 
        overall system may exceed the FLOP threshold.  </p>

    <p> There is also no specification as to how to treat model agents which may interact with both each other and/or use tools. 
        End performance of the agents is undoubtedly due to the interactions with other agents and access to 
        tools <dt-cite key="li2024agents"></dt-cite>, yet is unlikely to be considered a single model. It has already been shown that models 
        which are enabled with tool use, or can interact with a wider environment outperform a single model on its 
        own <dt-cite key="wang2023voyageropenendedembodiedagent,anwar2024foundationalchallengesassuringalignment,mialon2023augmentedlanguagemodelssurvey"></dt-cite>. 
        These are far from edge cases; the reality is that most technology deployed in the wild is rarely just an algorithm is 
        isolation. Typically, interdependent models feed into a user experience and interact with a set of choices about design and 
        delivery that impact the overall level of risk.  </p>

    <p> <b>FLOP varies dramatically for different modalities.</b> In Figure \ref{fig:different_modalities}, we plot the 
        FLOP requirements over time of models grouped according to modality and downstream use 
        case (model FLOP data from \citet{epoch2023pcdtrends}). It is easy to observe that the compute requirements have not 
        increased at the same rate across modalities. For example, code models typically require less 
        compute <dt-cite key="lin2024scaling"></dt-cite>, as do biological models <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>.   
        Multilingual models <dt-cite key="ustun2024aya,aryabumi2024aya"></dt-cite> tend to require more compute for each additional 
        language covered. This is often referred to as the <i>curse of multilinguality</i> <dt-cite key="ustun2024aya,arivazhagan2019massively,conneau2019unsupervised,pfeiffer2022lifting"></dt-cite>, 
        where capacity is split between more languages such that performance on any given language suffers relative to a 
        monolingual (single language) model of the same size. These differing compute needs mean that a single threshold may 
        penalize some types of models and reward others. For example, thresholds may penalize multilingual models that attempt 
        to serve many languages and improve access to technology <dt-cite key="ustun2024aya,aryabumi2024aya"></dt-cite>.</p>

    <p> One way to address differences in modalities is to maintain different compute thresholds for each modality. 
        While at first glance this is an attractive solution, it also imposes more technical overhead on governments who 
        must correctly set a hard-coded benchmark for each modality. For example, it is interesting to note that the 
        US Executive Order already has at least one modality-specific caveat to the compute thresholds by carving out a 
        separate compute threshold for biological models. It is set lower for models trained for biological sequence data 
        at 10<sup>23</sup>. However, since the threshold was set, models like xTrimoPGLM <dt-cite key="chen2024xtrimopglm"></dt-cite> already exceed 
        the biological threshold set at 10<sup>23</sup> operations by a factor of 6x <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        Many models <dt-cite key="lin2023,elnaggar2020,Dalla-Torre2023.01.11.523679"></dt-cite> are currently within a factor of 10x the 
        Executive Order’s reporting threshold <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        These models do not appear to present a decidedly different risk profile from previous generations, so if the goal 
        of the thresholds is to be an inflection point for amplified risk it is unclear if it has been set successfully. </p>

    <p> Specifying separate thresholds for different modalities also risks inviting gamification. For example, to 
        avoid a lower threshold for scrutiny for biological models one loophole is to preserve biology specific training 
        data at less than 50%. According to current guidance the model would no-longer qualify as a <q>biological</q> model and 
        would only be subject to the higher general purpose compute thresholds. Galactica-120B <dt-cite key="taylor2022galactica"></dt-cite> and 
        Llama-molinst-protein-7b <dt-cite key="fang2024domainagnostic"></dt-cite> are both examples of models with capabilities for biological 
        sequence modeling without primarily being trained on biological sequence data. Despite both presenting biological 
        capabilities, neither is likely to be considered  <q>biological</q> under the current Executive Order requirements <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        This highlights the fundamental tension of relying on compute alone -- since it is not anchored to the risk metric that is 
        of primary concern, it may be possible to sidestep in many creative ways while still presenting high-risk capabilities.</p>

    <p> In Appendix \ref{sect:technical_details_FLOP}, we also present some more technical aspects of the difficulty of 
        measuring FLOP in practice, such as the difference between theoretical and hardware FLOP, and how to handle difference 
        in quantization. Developing principled standards for measuring FLOP is essential for ensuring that safety measures are 
        applied in a proportionate and appropriate way. </p>

</div>

<div class="descriptions_">  
<h3>We are not very good at predicting the relationship between compute and risk.</h3>
</div>

<div class="description_">  
  
      <p> <q>In theory, there is no difference between theory and practice. But, in practice, there is.</q> <cite><i>Walter J. Savitch</i></cite> </p>

      <p> The choice of where compute thresholds are set will have far-ranging implications – too low and too 
        many models will be selected for additional auditing and benchmarking each year. In contrast, if it is 
        set too high, not enough models will be audited for risk, and the threshold risks become decorative 
        rather than a meaningful indicator of risk. None of the policies to date have provided justification about 
        where they have set their thresholds, or why it excludes almost all models deployed in the wild today. 
        In Section \ref{sect:tradeoff_compute_performance}, we grappled with the changing overall relationship 
        between compute and performance. However, scientific justification for a threshold requires predicting 
        how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold 
        reflects scientific consensus as to when particular risk factors are expected to emerge due to scale. 
        Hence, it is worth considering our success to date in estimating how different model properties change 
        with scale.  </p>
      
      
      <p> Warren Buffet once said <i><q>Don’t ask the barber if you need a haircut.</q></i> In the same 
        vein, don’t ask a computer scientist or economist whether you can predict the future. The temptation to 
        say yes often overrides a necessary humility about what can and cannot be predicted accurately. One such 
        area where hubris has overridden common sense is attempts to predict the relationship between scale and 
        performance in the form of <i>scaling laws</i> <dt-cite key="kaplan2020scaling,hernandez2021scaling,Dhariwal2021DataAP"></dt-cite>
        which either try and predict how a model's pre-training loss scales <dt-cite key="bowman2023things"></dt-cite> 
        or how downstream properties emerge with scale. It is the latter task which is urgently needed by policymakers 
        in order to anticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) 
        at inflection points where risk increases with scale <dt-cite key="anthropic_responsible_scaling,openai_global_affairs, kaminski_regulating_2023"></dt-cite>.  </p>
      
      <p> One of the biggest limitations of scaling laws is that they have only been shown to hold when predicting 
        a model’s pre-training test loss <dt-cite key="bowman2023things"></dt-cite>, which measures the model’s 
        ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual 
        performance on downstream tasks is used, the results are often murky or inconsistent <dt-cite key="Ganguli_2022,schaeffer2023emergent,anwar2024foundational,Ganguli_2022,schaeffer2024predictingdownstreamcapabilitiesfrontier,hu2024predictingemergentabilitiesinfinite"></dt-cite>. 
        Indeed, the term <i>emerging properties</i> is often used to describe this 
        discrepancy <dt-cite key="Wei2022,srivastava2023imitation"></dt-cite>: a property that appears “suddenly” as the 
        complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws 
        don't hold when you try to predict downstream performance instead of predicting test loss for the next 
        word token. </p>      
      
      <p> Even when limited to predicting test loss, there have been issues with replicability of scaling results 
        under slightly different assumptions about the distribution <dt-cite key="besiroglu2024chinchilla,anwar2024foundationalchallengesassuringalignment"></dt-cite>. 
        Research has also increasingly found that many downstream capabilities display irregular scaling 
        curves <dt-cite key="srivastava2023imitation"></dt-cite> or non power-law scaling <dt-cite key="caballero2023broken"></dt-cite>. 
        For complex systems that require projecting into the future, small errors end up accumulating due to time step 
        dependencies being modelled. This makes accurate predictions of when risks will emerge inherently hard, which 
        is compounded by the small samples sizes often available for analysis. each data point is a model, and 
        computation cost means scaling <q>laws</q> are frequently based upon analysis of less than 100 data
        points <dt-cite key="ruan2024observationalscalinglawspredictability"></dt-cite>. This means many 
        reported power law relationships can lack statistical support and power <dt-cite key="powerlawtruths"></dt-cite>.</p>
      
      <p> One immediate recommendation is that the accuracy of scaling laws and predictions of emerging risk can 
        be greatly improved by more guidance from policymakers about what range is of interest and specifying the 
        risks that policymakers are concerned about  <dt-cite key="powerlawtruths"></dt-cite>. For example, 
        there is a big difference between using scaling laws to optimize for the correct amount of training data 
        in your next large-scale run versus attempting to extrapolate trends several orders of magnitude out. 
        Typically, policy use cases demand high precision over a longer time horizon, which is exactly the type of 
        extrapolation we are currently worst at. Specifying which risks are of interest will also benefit 
        precision; scaling laws tend to have high variance in precision between tasks. For example, code-generation 
        has shown fairly predictable power law scaling across 10 orders of magnitude of compute <dt-cite key="hu2024predictingemergentabilitiesinfinite,anwar2024foundational"></dt-cite>. 
        However, other capabilities have been far shown to scale far more erratically <dt-cite key="srivastava2023imitation,caballero2023broken"></dt-cite>. 
        Perhaps as important, policymakers should be aware that accurately predicting the impact of scaling is 
        currently far from feasible. Hence, there is currently limited scientific support for using exact thresholds 
        of compute alone to triage different risk levels.</p>
</div>

<script>

filterSelection("atypical") // Execute the function and show all columns
function filterSelection(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfolio_");
  y = document.getElementsByClassName("column_header_");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
}
  
filterSelection_("pie") // Execute the function and show all columns
function filterSelection_(c) {
  var x, y, z, i;
  x = document.getElementsByClassName("column_portfolio");
  y = document.getElementsByClassName("column_header");
  z = document.getElementsByClassName("column_two_fig");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
  
  for (i = 0; i < z.length; i++) {
    RemoveClass(z[i], "show");
    if (z[i].className.indexOf(c) > -1) AddClass(z[i], "show");
  }
}
  
filterSelectionfinal("thirty") // Execute the function and show all columns
function filterSelectionfinal(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfoliofinal");
  y = document.getElementsByClassName("column_headerfinal");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
 
}
  
filterSelectionfinalfinal("frequently") // Execute the function and show all columns
function filterSelectionfinalfinal(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfoliofinalfinal");
  y = document.getElementsByClassName("column_headerfinalfinal");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected
  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
 
}
  

// Show filtered elements
function AddClass(element, name) {
  var i, arr1, arr2;
  arr1 = element.className.split(" ");
  arr2 = name.split(" ");
  for (i = 0; i < arr2.length; i++) {
    if (arr1.indexOf(arr2[i]) == -1) {
      element.className += " " + arr2[i];
    }
  }
}

// Hide elements that are not selected
function RemoveClass(element, name) {
  var i, arr1, arr2;
  arr1 = element.className.split(" ");
  arr2 = name.split(" ");
  for (i = 0; i < arr2.length; i++) {
    while (arr1.indexOf(arr2[i]) > -1) {
      arr1.splice(arr1.indexOf(arr2[i]), 1);
    }
  }
  element.className = arr1.join(" ");
}

// Add active class to the current button (highlight it)
var btnContainer1 = document.getElementById("myBtnContainer");
var btns1 = btnContainer1.getElementsByClassName("btn");
for (var i = 0; i < btns1.length; i++) {
  btns1[i].addEventListener("click", function(){
    var current1 = document.getElementsByClassName("active_1");
    current1[0].className = current1[0].className.replace(" active_1", "");
    this.className += " active_1";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer2 = document.getElementById("myBtnContainer_2");
var btns2 = btnContainer2.getElementsByClassName("btn");
for (var i = 0; i < btns2.length; i++) {
  btns2[i].addEventListener("click", function(){
    var current2 = document.getElementsByClassName("active_2");
    current2[0].className = current2[0].className.replace(" active_2", "");
    this.className += " active_2";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer3 = document.getElementById("myBtnContainer_3");
var btns3 = btnContainer3.getElementsByClassName("btn");
for (var i = 0; i < btns3.length; i++) {
  btns3[i].addEventListener("click", function(){
    var current3 = document.getElementsByClassName("active_3");
    current3[0].className = current3[0].className.replace(" active_3", "");
    this.className += " active_3";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer4 = document.getElementById("myBtnContainer_4");
var btns4 = btnContainer4.getElementsByClassName("btn");
for (var i = 0; i < btns4.length; i++) {
  btns4[i].addEventListener("click", function(){
    var current4 = document.getElementsByClassName("active_4");
    current4[0].className = current4[0].className.replace(" active_4", "");
    this.className += " active_4";
  });
}
</script> 
  </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<script src="template.v1.js"></script>
<dt-appendix>
<div class="description_">  
<h3>Acknowledgments</h3>

<p> <q>Wisdom is like a baobab tree; no one individual can embrace it.</q> <cite><i>Ewe Proverb</i></cite> </p>

<p> Thank you to many of my wonderful colleagues and peers who took time to provide valuable feedback on earlier versions of this essay. I do not have much time to write or think deeply in isolation about a topic these days. Unfortunately, my time is increasingly spent helping others create breakthroughs. However, I have greatly enjoyed the small parcels of time I have spent on this essay wrestling with these ideas. This essay felt important to write because it requires grappling with several topics that are timely: the changing relationship we have with compute, how we navigate the risks introduced by the technology we have helped build and how science should inform policy. I have decided to release it as an essay that reflects the evolution of my thought process. This was more enjoyable for me writing down my thoughts, and was perhaps necessary for getting this to the finish line given other demands on my time. We are in an interesting time; it is rare to see research progress that is adopted overnight. Computer science ideas do not just resonate in conference halls anymore, but profoundly impact the world around us. This merits accountability, evidence and care as we navigate this impact.</p>

<p> Thanks for valuable feedback from several colleagues across several drafts of this essay (in no particular order): Usman Anwar, Neil Thompson, Sanmi Kojeyo, Helen Toner, Lennard Heim, Irene Solaiman, Shayne Longpre, Leshem Choshen, Sasha Luccioni, Stephen Casper, Jaime Sevilla, Nitarshan Rajkumar, Patrick Lewis, Aaron Courville, Nick Frosst, Rishi Bommasani, Gary Marcus, Thomas Diettrich,  Margaret Jennings, Marzieh Fadaee, Ahmet Ustun, Aidan Peppin, Arash Ahmadian, Yoshua Bengio, Ivan Zhang, Markus Anderljung, Alexander Popper. Perhaps unusually, I regularly try and stress test ideas by seeking to understand the strongest counterarguments. I typically learn more from those who hold different viewpoints, and for this essay I have tried to invite input from colleagues with a varied set of stances on compute thresholds. No need to identify these worthy critics, but a huge thank you to everyone who engaged fully with this piece by providing very meaningful and rich feedback that greatly improved it. Many thanks to Aidan Peppin for additional valuable proofreading. An additional thanks to Linus Chui for visual input on the normal distribution plots in Figure \ref{fig:normal_distributions}. Many thanks to Shivalika Singh for putting together the associated website to make this essay more accessible to those beyond the academic community. </p>

<p> This article was in part prepared using the <a href="https://pair-code.github.io/saliency/">Google AI Pair</a> template and style guide.
   The citation management for this article uses the <a href="https://github.com/distillpub/template">template v1</a> of the Distill style script. </p>
<p>We thank the ... </p>
  
<h3>Citation</h3>
<pre class="citation long">@misc{hooker2024limitationscomputethresholdsgovernance,
  title={On the Limitations of Compute Thresholds as a Governance Strategy}, 
  author={Sara Hooker},
  year={2024},
  eprint={2407.05694},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2407.05694}, 
}

</pre>
</div>
</dt-appendix>
	
<div class="description_">  
<h3>Bibliography</h3>
</div>
<script type="text/bibliography">

@inbook{inbook,
  author = {Zhang, Hua},
  year = {2017},
  month = {01},
  pages = {},
  title = {The History of Microwave Heating}
  }

@misc{Chellapilla2006,
  title={High Performance Convolutional Neural Networks for Document Processing},
  author={Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  year={2006},
  month={10},
  }

@article{hooker2021,
  author = {Hooker, Sara},
  title = {The hardware lottery},
  year = {2021},
  issue_date = {December 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {64},
  number = {12},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/3467017},
  doi = {10.1145/3467017},
  abstract = {After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.},
  journal = {Commun. ACM},
  month = {nov},
  pages = {58–65},
  numpages = {8}
  }

@article{OH20041311kyoung,
  title = "GPU implementation of neural networks",
  journal = "Pattern Recognition",
  volume = "37",
  number = "6",
  pages = "1311 - 1314",
  year = "2004",
  issn = "0031-3203",
  doi = "https://doi.org/10.1016/j.patcog.2004.01.013",
  url = "http://www.sciencedirect.com/science/article/pii/S0031320304000524",
  author = "Kyoung-Su Oh and Keechul Jung",
  keywords = "Graphics processing unit(GPU), Neural network(NN), Multi-layer perceptron, Text detection",
  }

@InProceedings{Payne2005,
  author="Payne, Bryson R.
  and Belkasim, Saeid O.
  and Owen, G. Scott
  and Weeks, Michael C.
  and Zhu, Ying",
  editor="Sunderam, Vaidy S.
  and van Albada, Geert Dick
  and Sloot, Peter M. A.
  and Dongarra, Jack J.",
  title="Accelerated 2D Image Processing on GPUs",
  booktitle="Computational Science -- ICCS 2005",
  year="2005",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="256--264",
  isbn="978-3-540-32114-9"
  }

@article{BRODTKORB20134,
  title = "Graphics processing unit (GPU) programming strategies and trends in GPU computing",
  journal = "Journal of Parallel and Distributed Computing",
  volume = "73",
  number = "1",
  pages = "4 - 13",
  year = "2013",
  note = "Metaheuristics on GPUs",
  issn = "0743-7315",
  doi = "https://doi.org/10.1016/j.jpdc.2012.04.003",
  url = "http://www.sciencedirect.com/science/article/pii/S0743731512000998",
  author = "André R. Brodtkorb and Trond R. Hagen and Martin L. Sætra",
  keywords = "GPU computing, Heterogeneous computing, Profiling, Optimization, Debugging, Hardware, Future trends",
  }

@misc{DettmersGPU,
  author = {Tim Dettmers},
  title = {Which GPU for Deep Learning in 2023?},
  year = {2023},
  url = {https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/},
  urldate = {2023-10-04}
  }

@article{fawzi2022discovering,
  title={Discovering faster matrix multiplication algorithms with reinforcement learning},
  author={Fawzi, Ali and Balog, Miklos and Huang, Alex and Song, Ziwei and Song, Yang and Vinyals, Oriol},
  journal={Nature},
  volume={610},
  pages={47--53},
  year={2022},
  publisher={Nature Publishing Group}
  }

@inproceedings{davies2024,
  author = {Davies, Michael and McDougall, Ian and Anandaraj, Selvaraj and Machchhar, Deep and Jain, Rithik and Sankaralingam, Karthikeyan},
  title = {A Journey of a 1,000 Kernels Begins with a Single Step: A Retrospective of Deep Learning on GPUs},
  year = {2024},
  isbn = {9798400703850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3620665.3640367},
  doi = {10.1145/3620665.3640367},
  abstract = {We are in age of AI, with rapidly changing algorithms and a somewhat synergistic change in hardware. MLPerf is a recent benchmark suite that serves as a way to compare and evaluate hardware. However it has several drawbacks - it is dominated by CNNs and does a poor job of capturing the diversity of AI use cases, and only represents a sliver of production AI use cases. This paper performs a longitudinal study of state-of-art AI applications spanning vision, physical simulation, vision synthesis, language and speech processing, and tabular data processing, across three generations of hardware to understand how the AI revolution has panned out. We call this collection of applications and execution scaffolding the CaSiO suite. The paper reports on data gathered at the framework level, device API level, and hardware and microarchitecture level. The paper provides insights on the hardware-software revolution with pointers to future trends.},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages = {20–36},
  numpages = {17},
  location = {, La Jolla, CA, USA, },
  series = {ASPLOS '24}
  }

@inproceedings{inproceedings2011,
  author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
  year = {2011},
  month = {07},
  pages = {1237-1242},
  title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
  journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
  doi = {10.5591/978-1-57735-516-8/IJCAI11-210}
  }

@article{Krizhevsky2012,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012},
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  url = {https://doi.org/10.1145/3091627},
  doi = {10.1145/3091627}
  }

@misc{szegedy2014going,
  title={Going Deeper with Convolutions}, 
  author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  year={2014},
  eprint={1409.4842},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{le2012building,
  title={Building high-level features using large scale unsupervised learning}, 
  author={Quoc V. Le and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Greg S. Corrado and Jeff Dean and Andrew Y. Ng},
  year={2012},
  eprint={1112.6209},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{coates13,
  title          = {{Deep learning with COTS HPC systems}},
  author         = {Adam Coates and Brody Huval and Tao Wang and David Wu and Bryan Catanzaro and Ng Andrew},
  year           = {2013},
  month          = {17--19 Jun},
  booktitle      = {Proceedings of the 30th International Conference on Machine Learning},
  publisher      = {PMLR},
  address        = {Atlanta, Georgia, USA},
  series         = {Proceedings of Machine Learning Research},
  volume         = {28},
  pages          = {1337--1345},
  url            = {http://proceedings.mlr.press/v28/coates13.html},
  editor         = {Sanjoy Dasgupta and David McAllester},
  pdf            = {http://proceedings.mlr.press/v28/coates13.pdf}
}


</script> 

<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
