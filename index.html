<!doctype html>
<html>
<head>
<!--     <meta charset="utf-8"> -->
<!-- <title>Redirecting to https://bob.github.io/repo/</title> -->
<!--     <meta http-equiv="refresh" content="0; URL=https://computethresholds.github.io/"> -->
<!--     <link rel="canonical" href="https://computethresholds.github.io/"> -->

  <title>On the Limitations of Compute Thresholds as a Governance Strategy</title> 
  <!-- Twitter Card data -->
  <meta name="twitter:card" value="summary">
  <meta name="twitter:title" content="On the Limitations of Compute Thresholds as a Governance Strategy">
  <!-- <meta name="twitter:description" content="What do pruned deep neural networks forget?"> -->
  <meta name="twitter:url" content="https://computethresholds.github.io/">
  <meta name="twitter:image" content="https://cdn.glitch.com/02868eea-fe84-443e-964a-8f04885fa5fa%2Faccuracy_distribution_updated.png?v=1574118491306">
  <meta name="twitter:site" content="@CohereForAI" />
  
  <meta property="og:image:width" content="1920" />
  <meta property="og:image:height" content="1080" />
  <meta property="og:title" content="On the Limitations of Compute Thresholds as a Governance Strategy" />
  <meta property="og:type" content="article" />
  <!-- <meta property="og:description" content="What do pruned deep neural networks forget?" /> -->
  <meta property="og:image" content="https://cdn.glitch.com/02868eea-fe84-443e-964a-8f04885fa5fa%2Faccuracy_distribution.png?v=1574118354833" />
  <meta property="og:url" content="https://computethresholds.github.io/" />
  <!-- <meta property="og:site_name" content="Deep Neural Network Pruning"> -->
  <meta property="og:locale" content="en_US">
  
  
  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="On the Limitations of Compute Thresholds as a Governance Strategy: Measuring the Disparate Impact of Model Pruning">
  <meta name="citation_fulltext_html_url" content="https://computethresholds.github.io/">
   <!-- Update paper link  -->
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/1911.05248">
  <meta name="citation_fulltext_world_readable" content="">
  <meta name="citation_author" content="Hooker, Sara">
  <meta name="citation_author_institution" content="Cohere For AI">
  <!-- Update publication date -->
  <meta name="citation_publication_date" content="2024/08/13">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152824096-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-152824096-1');
</script>

  <!--  https://schema.org/Article -->
  <meta property="description" itemprop="description" content="On the Limitations of Compute Thresholds as a Governance Strategy.">
  <meta property="article:author" content="Sara Hooker">
  <meta property="article:url" content="https://computethresholds.github.io//" />
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400" rel="stylesheet">
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
  <style>
     body {
      font-family: "Roboto", "Helvetica", sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      font-size: 12px;
    }
    html {
      margin: 0;
      padding: 0;
      height: 100%;
    }
    b {
      font-weight:bold;
    }
    table td {
      font-size: 12px;
      text-align: center;
      outline: 1px solid white;
      padding: 0;
      margin: 0;
    }
    table.inner td {
      padding: 0;
      margin: 0;
      border: 0;
      width: 25%;
    }
    .footer-row {
      height: 15px;
    }
    table.inner tr {
      border: 0;
    }
    table.inner th {
      padding: 8px;
    }
    table th {
      font-size: 11px;
    }
    table {
      border-collapse: collapse;
      border-spacing: 0;
    }
    thead, tbody { display: block; }
    .rotated {
      transform: rotate(90deg);
      transform-origin: left bottom 0;
      margin-top: -111px;
      font-weight: bold;
      font-size: 1.2em;
      padding: 8px;
    }
    #headers {
      z-index: 1000;
      background-color: white;
      height: 65px;
      vertical-align: middle;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }
    #headers span {
      background-color: white;
      display: inline-block;
      line-height: 65px;
      font-size: 1.2em;
      font-weight: bold;
      text-align: center;
      text-overflow: ellipsis;
      white-space: nowrap;
    }
    .cover {
      background: #1e283a;
    }
    .cover-container {
      padding-top: 10px;
      padding-bottom: 60px;
    }
    .descriptions_, .description_ {
      padding-top: 20px;
    }
    .cover-container, .descriptions_, .description_ {
      padding-right: 5px;
      padding-left: 5px;
      margin-right: auto;
      margin-left: auto;   
    }
  
    
  
    @media (min-width: 415px) {
      authors .authors-affiliations,
       .base-grid, .imgs-container
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        width: 500px;
      }
      .column_portfolio_  .column_portfolio_final .column_portfolio figcaption, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        padding: 0;
        padding-top: 4px;
        word-wrap: break-word;
        word-break: break-word;
      }
    }
    @media (min-width: 768px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_  .column_portfolio .column_portfoliofinal {
        width: 650px;
      }
    }
    @media (min-width: 992px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal  {
        width: 770px;
      }
    }
    @media (min-width: 1200px) {
      authors .authors-affiliations, .imgs-container, 
      .cover-container, .descriptions_, .description_, .column_portfolio, .column_portfolio_,  .column_portfolio, .column_portfoliofinal {
        width: 970px;
      }
    }
    .cover h1 {
      font-family: "Roboto", "Gotham A", "Gotham B";
      letter-spacing: 0.05em;
      font-size: 63px;
      font-weight: 700;
      margin-bottom: 0.5em;
      text-transform: uppercase;
    }
    .cover h3 {
      font-size: 30px;
      letter-spacing: 0.05em;
      font-weight: 500;
    }
    .descriptions_ h3 {
      color: #313b4e;
      opacity: .8;
    }
    
    .descriptions_ p {
      color: #313b4e;
      opacity: .8;
      font-size: 16px;
    }
    .cover {
      color: #ddd;
    }
    
    .authors {
      margin-top: -40px;
      overflow: hidden;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    font-size: 1.5rem;
    line-height: 1.8em;
    padding: 1.5rem 0;
    min-height: 1.8em;
    }
    
    .subtitle {
      margin-top: -20px;
    }
    .icons {
      margin-top: 30px;
      padding-left: 4px;
    }
    .icons a {
      display: inline-block;
      font-size: 16px;
      color: #ccc;
      text-decoration: none;
    }
    .paper-icon {
      display: inline-block;
    }
    .paper-icon a {
      line-height: 35px;
      vertical-align: top;
    }
    .paper-icon:hover a {
      cursor: pointer;
      text-decoration: underline;
    }
    .description_ p {
      width: 100%;
      font-size: 16px;
    }
    .description_ img {
      vertical-align: middle;
      width: 100%;
    }
    .imgs-container {
      display: table-row;
    }
    .img-container {
      color: #62779c;
      text-align: center;
      font-weight: bold;
      font-size: 14px;
      padding-right: 6px;
      display: table-cell;
      width: 33%;
    }
    #headers.fixed-header {
      position: fixed;
      top: 0;
    }
    #table-container.fixed-header {
      margin-top: 106px;
    }
    .image-label {
      font-size: 15px;
      text-align: left;
      padding-bottom: 4px;
      padding-top: 6px;
      padding-left: 2px;
      font-weight: normal;
    }
    .img-times-selector-container {
      margin-left: -80px;
      margin-top: -45px;
      font-size: 18px;
      font-weight: bold;
      text-align: center;
     }
    .img-times-selector {
      width: 175px;
    }
    #table {
      margin-top: 0px;
      width: 100%;
    }
    
* {
  box-sizing: border-box;
}
/* Center website */
.row {
  margin: 8px -16px;
}
/* Add padding BETWEEN each column (if you want) */
.row,
.row > .column_portfolio {
  padding: 3px;
}
/* Create three equal columns that floats next to each other */
.column_portfolio {
  float: left;
  width: 33.33%;
  display: none; /* Hide columns by default */
}
    
.column_portfolio_  .column_portfolio .column_portfoliofinal figcaption {
      padding: 4px 8px;
     word-wrap: break-all;
      word-break: break-all;
  }
    
/* Create three equal columns that floats next to each other */
.column_portfolio_ {
  float: left;
  width: 25.00%;
  display: none; /* Hide columns by default */
}
    
.column_portfoliofinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_portfoliofinalfinal {
  float: left;
  width: 25.00%;
  display: none; /* Hide columns by default */
}
.column_header {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_header_ {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_headerfinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
    
.column_headerfinalfinal {
  float: left;
  width: 100.00%;
  display: none; /* Hide columns by default */
}
  
    
.column_two_fig {
  float: left;
  width: 50.00%;
  display: none; /* Hide columns by default */
}
/* Clear floats after rows */
.row:after {
  content: "";
  display: table;
  clear: both;
}
/* Content */
.content {
  background-color: white;
  padding: 10px;
  width: 80%;
  margin-left: auto;
  margin-right: auto;
}
	  
.content_reduced {
  background-color: white;
  padding: 10px;
  width: 70%;
  margin-left: auto;
  margin-right: auto;
}
    
.content_reduced_slightly {
  background-color: white;
  padding: 2px;
  width: 50%;
  margin-left: auto;
  margin-right: auto;
}
    
.content_resized {
  background-color: white;
  padding: 2px;
  width: 50%;
  margin-left: auto;
  margin-right: auto;
}
/* The "show" class is added to the filtered elements */
.show {
  display: block;
}
/* Style the buttons */
.btn {
  border: none;
  border-radius: 4px;
  outline: none;
  padding: 12px 16px;
  font-size: 14px;
  background-color:#599bb3;
  color:#ffffff;
  background:linear-gradient(to bottom, #599bb3 5%, #408c99 100%);
  text-shadow:0px 1px 0px #3d768a;
  margin-right: auto;
  margin-left: auto;  
   margin-bottom:5px;
  cursor:pointer;
}
/* Add a grey background color on mouse-over */
.btn:hover {
  background-color: #ddd;
}
/* Add a dark background color to the active button */
.btn.active_1, .btn.active_2, .btn.active_3, .btn.active_4, .btn:target
{ background:linear-gradient(to right, #666 3%, #666  100%);
  color: white;
  cursor:none;
}
    
figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 14px;
   font-weight: bold;
  line-height: 1.5em;
}
    
figcaption a {
  color: rgba(0, 0, 0, 0.6);
}
figcaption b,
figcaption .strong_, {
  font-weight: bold;
  font-size: 14px;
  color: #180A3E;
}
    
</style>
</head>
<body>
  <div id="scroll-container">
    <div class="cover">
      <div class="cover-container">
        <div class="icons">
          <div class="paper-icon">
            <!-- Update Paper Link -->
            <a href="https://arxiv.org/abs/1911.05248">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fpaper_icon.png?v=1572561063939" style="width: 100px"/><br>Paper
            </a>
          </div>
          <!-- <div class="paper-icon" style="margin-left: 20px">
            <a href="https://github.com/google-research/google-research/tree/master/pruning_identified_exemplars">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fcode_icon.png?v=1572562103868" style="width: 100px"/><br>Code
            </a>
          </div>     -->
        </div>
        <div class="title"><h2>On the Limitations of Compute Thresholds as a Governance Strategy</h2></div>
        <div class="authors">Sara Hooker</div>
      <div class="institutions"></div>
       </div>
    </div>
      <div class="descriptions_">
	<h3>The Uncertain Relationship Between Compute and Risk</h3>
        </div>

         <div class="description_">
          
          <p> <q>Well Babbage what are you dreaming about?</q> to which I replied, <q>I am thinking that all these tables might be calculated by machinery.</q> <cite><i>Charles Babbage</i></cite> </p>

          <p> Many inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube 
            was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, 
            noticed that a chocolate bar melted in his pocket whenever he was close to a radar set. This innocuous discovery 
            resulted in the patent for the first microwave <dt-cite key="inbook"></dt-cite>. In a similar vein, deep neural networks only began 
            to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally 
            introduced in the 1970s as a specialized accelerator for video games and for developing graphics for movies and 
            animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use 
            case – to train deep neural networks <dt-cite key="Chellapilla2006,hooker2021,OH20041311kyoung,Payne2005"></dt-cite>. 
            GPUs had one critical advantage over CPUs - they were far better at parallelizing matrix 
            multiples <dt-cite key="BRODTKORB20134,DettmersGPU"></dt-cite>, a mathemetical operation which dominates the definition of deep 
            neural network layers <dt-cite key="fawzi2022discovering,davies2024"></dt-cite>. This higher number of floating operation points 
            per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of 
            deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever 
            deeper networks in 2011 <dt-cite key="inproceedings2011"></dt-cite>, 2012 <dt-cite key="Krizhevsky2012"></dt-cite> and 
            2015 <dt-cite key="szegedy2014going"></dt-cite>. A striking example of this jump in compute is a comparison of the now famous 
            2012 Google paper which used 16,000 CPU cores to classify cats <dt-cite key="le2012building"></dt-cite> to a paper published a 
            mere year later that solved the same task with only two CPU cores and four GPUs <dt-cite key="coates13"></dt-cite>. </p>
            
           <p> This would ignite a rush for compute which has led to a bigger-is-better race in the number of model parameters 
            over the last decade <dt-cite key="2016Canziani,strubell2019energy,rae2021scaling,raffel2020exploring,bommasani2021opportunities,bender_gebru_2021"></dt-cite>. 
            The computer scientist Ken Thompson famously said <q>When in doubt, use brute force.</q>  
            This was formalized as the “bitter lesson” by Rich Sutton who posited that computer science history tells us that 
            throwing more compute at a problem has consistently outperformed all attempts to leverage human knowledge of a domain 
            to teach a model <dt-cite key="SilverBittrLesson"></dt-cite>. In a punch to the ego of every computer scientist out there, what Sutton is 
            saying is that symbolic methods that codify human knowledge have not worked as well as letting a model learn patterns 
            for itself coupled with ever-vaster amounts of compute.  </p>
	   
             <p> <b>Is Sutton right?</b> Certainly, he is correct that scaling has been a widely favored formula because 
                it has provided persuasive gains in overall performance – size is the most de-risked tool we have to unlock 
                new gains. As the computer scientist Michael Jordan quipped <q>Today we can’t think without holding a 
                    piece of metal.</q> Increasing compute also conveniently fits into the cadence of quarterly industry 
                    planning, it is less risky to propose training a bigger model than it is to propose an alternative 
                    optimization technique. However, relying on compute alone misses a critical shift that is underway in the 
                    relationship between compute and performance. It is not always the case that bigger models result in better 
                    performance. The bitter lesson doesn't explain why Falcon 180B <dt-cite key="almazrouei2023falconseriesopenlanguage"></dt-cite> is 
                    easily outperformed by far smaller open weights models such as Llama-3 8B <dt-cite key="llama3modelcard"></dt-cite>, 
                    Command R 35B <dt-cite key="cohere_c4ai_command_r_plus"></dt-cite>, Gemma 27B <dt-cite key="gemma_2024"></dt-cite>. It also doesn't explain why 
                    Aya 23 8B <dt-cite key="aryabumi2024aya"></dt-cite> easily outperforms BLOOM 176 B <dt-cite key="workshop2023bloom176bparameteropenaccessmultilingual"></dt-cite> 
                    despite having only 4.5% of the parameters.   </p>

             <!-- Add Figure 3 from Paper here -->

                 <!-- <div class="content_reduced_slightly">
     <img src="https://cdn.glitch.com/f1ebd1ee-d1ac-4538-8ad5-0034e332e4ae%2Fsynaptic_pruning_image.png?v=1574277111414" alt="abstract_1" style="width:100%">
      <div class="figcaption">
         <strong_>Synaptic pruning removes redundant neurons and strengthens connections that are most useful for the environment. (Figure courtesy of Seeman, 1999)</strong_><br>
       </div>
             </div>   -->
    
		 <p> These are not isolated examples, but rather indicative of an overall trend where there is no guarantee 
            larger models consistently outperform smaller models. Figure \ref{fig:above_13_b} plots the scores of models 
            submitted to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>
            over the last two years. Here, we plot <i>large models</i> with more than 13 billion parameters whose leaderboard 
            score is less than the top performing <i>small model</i> with less than 13 billion parameters. 
            We observe that over time, more and more large models have been submitted that are outperformed by the best 
            small model daily submission. To understand why this is the case, we must understand what key variables have been 
            driving gains in performance over the last decade. In an era where there are diminishing returns for the amount 
            of compute available <dt-cite key="lohn2022ai,2020Thompson"></dt-cite>, optimization and architecture breakthroughs define the rate 
            of return for a given unit of compute. <b>It is this rate of return which is most critical to the pace of
                progress and to the level of risk incurred by additional compute</b>.</p>

        </div>

        <div class="description_">
            <h4> A shift in the relationship between compute and performance </h4>

            <p> <q>The world has changed less since Jesus Christ than it has in the last 30 years.</q> <cite><i>Charles Peguy, 1913</i></cite> </p>

            <p> In complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. 
                Throughout the 20th century doctors recommended removing tonsils in response to any swelling or infection, 
                but research has recently shown the removal may lead to higher incidence of throat cancer <dt-cite key="liang2023"></dt-cite>. 
                Early televised drug prevention advertisements in the 2000s led to increased drug use <dt-cite key="Terry-McElrath2011"></dt-cite>. 
                In a similar vein, the belief that more compute equates with more risk belies a far more complex picture that 
                requires re-examining the relationship between performance and compute. A key limitation of simply throwing 
                more scale at a task is that the relationship between additional compute and generalization remains poorly 
                understood. A growing body of research suggests that the relationship between compute and performance is far more 
                complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier. </p>

            <p> <b>Data quality reduces reliance on compute.</b> Models trained on better data do not require as much compute. 
                A large body of work has emerged which shows that efforts to better curate training corpus, including 
                de-duping <dt-cite key="taylor2022galactica, kocetkov2022stack"></dt-cite>, data pruning <dt-cite key="marion2023more,ayadata2024,sorscher2023neural,albalak2024survey,tirumala2023d4,chimoto2024critical"></dt-cite> 
                or data prioritization <dt-cite key="boubdir2023prompts,thakkar2023selfinfluence"></dt-cite> can compensate for more weights. 
                This suggests that the number of learnable parameters is not definitively the constraint on improving performance; 
                investments in better data quality mitigate the need for more weights <dt-cite key="ayadata2024,penedo2023refinedweb,raffel2020exploring,lee2022deduplicating"></dt-cite>. 
                If the size of a training dataset can be reduced without impacting performance <dt-cite key="marion2023more"></dt-cite>, 
                training time is reduced. This directly impacts the number of training FLOP and means less compute is needed. </p>

            <p> <b>Optimization breakthroughs compensate for compute.</b> Progress over the last few years has been as 
                much due to optimization improvements as it has been due to compute. This includes extending pre-training 
                with instruction finetuning to teach models instruction following <dt-cite key="singh2024aya"></dt-cite>, model distillation 
                using synthetic data from larger more performant "teachers" to train highly capable, smaller 
                "students" <dt-cite key="gemmateam2024gemma,aryabumi2024aya"></dt-cite>, chain-of-thought reasoning <dt-cite key="wei2023chainofthought,hsieh2023distilling"></dt-cite>, 
                increased context-length <dt-cite key="xiong2023effective"></dt-cite>, enabled tool-use <dt-cite key="qin2023toolllm,wang2023voyager"></dt-cite>, 
                retrieval augmented generation <dt-cite key="pozzobon2023goodtriever,NEURIPS2020_6b493230"></dt-cite>, and preference training to align 
                models with human feedback <dt-cite key="dang2024rlhfspeaklanguagesunlocking,ahmadian2024basics,ouyang2022LLMRLHF,bai2022constitutional,lee2023rlaif,tunstall2023zephyr,khalifa2021distributional,rafailov2023DPO,azar2023IPO"></dt-cite>. 
                All these techniques compensate for the need for weights or expensive prolonged training <dt-cite key="ho2024algorithmicprogresslanguagemodels"></dt-cite>. 
                All things equal, these have been shown to dramatically improve model performance relative to a model trained 
                without these optimization tricks given the same level of compute <dt-cite key="davidson2023ai,hernandez2020,erdil2023algorithmic,METR_undated,liu2024sophia"></dt-cite>. 
                In Figure \ref{fig:13b_models}, we plot the best daily 13B or smaller model submitted to the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a> over time. 
                In a mere span of 2 years, the best-performing daily scores from small model went from an average of 38.59% across to an average of 77.15% across 2024 submissions.
                The takeaway is clear -- smaller models with the same amount of capacity are becoming more and more performant. </p>

            <p> <b>Architecture plays a significant role in determining scalability</b> The introduction of a new architecture
                design can fundamentally change the relationship between compute and 
                performance <dt-cite key="tay2022scaling,Sevilla_2022,ho2024algorithmic"></dt-cite> and render any compute threshold that is 
                set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of 
                architectures like convolutional neural networks (CNNs) for vision <dt-cite key="inproceedings2011,Krizhevsky2012,szegedy2014going"></dt-cite> and 
                Transformers for language modeling <dt-cite key="vaswani2023attention"></dt-cite>. </p>

            <p> While deep neural networks represent a huge step forward in performance for a given level of compute, what is 
                often missed is that our architectures also represent the ceiling in what is achievable through scaling. 
                While progress has revolved around deep neural networks for the last decade, there is much to suggest that the 
                next significant gain in efficiency will require an entirely different architecture. Deep neural networks remain 
                very inefficient as an algorithm. Our typical training regimes require that all examples are shown the same 
                number of times during the training <dt-cite key="xue2023adaptive"></dt-cite>. All modern networks are trained based upon 
                minimization of average error <dt-cite key="Goodfellow-et-al-2016"></dt-cite>. This means that learning rare artifacts requires 
                far more training time or capacity due to the diluted signal of infrequent attributes relative to the most 
                frequent patterns in the dataset <dt-cite key="Achille2017CriticalLP, jiang2020exploring, Mangalam2019DoDN, 2020fartash,frankle2020,pmlr-v70-arpit17a"></dt-cite>. 
                Small models are already good at learning the most frequent features, and most easy features and common patterns are 
                learned early on training with much harder rare features learned in later stages <dt-cite key="agarwal2020estimating,paul2021deep,Mangalam2019DoDN,siddiqui2022metadata,abbe2021staircasepropertyhierarchicalstructure"></dt-cite>. 
                When we radically scale the size of a model, we show the most gains in performance on are rare and underrepresented 
                attributes in the dataset -- the long-tail <dt-cite key="hooker2019compressed,hooker2020characterising"></dt-cite>. 
                Put differently, scaling is being used to inefficiently learn a very small fraction of the overall training 
                dataset. Our reliance on global updates also results in catastrophic forgetting, where performance deteriorates 
                on the original task because the new information interferes with previously learned behavior <dt-cite key="Mcclelland1995,pozzobon2023goodtriever"></dt-cite>. 
                All this suggests that our current architecture choices are probably not final and key disruptions lie ahead. 
                This is likely to radically change any scaling relationships, in the same way it has done in the last decade. 
                For example, it is unlikely any prediction of how compute scales based upon architectures before deep neural networks holds 
                true post-2012 after the introduction of convolutional neural networks.</p>
  </div>
           
    <!-- Uncomment below to create image container with key findings -->

           <!-- <div class="imgs-container">
        
            <div class="descriptions_">
              <p> The primary findings of our work can be summarized as follows: </p>

          <div class="img-container">1. Pruning would be better described as "selective brain damage." Pruning has a non-uniform impact across classes; a fraction of classes are disproportionately and systematically impacted by the introduction of sparsity.</div>
           <div class="img-container">2. The examples most impacted by pruning, which we term <i>Pruning Identified Exemplars</i> (PIEs), are more challenging for both pruned and non-pruned models to classify.</div>
          <div class="img-container">3. Pruning significantly reduces robustness to image corruptions and natural adversarial images.</div>
      </div>
     </div> -->
   
   <div class="descriptions_">
  <h3>Avoiding a FLOP FLOP</h3>
     </div>

<div class="description_">
  <p> <q>Any statistical relationship will break down when used for policy purposes.</q> <cite><i>Jon Danielsson</i></cite> </p>

   <p> <i>Are FLOP a reliable proxy for overall compute?</i> Even if the relationship between compute and generalization 
    were stable – there are difficulties operationalizing FLOP as a metric.  FLOP <dt-cite key="Goldberg1991"></dt-cite> refers 
    to <i>floating-point operations</i>, and has a fairly straightforward definition: sum up all the math operations in 
    floating point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers 
    were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields 
    that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. 
    This is because all these areas are dominated by simple primitive mathematical operations – for example, FLOP tend to be 
    closely associated with the size of models because deep neural network layers are dominated by a single 
    operation -- matrix multiplies -- which can be decomposed into a set of floating point operations <dt-cite key="fawzi2022discovering,davies2024"></dt-cite>. </p>
   
    <p> <b>We first begin by noting there are some reasons FLOP are attractive as a policy measure.</b> The primary one is 
        that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts 
        don’t change across hardware – the number of mathematical operations is the same no matter what hardware you train a 
        model on. In a world where hardware is increasingly heterogeneous <dt-cite key="hooker2021"></dt-cite> and it is hard to replicate the 
        exact training setting due to a lack of software portability <dt-cite key="NEURIPS2023_42c40aff"></dt-cite>, it is attractive to use a 
        metric that doesn’t depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could 
        occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of 
        new hardware generations <dt-cite key="epoch2023trendsinmachinelearninghardware"></dt-cite>, as well as engineering investments in 
        training infrastructure <dt-cite key="yoo2022scalable,lepikhin2020gshard"></dt-cite>, mean that over time much larger models will be 
        trained using the same number of devices. FLOP is also a metric which could potentially be inferred by cloud providers. 
        Given most machine learning workloads are run by a few key cloud providers, this may make administering such a measure 
        effectively easier <dt-cite key="heim2024governing"></dt-cite>.  </p>

    <p> A key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but 
        FLOP doesn’t say anything about end performance of a model --- only about the number of operations applied to the data. 
        For example, if you compare two models trained for the same number of FLOP but one has had safety alignment during 
        post-training <dt-cite key="aakanksha2024multilingualalignmentprismaligning,bai2022constitutional"></dt-cite> and the other has 
        none – these two models will still be accorded the same level of risk according to number of FLOP but one will present 
        a far lower risk to society because of safety alignment. </p>

    <p> Another key hurdle governance which adopts compute threshold will have to overcome is the lack of clear guidance 
        in all the policy to-date about how FLOP will actually be measured in practice. This ambiguity risks FLOP as a 
        metric being irrelevant or at the very least easy to manipulate. Developing principled standards for measuring any 
        metric of interest is essential for ensuring that safety measures are applied in a proportionate and appropriate way. 
        In the followings Section, we specify some of the key ways in which it is easy to manipulate FLOP if it is left 
        underspecified as a metric.  </p>
</div>

<div class="description_">
    <h4> Challenges of using FLOP as a metric </h4>

    <p> <q>If you cannot measure it, you cannot improve it.</q> <cite><i>Lord Kelvin</i></cite> </p>

    <p> <b>Training FLOP doesn't account for post-training leaps in performance</b> Applying scrutiny and regulation based 
        upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. 
        This can be grouped under <q>inference-time compute</q> and can result in large performance gains that dramatically 
        increase the risk profile of a model.  The limited work to-date which has evaluated a subset 
        of <q>inference-time compute</q> improvements estimates these can impart gains between 5x and 20x of base level
        post-training performance <dt-cite key="davidson2023ai"></dt-cite>.<q>inference-time compute</q> includes best-of-n sampling 
        techniques <dt-cite key="geminiteam2024gemini"></dt-cite>, chain-of-thought reasoning <dt-cite key="wei2023chainofthought,hsieh2023distilling,wang2023selfconsistency"></dt-cite> 
        and model distillation using synthetic data  <dt-cite key="aryabumi2024aya,shimabucoro2024llmseellmdo,ustun2024aya, geminiteam2024gemini"></dt-cite>. 
        All these techniques require more compute at test-time because of the need to perform more forward passes of the 
        model to generate additional samples. However, these are not reflected in training time costs and indeed 
        can often <i>reduce</i> the compute needed during training. For example, smaller, more performant models are often 
        trained on smaller amounts of synthetic data from a highly performant teacher <dt-cite key="epoch2023tradingoffcomputeintrainingandinference,huang2022large"></dt-cite>. 
        These improvements dramatically improve performance but are currently completely ignored by compute thresholds 
        since they don't contribute to <i>training</i> FLOP. </p>

    <p> Increasing the context-length <dt-cite key="xiong2023effective"></dt-cite> and retrieval augmented 
        systems <dt-cite key="lee2024longcontext,pozzobon2023goodtriever,NEURIPS2020_6b493230"></dt-cite> are additional examples of 
        introducing additional computational overhead at test-time by increasing the number of tokens to process. 
        Retrieval augmented models (RAG) have become a mainstay of state-of-art models yet are often introduced after training. 
        Most RAG systems are critical for keeping models up-to-date with knowledge yet contribute minimal or no FLOP. 
        Retrieval augmented models are particularly good at supplementing models with search capabilities or external 
        knowledge, which can enhances risks which depend on up-to-date knowledge such as biorisk and cybersecurity threats. </p>

    <p> Additionally increasing the context length often requires minimal FLOP but can dramatically increase performance 
        of a model. Entire books can be passed in at test time dramatically improving model performance on specialized 
        tasks (Gemini has 2M context window) <dt-cite key="xiong2023effective"></dt-cite>. This can make the number of FLOP irrelevant if 
        sensitive biological data can be passed at inference time in a long-context window. </p>

    <p> <b>Difficulty Tracking FLOP across model lifecycle.</b> Increasingly, training a model falls into distinct stages 
        that all confer different properties. For example, unsupervised pre-training dominates compute costs because the 
        volume of data is typically in the trillions of tokens <dt-cite key="epoch2023trendsinthedollartrainingcostofmachinelearningsystems,heim2023palm"></dt-cite>. 
        Following this, there is instruction finetuning, which confers the model the ability to follow 
        instructions <dt-cite key="ayadata2024"></dt-cite> and then preference training <dt-cite key="aakanksha2024multilingualalignmentprismaligning,ahmadian2024basics,bai2022constitutional,ouyang2022LLMRLHF,lee2023rlaif,tunstall2023zephyr,khalifa2021distributional,rafailov2023DPO,azar2023IPO"></dt-cite>, 
        which aligns model performance with human values. Between each of these steps models are often released 
        publicly <dt-cite key="ustun2024aya,touvron2023llama,aryabumi2024aya"></dt-cite>, meaning that developers can take a model from a 
        different developer and continue optimizing. The models with the most downloads on platforms like HuggingFace are 
        base models which are most conducive for continued pre-training. As sharing of models at different stages of the 
        life-cycle becomes more common, so will difficulties in tallying FLOP across the entire model life-cycle. 
        Furthermore, it may simply be infeasible to trace federated, decentralized training of models where hardware often 
        belongs to many different participants and training is conducted in a privacy-preserving manner <dt-cite key="donyehiya2023cold,borzunov2023petals,yuan2023decentralizedtrainingfoundationmodels,qin2024federatedfullparametertuningbillionsized"></dt-cite>. </p>

    <p> <b>How to handle Mixture of Experts (MoEs) and classic ensembling?</b> 
        MoEs <dt-cite key="zadouri2023pushing,shazeer2018meshtensorflow,riquelme2021scaling,du2022glam,fedus2022switch,tan2024scattered"></dt-cite> 
        are examples of adaptive compute -- where examples are routed to different parts of a model. This type of architecture 
        can often provide powerful efficiency gains, as despite a much larger overall architecture, only a subset of weights 
        are activated for a given example. Current policy frameworks do clearly not specify how to handle Mixture of 
        Experts (MoEs), which constitute some of the most highly performant systems currently deployed, such as 
        Mixtral <dt-cite key="jiang2024mixtral"></dt-cite> and the Gemini family of models <dt-cite key="geminiteam2024gemini"></dt-cite>. However, this raises 
        important questions – should the compute for each expert be counted towards total FLOP, or only the FLOP used to train 
        the subset of experts that are active at inference time? Given final performance depends on all experts in an 
        MoE, a recommendation should be to include all FLOP in the final consideration, but this is currently under-specified. 
        It also raises the question of how to treat new \emph{hybrid techniques} which train several specialized experts and then 
        both average parameters and utilize routing <dt-cite key="sukhbaatar2024branchtrainmix"></dt-cite>. </p>

    <p>Classical <i>simple ensembling techniques</i> dominate production systems in the real 
        world <dt-cite key="ko2023fairensemble,li2024agents"></dt-cite> and have been shown to heavily outperform a single model. 
        Unlike MoEs which are jointly optimized or trained using a router, classic ensembles are often only combined at 
        inference time using simple averaging of weights. Given the ensemble is never trained together, it is unclear whether 
        FLOP should reflect the compute of the single final model or the sum of all the training compute across models that
         were averaged. If it only reflects the FLOP of the final model, this may underestimate risk given ensembling is known 
         to improve performance. </p>

    <p> <b>FLOP only accounts for a single model, but does not capture risk of the overall system.</b>  
        The emphasis on compute thresholds as an indicator of risk also implies that risk is the property of a single model 
        rather than the system in which it is deployed. In the real-world, impact and risk are rarely attributable to a 
        single model but are a facet of the entire system a model sits in and the way it interacts with its 
        environment <dt-cite key="compound-ai-blog,NIPS2015_86df7dcf,jatho2023concretesafetymlproblems,raji2020closingaiaccountabilitygap"></dt-cite>. 
        Many real-world production systems are made up of cascading models where the final output is produced as a results of 
        inputs being processed by multiple algorithms in sequence <dt-cite key="paleyes2022,FrontierModelForum,NIPS2015_86df7dcf,shankar2022operationalizing"></dt-cite>. 
        There has yet to be guidance on whether the FLOP threshold is specific to a single model or whether all models that 
        constitute an end-to-end system contribute to the final tally. This has significant implications for model 
        providers – a cascade system is often made up of models which are not individually very powerful or risky – yet the 
        overall system may exceed the FLOP threshold.  </p>

    <p> There is also no specification as to how to treat model agents which may interact with both each other and/or use tools. 
        End performance of the agents is undoubtedly due to the interactions with other agents and access to 
        tools <dt-cite key="li2024agents"></dt-cite>, yet is unlikely to be considered a single model. It has already been shown that models 
        which are enabled with tool use, or can interact with a wider environment outperform a single model on its 
        own <dt-cite key="wang2023voyageropenendedembodiedagent,anwar2024foundationalchallengesassuringalignment,mialon2023augmentedlanguagemodelssurvey"></dt-cite>. 
        These are far from edge cases; the reality is that most technology deployed in the wild is rarely just an algorithm is 
        isolation. Typically, interdependent models feed into a user experience and interact with a set of choices about design and 
        delivery that impact the overall level of risk.  </p>

    <p> <b>FLOP varies dramatically for different modalities.</b> In Figure \ref{fig:different_modalities}, we plot the 
        FLOP requirements over time of models grouped according to modality and downstream use 
        case (model FLOP data from \citet{epoch2023pcdtrends}). It is easy to observe that the compute requirements have not 
        increased at the same rate across modalities. For example, code models typically require less 
        compute <dt-cite key="lin2024scaling"></dt-cite>, as do biological models <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>.   
        Multilingual models <dt-cite key="ustun2024aya,aryabumi2024aya"></dt-cite> tend to require more compute for each additional 
        language covered. This is often referred to as the <i>curse of multilinguality</i> <dt-cite key="ustun2024aya,arivazhagan2019massively,conneau2019unsupervised,pfeiffer2022lifting"></dt-cite>, 
        where capacity is split between more languages such that performance on any given language suffers relative to a 
        monolingual (single language) model of the same size. These differing compute needs mean that a single threshold may 
        penalize some types of models and reward others. For example, thresholds may penalize multilingual models that attempt 
        to serve many languages and improve access to technology <dt-cite key="ustun2024aya,aryabumi2024aya"></dt-cite>.</p>

    <p> One way to address differences in modalities is to maintain different compute thresholds for each modality. 
        While at first glance this is an attractive solution, it also imposes more technical overhead on governments who 
        must correctly set a hard-coded benchmark for each modality. For example, it is interesting to note that the 
        US Executive Order already has at least one modality-specific caveat to the compute thresholds by carving out a 
        separate compute threshold for biological models. It is set lower for models trained for biological sequence data 
        at 10<sup>23</sup>. However, since the threshold was set, models like xTrimoPGLM <dt-cite key="chen2024xtrimopglm"></dt-cite> already exceed 
        the biological threshold set at 10<sup>23</sup> operations by a factor of 6x <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        Many models <dt-cite key="lin2023,elnaggar2020,Dalla-Torre2023.01.11.523679"></dt-cite> are currently within a factor of 10x the 
        Executive Order’s reporting threshold <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        These models do not appear to present a decidedly different risk profile from previous generations, so if the goal 
        of the thresholds is to be an inflection point for amplified risk it is unclear if it has been set successfully. </p>

    <p> Specifying separate thresholds for different modalities also risks inviting gamification. For example, to 
        avoid a lower threshold for scrutiny for biological models one loophole is to preserve biology specific training 
        data at less than 50%. According to current guidance the model would no-longer qualify as a <q>biological</q> model and 
        would only be subject to the higher general purpose compute thresholds. Galactica-120B <dt-cite key="taylor2022galactica"></dt-cite> and 
        Llama-molinst-protein-7b <dt-cite key="fang2024domainagnostic"></dt-cite> are both examples of models with capabilities for biological 
        sequence modeling without primarily being trained on biological sequence data. Despite both presenting biological 
        capabilities, neither is likely to be considered  <q>biological</q> under the current Executive Order requirements <dt-cite key="epoch2024biologicalsequencemodelsinthecontextoftheaidirectives"></dt-cite>. 
        This highlights the fundamental tension of relying on compute alone -- since it is not anchored to the risk metric that is 
        of primary concern, it may be possible to sidestep in many creative ways while still presenting high-risk capabilities.</p>

    <p> In Appendix \ref{sect:technical_details_FLOP}, we also present some more technical aspects of the difficulty of 
        measuring FLOP in practice, such as the difference between theoretical and hardware FLOP, and how to handle difference 
        in quantization. Developing principled standards for measuring FLOP is essential for ensuring that safety measures are 
        applied in a proportionate and appropriate way. </p>

</div>

<div class="descriptions_">  
<h3>We are not very good at predicting the relationship between compute and risk.</h3>
</div>

<div class="description_">  
  
      <p> <q>In theory, there is no difference between theory and practice. But, in practice, there is.</q> <cite><i>Walter J. Savitch</i></cite> </p>

      <p> The choice of where compute thresholds are set will have far-ranging implications – too low and too 
        many models will be selected for additional auditing and benchmarking each year. In contrast, if it is 
        set too high, not enough models will be audited for risk, and the threshold risks become decorative 
        rather than a meaningful indicator of risk. None of the policies to date have provided justification about 
        where they have set their thresholds, or why it excludes almost all models deployed in the wild today. 
        In Section \ref{sect:tradeoff_compute_performance}, we grappled with the changing overall relationship 
        between compute and performance. However, scientific justification for a threshold requires predicting 
        how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold 
        reflects scientific consensus as to when particular risk factors are expected to emerge due to scale. 
        Hence, it is worth considering our success to date in estimating how different model properties change 
        with scale.  </p>
      
      
      <p> Warren Buffet once said <i><q>Don’t ask the barber if you need a haircut.</q></i> In the same 
        vein, don’t ask a computer scientist or economist whether you can predict the future. The temptation to 
        say yes often overrides a necessary humility about what can and cannot be predicted accurately. One such 
        area where hubris has overridden common sense is attempts to predict the relationship between scale and 
        performance in the form of <i>scaling laws</i> <dt-cite key="kaplan2020scaling,hernandez2021scaling,Dhariwal2021DataAP"></dt-cite>
        which either try and predict how a model's pre-training loss scales <dt-cite key="bowman2023things"></dt-cite> 
        or how downstream properties emerge with scale. It is the latter task which is urgently needed by policymakers 
        in order to anticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) 
        at inflection points where risk increases with scale <dt-cite key="anthropic_responsible_scaling,openai_global_affairs, kaminski_regulating_2023"></dt-cite>.  </p>
      
      <p> One of the biggest limitations of scaling laws is that they have only been shown to hold when predicting 
        a model’s pre-training test loss <dt-cite key="bowman2023things"></dt-cite>, which measures the model’s 
        ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual 
        performance on downstream tasks is used, the results are often murky or inconsistent <dt-cite key="Ganguli_2022,schaeffer2023emergent,anwar2024foundational,Ganguli_2022,schaeffer2024predictingdownstreamcapabilitiesfrontier,hu2024predictingemergentabilitiesinfinite"></dt-cite>. 
        Indeed, the term <i>emerging properties</i> is often used to describe this 
        discrepancy <dt-cite key="Wei2022,srivastava2023imitation"></dt-cite>: a property that appears “suddenly” as the 
        complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws 
        don't hold when you try to predict downstream performance instead of predicting test loss for the next 
        word token. </p>      
      
      <p> Even when limited to predicting test loss, there have been issues with replicability of scaling results 
        under slightly different assumptions about the distribution <dt-cite key="besiroglu2024chinchilla,anwar2024foundationalchallengesassuringalignment"></dt-cite>. 
        Research has also increasingly found that many downstream capabilities display irregular scaling 
        curves <dt-cite key="srivastava2023imitation"></dt-cite> or non power-law scaling <dt-cite key="caballero2023broken"></dt-cite>. 
        For complex systems that require projecting into the future, small errors end up accumulating due to time step 
        dependencies being modelled. This makes accurate predictions of when risks will emerge inherently hard, which 
        is compounded by the small samples sizes often available for analysis. each data point is a model, and 
        computation cost means scaling <q>laws</q> are frequently based upon analysis of less than 100 data
        points <dt-cite key="ruan2024observationalscalinglawspredictability"></dt-cite>. This means many 
        reported power law relationships can lack statistical support and power <dt-cite key="powerlawtruths"></dt-cite>.</p>
      
      <p> One immediate recommendation is that the accuracy of scaling laws and predictions of emerging risk can 
        be greatly improved by more guidance from policymakers about what range is of interest and specifying the 
        risks that policymakers are concerned about  <dt-cite key="powerlawtruths"></dt-cite>. For example, 
        there is a big difference between using scaling laws to optimize for the correct amount of training data 
        in your next large-scale run versus attempting to extrapolate trends several orders of magnitude out. 
        Typically, policy use cases demand high precision over a longer time horizon, which is exactly the type of 
        extrapolation we are currently worst at. Specifying which risks are of interest will also benefit 
        precision; scaling laws tend to have high variance in precision between tasks. For example, code-generation 
        has shown fairly predictable power law scaling across 10 orders of magnitude of compute <dt-cite key="hu2024predictingemergentabilitiesinfinite,anwar2024foundational"></dt-cite>. 
        However, other capabilities have been far shown to scale far more erratically <dt-cite key="srivastava2023imitation,caballero2023broken"></dt-cite>. 
        Perhaps as important, policymakers should be aware that accurately predicting the impact of scaling is 
        currently far from feasible. Hence, there is currently limited scientific support for using exact thresholds 
        of compute alone to triage different risk levels.</p>
</div>

<script>

filterSelection("atypical") // Execute the function and show all columns
function filterSelection(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfolio_");
  y = document.getElementsByClassName("column_header_");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
}
  
filterSelection_("pie") // Execute the function and show all columns
function filterSelection_(c) {
  var x, y, z, i;
  x = document.getElementsByClassName("column_portfolio");
  y = document.getElementsByClassName("column_header");
  z = document.getElementsByClassName("column_two_fig");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
  
  for (i = 0; i < z.length; i++) {
    RemoveClass(z[i], "show");
    if (z[i].className.indexOf(c) > -1) AddClass(z[i], "show");
  }
}
  
filterSelectionfinal("thirty") // Execute the function and show all columns
function filterSelectionfinal(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfoliofinal");
  y = document.getElementsByClassName("column_headerfinal");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected

  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
 
}
  
filterSelectionfinalfinal("frequently") // Execute the function and show all columns
function filterSelectionfinalfinal(c) {
  var x, y, i;
  x = document.getElementsByClassName("column_portfoliofinalfinal");
  y = document.getElementsByClassName("column_headerfinalfinal");
  // Add the "show" class (display:block) to the filtered elements,
  // and remove the "show" class from the elements that are not selected
  for (i = 0; i < x.length; i++) {
    RemoveClass(x[i], "show");
    if (x[i].className.indexOf(c) > -1) AddClass(x[i], "show");
  }
  for (i = 0; i < y.length; i++) {
    RemoveClass(y[i], "show");
    if (y[i].className.indexOf(c) > -1) AddClass(y[i], "show");
  }
 
}
  

// Show filtered elements
function AddClass(element, name) {
  var i, arr1, arr2;
  arr1 = element.className.split(" ");
  arr2 = name.split(" ");
  for (i = 0; i < arr2.length; i++) {
    if (arr1.indexOf(arr2[i]) == -1) {
      element.className += " " + arr2[i];
    }
  }
}

// Hide elements that are not selected
function RemoveClass(element, name) {
  var i, arr1, arr2;
  arr1 = element.className.split(" ");
  arr2 = name.split(" ");
  for (i = 0; i < arr2.length; i++) {
    while (arr1.indexOf(arr2[i]) > -1) {
      arr1.splice(arr1.indexOf(arr2[i]), 1);
    }
  }
  element.className = arr1.join(" ");
}

// Add active class to the current button (highlight it)
var btnContainer1 = document.getElementById("myBtnContainer");
var btns1 = btnContainer1.getElementsByClassName("btn");
for (var i = 0; i < btns1.length; i++) {
  btns1[i].addEventListener("click", function(){
    var current1 = document.getElementsByClassName("active_1");
    current1[0].className = current1[0].className.replace(" active_1", "");
    this.className += " active_1";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer2 = document.getElementById("myBtnContainer_2");
var btns2 = btnContainer2.getElementsByClassName("btn");
for (var i = 0; i < btns2.length; i++) {
  btns2[i].addEventListener("click", function(){
    var current2 = document.getElementsByClassName("active_2");
    current2[0].className = current2[0].className.replace(" active_2", "");
    this.className += " active_2";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer3 = document.getElementById("myBtnContainer_3");
var btns3 = btnContainer3.getElementsByClassName("btn");
for (var i = 0; i < btns3.length; i++) {
  btns3[i].addEventListener("click", function(){
    var current3 = document.getElementsByClassName("active_3");
    current3[0].className = current3[0].className.replace(" active_3", "");
    this.className += " active_3";
  });
}
  
// Add active class to the current button (highlight it)
var btnContainer4 = document.getElementById("myBtnContainer_4");
var btns4 = btnContainer4.getElementsByClassName("btn");
for (var i = 0; i < btns4.length; i++) {
  btns4[i].addEventListener("click", function(){
    var current4 = document.getElementsByClassName("active_4");
    current4[0].className = current4[0].className.replace(" active_4", "");
    this.className += " active_4";
  });
}
</script> 
  </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<script src="template.v1.js"></script>
<dt-appendix>
<div class="description_">  
<h3>Acknowledgments</h3>
  
<p> A special thank you is due to ...</p>
<p> This article was in part prepared using the <a href="https://pair-code.github.io/saliency/">Google AI Pair</a> template and style guide.
   The citation management for this article uses the <a href="https://github.com/distillpub/template">template v1</a> of the Distill style script. </p>
<p>We thank the ... </p>
  
<h3>Citation</h3>
<pre class="citation long">@article{hooker2024compute,
    title={On the Limitations of Compute Thresholds as a Governance Strategy},
    author={Sara Hooker},
    year={2024},
    url={https://arxiv.org/abs/1911.05248},
    eprint={1911.05248},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

</pre>
</div>
</dt-appendix>
	
<div class="description_">  
<h3>Bibliography</h3>
</div>
<script type="text/bibliography">

@misc{ustun2024aya,
  title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}, 
  author={Ahmet Üstün and Viraat Aryabumi and Zheng-Xin Yong and Wei-Yin Ko and Daniel D'souza and Gbemileke Onilude and Neel Bhandari and Shivalika Singh and Hui-Lee Ooi and Amr Kayid and Freddie Vargus and Phil Blunsom and Shayne Longpre and Niklas Muennighoff and Marzieh Fadaee and Julia Kreutzer and Sara Hooker},
  year={2024},
  eprint={2402.07827},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@incollection{Marchant2011,
author = {Marchant, Gary E.},
title = {The Growing Gap Between Emerging Technologies and the Law},
year = {2011},
booktitle = {The Growing Gap Between Emerging Technologies and Legal-Ethical Oversight},
editor = {Marchant, Gary and Allenby, Braden and Herkert, Joseph},
series = {The International Library of Ethics, Law and Technology},
volume = {7},
publisher = {Springer},
location = {Dordrecht},
url = {https://doi.org/10.1007/978-94-007-1356-7_2},
doi = {10.1007/978-94-007-1356-7_2}
}

@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
journal = {Communications of the ACM},
volume = {60},
number = {6},
pages = {84--90},
url = {https://doi.org/10.1145/3091627},
doi = {10.1145/3091627}
}

@inproceedings{inproceedings2011,
author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year = {2011},
month = {07},
pages = {1237-1242},
title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210}
}


@misc{DettmersGPU,
author = {Tim Dettmers},
title = {Which GPU for Deep Learning in 2023?},
year = {2023},
url = {https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/},
urldate = {2023-10-04}
}

@article{BRODTKORB20134,
title = "Graphics processing unit (GPU) programming strategies and trends in GPU computing",
journal = "Journal of Parallel and Distributed Computing",
volume = "73",
number = "1",
pages = "4 - 13",
year = "2013",
note = "Metaheuristics on GPUs",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2012.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0743731512000998",
author = "André R. Brodtkorb and Trond R. Hagen and Martin L. Sætra",
keywords = "GPU computing, Heterogeneous computing, Profiling, Optimization, Debugging, Hardware, Future trends",
}


@InProceedings{Payne2005,
author="Payne, Bryson R.
and Belkasim, Saeid O.
and Owen, G. Scott
and Weeks, Michael C.
and Zhu, Ying",
editor="Sunderam, Vaidy S.
and van Albada, Geert Dick
and Sloot, Peter M. A.
and Dongarra, Jack J.",
title="Accelerated 2D Image Processing on GPUs",
booktitle="Computational Science -- ICCS 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="256--264",
isbn="978-3-540-32114-9"
}

@article{OH20041311kyoung,
title = "GPU implementation of neural networks",
journal = "Pattern Recognition",
volume = "37",
number = "6",
pages = "1311 - 1314",
year = "2004",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2004.01.013",
url = "http://www.sciencedirect.com/science/article/pii/S0031320304000524",
author = "Kyoung-Su Oh and Keechul Jung",
keywords = "Graphics processing unit(GPU), Neural network(NN), Multi-layer perceptron, Text detection",
}

@misc{Chellapilla2006,
title={High Performance Convolutional Neural Networks for Document Processing},
author={Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
year={2006},
month={10},
}

@misc{SilverBittrLesson,
author = {Sutton, Richard},
title = {The Bitter Lesson},
year = {2019},
url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
urldate = {2023-10-04}
}

@misc{heim2024governing,
    title={Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation}, 
    author={Lennart Heim and Tim Fist and Janet Egan and Sihao Huang and Stephen Zekany and Robert Trager and Michael A Osborne and Noa Zilberman},
    year={2024},
    eprint={2403.08501},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@misc{wang2023selfconsistency,
    title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
    author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
    year={2023},
    eprint={2203.11171},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{lee2024longcontext,
    title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?}, 
    author={Jinhyuk Lee and Anthony Chen and Zhuyun Dai and Dheeru Dua and Devendra Singh Sachan and Michael Boratko and Yi Luan and Sébastien M. R. Arnold and Vincent Perot and Siddharth Dalmia and Hexiang Hu and Xudong Lin and Panupong Pasupat and Aida Amini and Jeremy R. Cole and Sebastian Riedel and Iftekhar Naim and Ming-Wei Chang and Kelvin Guu},
    year={2024},
    eprint={2406.13121},
    archivePrefix={arXiv},
    primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@InProceedings{xiao2023smoothquant,
  title = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year = {2023}
}

@article{frantar-gptq,
title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
year={2022},
journal={arXiv preprint arXiv:2210.17323}
}

@inproceedings{lin2023awq,
title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
booktitle={MLSys},
year={2024}
}
@misc{huang2022large,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year={2022},
  eprint={2210.11610},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{ruan2024observationalscalinglawspredictability,
    title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
    author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
    year={2024},
    eprint={2405.10938},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2405.10938}, 
}

@article{powerlawtruths,
author = {Michael P. H. Stumpf  and Mason A. Porter },
title = {Critical Truths About Power Laws},
journal = {Science},
volume = {335},
number = {6069},
pages = {665-666},
year = {2012},
doi = {10.1126/science.1216142},
URL = {https://www.science.org/doi/abs/10.1126/science.1216142},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1216142},
abstract = {Most reported power laws lack statistical support and mechanistic backing. The ability to summarize observations using explanatory and predictive theories is the greatest strength of modern science. A theoretical framework is perceived as particularly successful if it can explain very disparate facts. The observation that some apparently complex phenomena can exhibit startling similarities to dynamics generated with simple mathematical models (1) has led to empirical searches for fundamental laws by inspecting data for qualitative agreement with the behavior of such models. A striking feature that has attracted considerable attention is the apparent ubiquity of power-law relationships in empirical data. However, although power laws have been reported in areas ranging from finance and molecular biology to geophysics and the Internet, the data are typically insufficient and the mechanistic insights are almost always too limited for the identification of power-law behavior to be scientifically useful (see the figure). Indeed, even most statistically “successful” calculations of power laws offer little more than anecdotal value.}}


@misc{wang2023voyager,
    title={Voyager: An Open-Ended Embodied Agent with Large Language Models}, 
    author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
    year={2023},
    eprint={2305.16291},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{xiong2023effective,
    title={Effective Long-Context Scaling of Foundation Models}, 
    author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
    year={2023},
    eprint={2309.16039},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{heim2023palm,
  title={Estimating PaLM's training cost},
  author={Lennart Heim},
  year={2023},
  url={https://blog.heim.xyz/palm-training-cost/},
  note={Accessed: 2023-08-10}
}

@misc{reuters_2024_export_control,
title = {U.S. Eyes Curbs on China's Access to AI Software Behind Apps Like ChatGPT},
url = {https://www.reuters.com/technology/us-eyes-curbs-chinas-access-ai-software-behind-apps-like-chatgpt-2024-05-08/},
author = {Reuters},
year = {2024},
}

@misc{abbe2021staircasepropertyhierarchicalstructure,
    title={The staircase property: How hierarchical structure can guide deep learning}, 
    author={Emmanuel Abbe and Enric Boix-Adsera and Matthew Brennan and Guy Bresler and Dheeraj Nagaraj},
    year={2021},
    eprint={2108.10573},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2108.10573}, 
}

@misc{mialon2023augmentedlanguagemodelssurvey,
    title={Augmented Language Models: a Survey}, 
    author={Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
    year={2023},
    eprint={2302.07842},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2302.07842}, 
}

@misc{wang2023voyageropenendedembodiedagent,
    title={Voyager: An Open-Ended Embodied Agent with Large Language Models}, 
    author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
    year={2023},
    eprint={2305.16291},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2305.16291}, 
}

@misc{anwar2024foundationalchallengesassuringalignment,
    title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models}, 
    author={Usman Anwar and Abulhair Saparov and Javier Rando and Daniel Paleka and Miles Turpin and Peter Hase and Ekdeep Singh Lubana and Erik Jenner and Stephen Casper and Oliver Sourbut and Benjamin L. Edelman and Zhaowei Zhang and Mario Günther and Anton Korinek and Jose Hernandez-Orallo and Lewis Hammond and Eric Bigelow and Alexander Pan and Lauro Langosco and Tomasz Korbak and Heidi Zhang and Ruiqi Zhong and Seán Ó hÉigeartaigh and Gabriel Recchia and Giulio Corsi and Alan Chan and Markus Anderljung and Lilian Edwards and Yoshua Bengio and Danqi Chen and Samuel Albanie and Tegan Maharaj and Jakob Foerster and Florian Tramer and He He and Atoosa Kasirzadeh and Yejin Choi and David Krueger},
    year={2024},
    eprint={2404.09932},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2404.09932}, 
}

@misc{schaeffer2024predictingdownstreamcapabilitiesfrontier,
    title={Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?}, 
    author={Rylan Schaeffer and Hailey Schoelkopf and Brando Miranda and Gabriel Mukobi and Varun Madan and Adam Ibrahim and Herbie Bradley and Stella Biderman and Sanmi Koyejo},
    year={2024},
    eprint={2406.04391},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2406.04391}, 
}

@misc{HR8315,
author = {U.S. House Committee on Foreign Affairs},
title = {H.R.8315 - Enhancing National Frameworks for Overseas Restriction of Critical Exports Act or ENFORCE Act.},
year = {2024},
url = {https://foreignaffairs.house.gov/wp-content/uploads/2024/05/HR-8315.pdf},
urldate = {2024-05-12}
}

@misc{openai_global_affairs,
  title = {Our Approach to Frontier Risk},
  url = {https://openai.com/global-affairs/our-approach-to-frontier-risk/},
  journal = {OpenAI},
  author = {OpenAI},
  year = {2023}
}

@misc{tirumala2023d4,
    title={D4: Improving LLM Pretraining via Document De-Duplication and Diversification}, 
    author={Kushal Tirumala and Daniel Simig and Armen Aghajanyan and Ari S. Morcos},
    year={2023},
    eprint={2308.12284},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{anderljung2023frontier,
    title={Frontier AI Regulation: Managing Emerging Risks to Public Safety}, 
    author={Markus Anderljung and Joslyn Barnhart and Anton Korinek and Jade Leung and Cullen O'Keefe and Jess Whittlestone and Shahar Avin and Miles Brundage and Justin Bullock and Duncan Cass-Beggs and Ben Chang and Tantum Collins and Tim Fist and Gillian Hadfield and Alan Hayes and Lewis Ho and Sara Hooker and Eric Horvitz and Noam Kolt and Jonas Schuett and Yonadav Shavit and Divya Siddarth and Robert Trager and Kevin Wolf},
    year={2023},
    eprint={2307.03718},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@misc{liu2024sophia,
    title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training}, 
    author={Hong Liu and Zhiyuan Li and David Hall and Percy Liang and Tengyu Ma},
    year={2024},
    eprint={2305.14342},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{hsieh2023distilling,
    title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}, 
    author={Cheng-Yu Hsieh and Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
    year={2023},
    eprint={2305.02301},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{wei2023chainofthought,
    title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
    author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
    year={2023},
    eprint={2201.11903},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{wang2019glue,
    title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
    author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
    year={2019},
    eprint={1804.07461},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@ARTICLE{591665,
author={Schaller, R.R.},
journal={IEEE Spectrum}, 
title={Moore's law: past, present and future}, 
year={1997},
volume={34},
number={6},
pages={52-59},
keywords={Moore's Law;Silicon;Technological innovation;Consumer products;Roads},
doi={10.1109/6.591665}}

@misc{anthropic_responsible_scaling,
  title = {Responsible Scaling of AI},
  url = {https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf},
  author= {Anthropic}, 
  year = {2023}
}

@misc{ca_senate_bill,
title = {Senate Bill 1047: Safe and Secure Innovation for Frontier Artificial Intelligence Models Act.},
author = {California Senate},
url = {https://legiscan.com/CA/text/SB1047/id/2999979},
journal = {Legiscan},
publisher = {Legiscan, Inc.},
year = {2024}
}

@misc{y2kreport,
  title = {Y2K Aftermath – Crisis Averted Final Committee Report},
  howpublished = {\url{https://permanent.access.gpo.gov/lps90964/y2kfinalreport.pdf}},
  year = {2000},
  note = {Accessed: <2024-05-21>}
}

@misc{greenspan1998testimony,
  author = {Greenspan, Alan},
  title = {Testimony on the Year 2000 computer problem},
  howpublished = {\url{https://www.federalreserve.gov/boarddocs/testimony/1998/19980428.htm}},
  year = {1998},
  month = {04},
  day = {28},
  note = {Accessed: <2023-08-23>}
}

@article{JMLR:v15:srivastava14a,
author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
journal = {Journal of Machine Learning Research},
year    = {2014},
volume  = {15},
number  = {56},
pages   = {1929--1958},
url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{10.1145/3620665.3640367,
author = {Davies, Michael and McDougall, Ian and Anandaraj, Selvaraj and Machchhar, Deep and Jain, Rithik and Sankaralingam, Karthikeyan},
title = {A Journey of a 1,000 Kernels Begins with a Single Step: A Retrospective of Deep Learning on GPUs},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640367},
doi = {10.1145/3620665.3640367},
abstract = {We are in age of AI, with rapidly changing algorithms and a somewhat synergistic change in hardware. MLPerf is a recent benchmark suite that serves as a way to compare and evaluate hardware. However it has several drawbacks - it is dominated by CNNs and does a poor job of capturing the diversity of AI use cases, and only represents a sliver of production AI use cases. This paper performs a longitudinal study of state-of-art AI applications spanning vision, physical simulation, vision synthesis, language and speech processing, and tabular data processing, across three generations of hardware to understand how the AI revolution has panned out. We call this collection of applications and execution scaffolding the CaSiO suite. The paper reports on data gathered at the framework level, device API level, and hardware and microarchitecture level. The paper provides insights on the hardware-software revolution with pointers to future trends.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {20–36},
numpages = {17},
location = {, La Jolla, CA, USA, },
series = {ASPLOS '24}
}



@article{hooker2021,
author = {Hooker, Sara},
title = {The hardware lottery},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3467017},
doi = {10.1145/3467017},
abstract = {After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.},
journal = {Commun. ACM},
month = {nov},
pages = {58–65},
numpages = {8}
}


@article{Engstrom2020,
author = {Engstrom, David Freeman and Ho, Daniel E. and Sharkey, Catherine M. and Cuellar, Mariano-Florentino},
title = {Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies},
year = {2020},
month = feb,
url = {https://ssrn.com/abstract=3551505},
urldate = {2020-02-01},
journal = {NYU School of Law, Public Law Research Paper},
doi = {10.2139/ssrn.3551505}
}

@article{Aitken2022,
author = {Aitken, Murray and Leslie, David and Ostmann, Fabian and Pratt, Joseph and Margetts, Helen and Dorobantu, Cristina},
title = {Common Regulatory Capacity for {AI}},
year = {2022},
url = {https://doi.org/10.5281/zenodo.6838946},
journal = {The Alan Turing Institute},
doi = {10.5281/zenodo.6838946}
}

@misc{NISTAISafetyLab,
author = {Cat Zakrzewski},
title = {The nation’s top AI safety lab is decaying from within, scientists say},
year = {2024},
url = {https://www.washingtonpost.com/technology/2024/03/06/nist-ai-safety-lab-decaying/},
journal = {Washington Post}
}

@article{Seri2008,
author = {Seri, Irit and Evans, Jonathan},
year = {2008},
title = {Limits of viability: definition of the gray zone},
journal = {Journal of Perinatology},
volume = {28 Suppl 1},
pages = {S4--8},
publisher = {Nature Publishing Group},
doi = {10.1038/jp.2008.42},
pmid = {18446176},
month = may
}

@article{adamic2001,
author = {Adamic, Lada and Huberman, Bernardo},
year = {2001},
month = {11},
pages = {},
title = {Zipf's Law and the Internet},
volume = {3},
journal = {Glottometrics}
}

@article{DeWitt2024,
author = {DeWitt, Tristan D. and Garrett, Timothy J. and Rees, Katherine N. and Bois, Christophe and Krueger, Scott K. and Ferlay, Nicolas},
year = {2024},
title = {Climatologically invariant scale invariance seen in distributions of cloud horizontal sizes},
journal = {Atmospheric Chemistry and Physics},
volume = {24},
pages = {109--122},
doi = {10.5194/acp-24-109-2024}
}

@book{1929fundamento,
title={Fundamento de esperanto. Fundament van esperanto},
url={https://books.google.com/books?id=yD9n0AEACAAJ},
year={1929},
publisher={Belga Esperanto-Instituto}
}


@article{Piantadosi2014,
author = {Piantadosi, Steven T.},
year = {2014},
title = {Zipf's word frequency law in natural language: a critical review and future directions},
journal = {Psychonomic Bulletin \& Review},
volume = {21},
number = {5},
pages = {1112--30},
doi = {10.3758/s13423-014-0585-6}
}

@article{Wynants2019,
author = {Wynants, Lars and van Smeden, Manuel and McLernon, David J. and Timmerman, Daan and Steyerberg, Ewout W. and Van Calster, Ben},
year = {2019},
title = {Three myths about risk thresholds for prediction models},
journal = {BMC Medicine},
volume = {17},
number = {1},
pages = {192},
publisher = {BMC},
doi = {10.1186/s12916-019-1425-3},
pmid = {31651317},
pmcid = {PMC6814132}
}

@article{Cutland2017,
author = {Cutland, Claire L. and Lackritz, Emily M. and Mallett-Moore, Tanis and Bardají, Anna and Chandrasekaran, Ramakrishnan and Lahariya, Cagri and Nisar, Muhammad Imran and Tapia, Martin D. and Pathirana, Jay and Kochhar, Sumita and Muñoz, Francisco M. and Brighton Collaboration Low Birth Weight Working Group},
year = {2017},
title = {Low birth weight: Case definition \& guidelines for data collection, analysis, and presentation of maternal immunization safety data},
journal = {Vaccine},
volume = {35},
number = {48 Pt A},
pages = {6492--6500},
publisher = {Elsevier {BV}},
doi = {10.1016/j.vaccine.2017.01.049}
}

@online{SheehanTweet,
author = {Matt Sheehan},
title = {AI is eating the world},
year = 2022,
url = {https://x.com/mattsheehan88/status/1770902104795729936},
urldate = {2023-10-04}
}


@article{Saudek2008,
  author = {Saudek, Christopher D. and Herman, William H. and Sacks, David B. and Bergenstal, Richard M. and Edelman, David and Davidson, Mayer B.},
  title = {A New Look at Screening and Diagnosing Diabetes Mellitus},
  journal = {The Journal of Clinical Endocrinology},
  volume = {93},
  number = {7},
  pages = {2447-2453},
  year = {2008},
  month = {07},
  issn = {0021-972X},
  doi = {10.1210/jc.2007-2174},
  url = {https://doi.org/10.1210/jc.2007-2174}
}
@inproceedings{pfeiffer2022lifting,
title        = {Lifting the Curse of Multilinguality by Pre-training Modular Transformers},
author       = {Pfeiffer, Jonas and Goyal, Naman and Lin, Xi and Li, Xian and Cross, James and Riedel, Sebastian and Artetxe, Mikel},
year         = 2022,
month        = jul,
booktitle    = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
publisher    = {Association for Computational Linguistics},
address      = {Seattle, United States},
pages        = {3479--3495},
doi          = {10.18653/v1/2022.naacl-main.255},
url          = {https://aclanthology.org/2022.naacl-main.255}
}
@article{conneau2019unsupervised,
title        = {Unsupervised Cross-lingual Representation Learning at Scale},
author       = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
year         = 2019,
month        = jul,
booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
publisher    = {Association for Computational Linguistics},
address      = {Online},
pages        = {8440--8451},
doi          = {10.18653/v1/2020.acl-main.747},
url          = {https://aclanthology.org/2020.acl-main.747}
}

@misc{BidenAIExecutiveOrder,
author = {Karen Hao, Matteo Wong},
title = {The White House Is Preparing for an AI-Dominated Future},
year = {2023},
url = {https://www.theatlantic.com/technology/archive/2023/10/biden-white-house-ai-executive-order/675837/},
journal = {The Atlantic}
}

@article{HerculanoHouzel2014TheEB,
title={The elephant brain in numbers},
author={Suzana Herculano-Houzel and Kamilla Avelino-de-Souza and Kleber Neves and Jairo Porfirio and D{\'e}bora J. Messeder and Larissa Mattos Feij{\'o} and Jose Maldonado and Paul R Manger},
journal={Frontiers in Neuroanatomy},
year={2014},
volume={8}
}

@ARTICLE{Sackinger129422,
author={E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal={IEEE Transactions on Neural Networks}, 
title={Application of the ANNA neural network chip to high-speed character recognition}, 
year={1992},
volume={3},
number={3},
pages={498-505},}

@article{Stroop1935,
pages = {643},
doi = {10.1037/h0054651},
title = {Studies of Interference in Serial Verbal Reactions},
number = {6},
journal = {Journal of Experimental Psychology},
year = {1935},
volume = {18},
author = {J. R. Stroop}
}

@ARTICLE{2020arXiv200705558T,
author = {{Thompson}, Neil C. and {Greenewald}, Kristjan and {Lee}, Keeheon and
{Manso}, Gabriel F.},
title = "{The Computational Limits of Deep Learning}",
journal = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
year = 2020,
month = jul,
eid = {arXiv:2007.05558},
pages = {arXiv:2007.05558},
archivePrefix = {arXiv},
eprint = {2007.05558},
primaryClass = {cs.LG},
adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200705558T},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{macia2014,
author = {Macía, Javier and Sole, Ricard},
year = {2014},
month = {02},
pages = {e81248},
title = {How to Make a Synthetic Multicellular Computer},
volume = {9},
journal = {PloS one},
doi = {10.1371/journal.pone.0081248}
}

@article {Kriegman1853,
author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
title = {A scalable pipeline for designing reconfigurable organisms},
volume = {117},
number = {4},
pages = {1853--1859},
year = {2020},
doi = {10.1073/pnas.1910837117},
publisher = {National Academy of Sciences},
issn = {0027-8424},
URL = {https://www.pnas.org/content/117/4/1853},
eprint = {https://www.pnas.org/content/117/4/1853.full.pdf},
journal = {Proceedings of the National Academy of Sciences}
}

@article {Heeger1773,
author = {Heeger, David J.},
title = {Theory of cortical function},
volume = {114},
number = {8},
pages = {1773--1782},
year = {2017},
doi = {10.1073/pnas.1619788114},
publisher = {National Academy of Sciences},
issn = {0027-8424},
URL = {https://www.pnas.org/content/114/8/1773},
eprint = {https://www.pnas.org/content/114/8/1773.full.pdf},
journal = {Proceedings of the National Academy of Sciences}
}

@INPROCEEDINGS{8741810,
author={M. {Davies}},
booktitle={2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)}, 
title={Progress in Neuromorphic Computing : Drawing Inspiration from Nature for Gains in AI and Computing}, 
year={2019},
volume={},
number={},
pages={1-1},}


@misc{bremner2013,
author = {Bremner, Andrew and Lewkowicz, David and Spence, Charles},
year = {2013},
month = {11},
pages = {},
title = {Multisensory Development},
doi = {10.1093/acprof:oso/9780199586059.003.0001}
}

@article{benna2016,
author = {Benna, Marcus and Fusi, Stefano},
year = {2016},
month = {10},
pages = {},
title = {Computational principles of synaptic memory consolidation},
volume = {19},
journal = {Nature Neuroscience},
doi = {10.1038/nn.4401}
}


@ARTICLE{2020Thompson,
author = {{Thompson}, Neil C. and {Greenewald}, Kristjan and {Lee}, Keeheon and
{Manso}, Gabriel F.},
title = "{The Computational Limits of Deep Learning}",
journal = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
year = 2020,
month = jul,
eid = {arXiv:2007.05558},
pages = {arXiv:2007.05558},
archivePrefix = {arXiv},
eprint = {2007.05558},
primaryClass = {cs.LG},
adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200705558T},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{bubic2010,
AUTHOR={Bubic, Andreja and Von Cramon, D. Yves and Schubotz, Ricarda},   
TITLE={Prediction, cognition and the brain},      
JOURNAL={Frontiers in Human Neuroscience},      
VOLUME={4},      
PAGES={25},     
YEAR={2010},      
URL={https://www.frontiersin.org/article/10.3389/fnhum.2010.00025},       
DOI={10.3389/fnhum.2010.00025},      
ISSN={1662-5161},   
}

@inproceedings{2019barham,
author = {Barham, Paul and Isard, Michael},
title = {Machine Learning Systems Are Stuck in a Rut},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321441},
doi = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {177–183},
numpages = {7},
location = {Bertinoro, Italy},
series = {HotOS ’19}
}


@article{PMID15632230,
Title = {How the brain decides what we see},
Author = {Smythies, John},
DOI = {10.1177/014107680509800106},
Number = {1},
Volume = {98},
Month = {January},
Year = {2005},
Journal = {Journal of the Royal Society of Medicine},
ISSN = {0141-0768},
Pages = {18—20},
URL = {https://europepmc.org/articles/PMC1079232},
}

@article{Kennedy2000SignalprocessingMA,
title={Signal-processing machines at the postsynaptic density.},
author={Mary B. Kennedy},
journal={Science},
year={2000},
volume={290 5492},
pages={
750-4
}
}

@ARTICLE{hooker2019,
author = {{Hooker}, Sara and {Courville}, Aaron and {Clark}, Gregory and
{Dauphin}, Yann and {Frome}, Andrea},
title = "{What Do Compressed Deep Neural Networks Forget?}",
journal = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year = 2019,
month = nov,
eid = {arXiv:1911.05248},
pages = {arXiv:1911.05248},
archivePrefix = {arXiv},
eprint = {1911.05248},
primaryClass = {cs.LG},
adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{ustun2024aya,
    title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}, 
    author={Ahmet Üstün and Viraat Aryabumi and Zheng-Xin Yong and Wei-Yin Ko and Daniel D'souza and Gbemileke Onilude and Neel Bhandari and Shivalika Singh and Hui-Lee Ooi and Amr Kayid and Freddie Vargus and Phil Blunsom and Shayne Longpre and Niklas Muennighoff and Marzieh Fadaee and Julia Kreutzer and Sara Hooker},
    year={2024},
    eprint={2402.07827},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{ayadata2024,
    title={Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning}, 
    author={Shivalika Singh and Freddie Vargus and Daniel Dsouza and Börje F. Karlsson and Abinaya Mahendiran and Wei-Yin Ko and Herumb Shandilya and Jay Patel and Deividas Mataciunas and Laura OMahony and Mike Zhang and Ramith Hettiarachchi and Joseph Wilson and Marina Machado and Luisa Souza Moura and Dominik Krzemiński and Hakimeh Fadaei and Irem Ergün and Ifeoma Okoh and Aisha Alaagib and Oshan Mudannayake and Zaid Alyafeai and Vu Minh Chien and Sebastian Ruder and Surya Guthikonda and Emad A. Alghamdi and Sebastian Gehrmann and Niklas Muennighoff and Max Bartolo and Julia Kreutzer and Ahmet Üstün and Marzieh Fadaee and Sara Hooker},
    year={2024},
    journal={arXiv preprint arXiv:2402.06619},
}

@misc{geminiteam2024gemini,
    title={Gemini: A Family of Highly Capable Multimodal Models}, 
    author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac et al.},
    year={2024},
    eprint={2312.11805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{gemmareport,
    title={Gemma: Open Models Based on Gemini Research and Technology}, 
    author={Gemma-Team},
    year={2024},
}

@misc{jiang2023mistral,
    title={Mistral 7B}, 
    author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
    year={2023},
    eprint={2310.06825},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@online{DatabricksBlog2023DollyV2,
  author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year      = {2023},
  url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate   = {2023-06-30}
}

@article{bandarkar2023belebele,
    title={The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants}, 
    author={Lucas Bandarkar and Davis Liang and Benjamin Muller and Mikel Artetxe and Satya Narayan Shukla and Donald Husa and Naman Goyal and Abhinandan Krishnan and Luke Zettlemoyer and Madian Khabsa},
    year={2023},
    journal={arXiv preprint arXiv:2308.16884}
}

@article{team2024gemma,
title={Gemma: Open models based on gemini research and technology},
author={Gemini Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
journal={arXiv preprint arXiv:2403.08295},
year={2024}
}

@article{jiang2023mistral,
title={Mistral 7B},
author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
journal={arXiv preprint arXiv:2310.06825},
year={2023}
}

@article{khanuja2021muril,
    title={MuRIL: Multilingual Representations for Indian Languages}, 
    author={Simran Khanuja and Diksha Bansal and Sarvesh Mehtani and Savya Khosla and Atreyee Dey and Balaji Gopalan and Dilip Kumar Margam and Pooja Aggarwal and Rajiv Teja Nagipogu and Shachi Dave and Shruti Gupta and Subhash Chandra Bose Gali and Vish Subramanian and Partha Talukdar},
    year={2021},
    eprint={2103.10730},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
  oladipo-etal-2023-better,
  title = "Better Quality Pre-training Data and T5 Models for {A}frican Languages",
  author = "Oladipo, Akintunde  and
    Adeyemi, Mofetoluwa  and
    Ahia, Orevaoghene  and
    Owodunni, Abraham  and
    Ogundepo, Odunayo  and
    Adelani, David  and
    Lin, Jimmy",
  editor = "Bouamor, Houda  and
    Pino, Juan  and
    Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.emnlp-main.11",
  pages = "158--168",
  abstract = "In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. To compile this dataset, we undertake a rigorous examination of current data sources for thirteen languages within one of the most extensive multilingual web crawls, mC4, and extract cleaner data through meticulous auditing and improved web crawling strategies. Subsequently, we pretrain a new T5-based model on this dataset and evaluate its performance on multiple downstream tasks. Our model demonstrates better downstream effectiveness over existing pretrained models across four NLP tasks, underscoring the critical role data quality plays in pretraining language models in low-resource scenarios. Specifically, on cross-lingual QA evaluation, our new model is more than twice as effective as multilingual T5. All code, data and models are publicly available at https://github.com/castorini/AfriTeVa-keji.",
}



@article{Treviso2023,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  volume = {abs/https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}


@article{gale2019state,
    title={The State of Sparsity in Deep Neural Networks}, 
    author={Trevor Gale and Erich Elsen and Sara Hooker},
    year={2019},
    eprint={1902.09574},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Nakamura2023,
author = {Nakamura, Gabriel and Soares, Bruno and Pillar, Valério and Diniz-Filho, José and Duarte, Leandro},
year = {2023},
month = {08},
pages = {},
title = {Three pathways to better recognize the expertise of Global South researchers},
journal = {npj Biodiversity},
doi = {10.1038/s44185-023-00021-7}
}

@article{Park2023PapersAP,
title={Papers and patents are becoming less disruptive over time},
author={Michael Park and Erin Leahey and Russell J. Funk},
journal={Nature},
year={2023},
volume={613},
pages={138-144},
url={https://api.semanticscholar.org/CorpusID:255466666}
}

@article{zeng2021pangu,
title        = {PanGu-$\alpha$: Large-scale autoregressive pretrained Chinese language models with auto-parallel computation},
author       = {Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
year         = 2021,
journal      = {arXiv preprint arXiv:2104.12369}
}


@article{srivastava2022beyond,
title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
journal={arXiv preprint arXiv:2206.04615},
year={2022}
}

@article{paul2023deep,
    title={Deep Learning on a Data Diet: Finding Important Examples Early in Training}, 
    author={Mansheej Paul and Surya Ganguli and Gintare Karolina Dziugaite},
    year={2023},
    eprint={2107.07075},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{wenzek2019ccnet,
    title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data}, 
    author={Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzmán and Armand Joulin and Edouard Grave},
    year={2019},
    eprint={1911.00359},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{chen2024monolingual,
    title={Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca}, 
    author={Pinzhen Chen and Shaoxiong Ji and Nikolay Bogoychev and Andrey Kutuzov and Barry Haddow and Kenneth Heafield},
    year={2024},
    eprint={2309.08958},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{akiki2022bigscience,
title={BigScience: A case study in the social construction of a multilingual large language model},
author={Akiki, Christopher and Pistilli, Giada and Mieskes, Margot and Gall{\'e}, Matthias and Wolf, Thomas and Ili{\'c}, Suzana and Jernite, Yacine},
journal={arXiv preprint arXiv:2212.04960},
year={2022}
}

@article{laurenccon2022bigscience,
title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
journal={Advances in Neural Information Processing Systems},
volume={35},
pages={31809--31826},
year={2022}
}

@article{workshop2022bloom,
title={Bloom: A 176b-parameter open-access multilingual language model},
author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
journal={arXiv preprint arXiv:2211.05100},
year={2022}
}

@article{kwiatkowski-etal-2019-natural,
  title = "Natural Questions: A Benchmark for Question Answering Research",
  author = "Kwiatkowski, Tom  and
    Palomaki, Jennimaria  and
    Redfield, Olivia  and
    Collins, Michael  and
    Parikh, Ankur  and
    Alberti, Chris  and
    Epstein, Danielle  and
    Polosukhin, Illia  and
    Devlin, Jacob  and
    Lee, Kenton  and
    Toutanova, Kristina  and
    Jones, Llion  and
    Kelcey, Matthew  and
    Chang, Ming-Wei  and
    Dai, Andrew M.  and
    Uszkoreit, Jakob  and
    Le, Quoc  and
    Petrov, Slav",
  editor = "Lee, Lillian  and
    Johnson, Mark  and
    Roark, Brian  and
    Nenkova, Ani",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "7",
  year = "2019",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/Q19-1026",
  doi = "10.1162/tacl_a_00276",
  pages = "452--466",
  abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@article{frantar-sparsegpt,
title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
author={Elias Frantar and Dan Alistarh},
year={2023},
journal={arXiv preprint arXiv:2301.00774}
}

@article{frantar2022gptq,
title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
journal={arXiv preprint arXiv:2210.17323},
year={2022}
}

@article{arivazhagan2019massively,
title={Massively multilingual neural machine translation in the wild: Findings and challenges},
author={Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George and Cherry, Colin and others},
journal={arXiv preprint arXiv:1907.05019},
year={2019}
}

@misc{niu2024scaling,
    title={Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory}, 
    author={Xueyan Niu and Bo Bai and Lei Deng and Wei Han},
    year={2024},
    eprint={2405.08707},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lohn2022ai,
  title={AI and Compute: How Much Longer Can Computing Power Drive Artificial Intelligence Progress?},
  author={Lohn, Andrew J. and Musser, Micah},
  year={2022},
  month={January},
  publisher={Center for Security and Emerging Technology},
  url={https://doi.org/10.51593/2021CA009}
}

@article{Vaswani2017,
author       = {Ashish Vaswani and
                Noam Shazeer and
                Niki Parmar and
                Jakob Uszkoreit and
                Llion Jones and
                Aidan N. Gomez and
                Lukasz Kaiser and
                Illia Polosukhin},
title        = {Attention Is All You Need},
journal      = {CoRR},
volume       = {abs/1706.03762},
year         = {2017},
url          = {http://arxiv.org/abs/1706.03762},
eprinttype    = {arXiv},
eprint       = {1706.03762},
timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
ahmadian2023intriguing,
title={Intriguing Properties of Quantization at Scale},
author={Arash Ahmadian and Saurabh Dash and Hongyu Chen and Bharat Venkitesh and Zhen Stephen Gou and Phil Blunsom and Ahmet {\"U}st{\"u}n and Sara Hooker},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=IYe8j7Gy8f}
}

@inproceedings{Sevilla_2022,
 title={Compute Trends Across Three Eras of Machine Learning},
 url={http://dx.doi.org/10.1109/IJCNN55064.2022.9891914},
 DOI={10.1109/ijcnn55064.2022.9891914},
 booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
 publisher={IEEE},
 author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
 year={2022},
 month=jul }


@misc{tay2022scaling,
    title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?}, 
    author={Yi Tay and Mostafa Dehghani and Samira Abnar and Hyung Won Chung and William Fedus and Jinfeng Rao and Sharan Narang and Vinh Q. Tran and Dani Yogatama and Donald Metzler},
    year={2022},
    eprint={2207.10551},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{METR_undated,
title = {Elicitation Gap},
url = {https://metr.github.io/autonomy-evals-guide/elicitation-gap/},
author = {{METR Team}},
urldate = {2023-09-06}
}

@article{dettmers2022llm,
title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
journal={arXiv preprint arXiv:2208.07339},
year={2022}
}


@article{mesham2021low,
title={Low-resource language modelling of South African languages},
author={Mesham, Stuart and Hayward, Luc and Shapiro, Jared and Buys, Jan},
journal={arXiv preprint arXiv:2104.00772},
year={2021}
}

@misc{zolkepli2024mallam,
    title={MaLLaM -- Malaysia Large Language Model}, 
    author={Husein Zolkepli and Aisyah Razak and Kamarul Adha and Ariff Nazhan},
    year={2024},
    eprint={2401.14680},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{PhoGPT,
title     = {{PhoGPT: Generative Pre-training for Vietnamese}},
author    = {Dat Quoc Nguyen and Linh The Nguyen and Chi Tran and Dung Ngoc Nguyen and Dinh Phung and Hung Bui},
journal   = {arXiv preprint},
volume    = {arXiv:2311.02945},
year      = {2023}
}

@misc{sea_lion_2023,
title={SEA-LION (Southeast Asian Languages In One Network): A Family of Large Language Models for Southeast Asia},
author={AI Singapore},
year={2023},
howpublished={\url{https://github.com/aisingapore/sealion}}
}

@article{jiang2024mixtral,
title={Mixtral of experts},
author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
journal={arXiv preprint arXiv:2401.04088},
year={2024}
}

@inproceedings{2014Horowitz,
  title          = {{1.1 Computing's energy problem (and what we can do about it)}},
  author         = {Horowitz, Mark},
  year           = {2014},
  booktitle      = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  pages          = {10--14}
}

@InProceedings{Zhang_2020_CVPR,
author = {Zhang, Zizhao and Zhang, Han and Arik, Sercan O. and Lee, Honglak and Pfister, Tomas},
title = {Distilling Effective Supervision From Severe Label Noise},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}
@misc{agarwal2020estimating,
    title={Estimating Example Difficulty Using Variance of Gradients}, 
    author={Chirag Agarwal and Sara Hooker},
    year={2020},
    eprint={2008.11600},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{Baldock2021DeepLT,
title={Deep Learning Through the Lens of Example Difficulty},
author={R. Baldock and Hartmut Maennel and Behnam Neyshabur},
journal={ArXiv},
year={2021},
volume={abs/2106.09647}
}

@article{erfani2016high,
title={High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning},
author={Erfani, Sarah M and Rajasegarar, Sutharshan and Karunasekera, Shanika and Leckie, Christopher},
journal={Pattern Recognition},
volume={58},
pages={121--134},
year={2016},
publisher={Elsevier}
}
@article{parzen1962estimation,
title={On estimation of a probability density function and mode},
author={Parzen, Emanuel},
journal={The annals of mathematical statistics},
volume={33},
number={3},
pages={1065--1076},
year={1962},
publisher={JSTOR}
}

@inproceedings{pan-etal-2017-cross,
  title = "Cross-lingual Name Tagging and Linking for 282 Languages",
  author = "Pan, Xiaoman  and
    Zhang, Boliang  and
    May, Jonathan  and
    Nothman, Joel  and
    Knight, Kevin  and
    Ji, Heng",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2017",
  address = "Vancouver, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P17-1178",
  doi = "10.18653/v1/P17-1178",
  pages = "1946--1958",
  abstract = "The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",
}

@inproceedings{conneau-etal-2020-unsupervised,
  title = "Unsupervised Cross-lingual Representation Learning at Scale",
  author = "Conneau, Alexis  and
    Khandelwal, Kartikay  and
    Goyal, Naman  and
    Chaudhary, Vishrav  and
    Wenzek, Guillaume  and
    Guzm{\'a}n, Francisco  and
    Grave, Edouard  and
    Ott, Myle  and
    Zettlemoyer, Luke  and
    Stoyanov, Veselin",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.747",
  doi = "10.18653/v1/2020.acl-main.747",
  pages = "8440--8451",
  abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{%} average accuracy on XNLI, +13{%} average F1 score on MLQA, and +2.4{%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{%} in XNLI accuracy for Swahili and 11.4{%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{eiselen-2016-government,
  title = "Government Domain Named Entity Recognition for {S}outh {A}frican Languages",
  author = "Eiselen, Roald",
  booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
  month = may,
  year = "2016",
  address = "Portoro{\v{z}}, Slovenia",
  publisher = "European Language Resources Association (ELRA)",
  url = "https://aclanthology.org/L16-1533",
  pages = "3344--3348",
  abstract = "This paper describes the named entity language resources developed as part of a development project for the South African languages. The development efforts focused on creating protocols and annotated data sets with at least 15,000 annotated named entity tokens for ten of the official South African languages. The description of the protocols and annotated data sets provide an overview of the problems encountered during the annotation of the data sets. Based on these annotated data sets, CRF named entity recognition systems are developed that leverage existing linguistic resources. The newly created named entity recognisers are evaluated, with F-scores of between 0.64 and 0.77, and error analysis is performed to identify possible avenues for improving the quality of the systems.",
}


@article{adelani-etal-2021-masakhaner,
  title = "{M}asakha{NER}: Named Entity Recognition for {A}frican Languages",
  author = "Adelani, David Ifeoluwa  and
    Abbott, Jade  and
    Neubig, Graham  and
    D{'}souza, Daniel  and
    Kreutzer, Julia  and
    Lignos, Constantine  and
    Palen-Michel, Chester  and
    Buzaaba, Happy  and
    Rijhwani, Shruti  and
    Ruder, Sebastian  and
    Mayhew, Stephen  and
    Azime, Israel Abebe  and
    Muhammad, Shamsuddeen H.  and
    Emezue, Chris Chinenye  and
    Nakatumba-Nabende, Joyce  and
    Ogayo, Perez  and
    Anuoluwapo, Aremu  and
    Gitau, Catherine  and
    Mbaye, Derguene  and
    Alabi, Jesujoba  and
    Yimam, Seid Muhie  and
    Gwadabe, Tajuddeen Rabiu  and
    Ezeani, Ignatius  and
    Niyongabo, Rubungo Andre  and
    Mukiibi, Jonathan  and
    Otiende, Verrah  and
    Orife, Iroro  and
    David, Davis  and
    Ngom, Samba  and
    Adewumi, Tosin  and
    Rayson, Paul  and
    Adeyemi, Mofetoluwa  and
    Muriuki, Gerald  and
    Anebi, Emmanuel  and
    Chukwuneke, Chiamaka  and
    Odu, Nkiruka  and
    Wairagala, Eric Peter  and
    Oyerinde, Samuel  and
    Siro, Clemencia  and
    Bateesa, Tobius Saul  and
    Oloyede, Temilola  and
    Wambui, Yvonne  and
    Akinode, Victor  and
    Nabagereka, Deborah  and
    Katusiime, Maurice  and
    Awokoya, Ayodele  and
    MBOUP, Mouhamadane  and
    Gebreyohannes, Dibora  and
    Tilaye, Henok  and
    Nwaike, Kelechi  and
    Wolde, Degaga  and
    Faye, Abdoulaye  and
    Sibanda, Blessing  and
    Ahia, Orevaoghene  and
    Dossou, Bonaventure F. P.  and
    Ogueji, Kelechi  and
    DIOP, Thierno Ibrahima  and
    Diallo, Abdoulaye  and
    Akinfaderin, Adewale  and
    Marengereke, Tendai  and
    Osei, Salomey",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "9",
  year = "2021",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/2021.tacl-1.66",
  doi = "10.1162/tacl_a_00416",
  pages = "1116--1131",
  abstract = "We take a step towards addressing the under- representation of the African continent in NLP research by bringing together different stakeholders to create the first large, publicly available, high-quality dataset for named entity recognition (NER) in ten African languages. We detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for NER tasks. We analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. Finally, we release the data, code, and models to inspire future research on African NLP.1",
}

@misc{MIT_Technology_Review_2023,
title = {China is limiting children to one hour of online gaming a day},
url = {https://www.technologyreview.com/2023/08/09/1077567/china-children-screen-time-regulation/},
author = {{MIT Technology Review}},
year = {2023},
month = {Aug},
urldate = {2023-09-06}
}

@misc{who_undated,
title = {Global Health Observatory (GHO) - Data},
url = {https://apps.who.int/gho/data/view.main.54600},
author = {{World Health Organization}},
urldate = {2023-09-06}
}

@misc{US_Department_of_Transportation_undated,
title = {Chapter 6: Government Roles and Related Considerations},
url = {https://highways.dot.gov/safety/speed-management/speed-concepts-informational-guide/chapter-6-government-roles-related},
author = {{US Department of Transportation}},
urldate = {2023-09-06},
year={2020}
}

@article{Achille2017CriticalLP,
title={Critical Learning Periods in Deep Neural Networks},
author={Alessandro Achille and Matteo Rovere and Stefano Soatto},
journal={ArXiv},
year={2017},
volume={abs/1711.08856}
}

@misc{zadouri2023pushing,
    title={Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning}, 
    author={Ted Zadouri and Ahmet Üstün and Arash Ahmadian and Beyza Ermiş and Acyr Locatelli and Sara Hooker},
    year={2023},
    eprint={2309.05444},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@ARTICLE{2020fartash,
     author = {{Faghri}, Fartash and {Duvenaud}, David and {Fleet}, David J. and
       {Ba}, Jimmy},
      title = "{A Study of Gradient Variance in Deep Learning}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = 2020,
      month = jul,
        eid = {arXiv:2007.04532},
      pages = {arXiv:2007.04532},
archivePrefix = {arXiv},
     eprint = {2007.04532},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200704532F},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{Mangalam2019DoDN,
title={Do deep neural networks learn shallow learnable examples first},
author={Karttikeya Mangalam and Vinay Uday Prabhu},
year={2019}
}


@article{jiang2020exploring,
title={Exploring the Memorization-Generalization Continuum in Deep Learning},
author={Jiang, Ziheng and Zhang, Chiyuan and Talwar, Kunal and Mozer, Michael C},
journal={arXiv preprint arXiv:2002.03206},
year={2020}
}


@inproceedings{see-etal-2016-compression,
  title = "Compression of Neural Machine Translation Models via Pruning",
  author = "See, Abigail  and
    Luong, Minh-Thang  and
    Manning, Christopher D.",
  booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
  month = aug,
  year = "2016",
  address = "Berlin, Germany",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/K16-1029",
  doi = "10.18653/v1/K16-1029",
  pages = "291--301",
}

@article{treviso2023,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}



@article{deghani2021,
author       = {Mostafa Dehghani and
                Anurag Arnab and
                Lucas Beyer and
                Ashish Vaswani and
                Yi Tay},
title        = {The Efficiency Misnomer},
journal      = {CoRR},
volume       = {abs/2110.12894},
year         = {2021},
url          = {https://arxiv.org/abs/2110.12894},
eprinttype    = {arXiv},
eprint       = {2110.12894},
timestamp    = {Thu, 28 Oct 2021 15:25:31 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2110-12894.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dai-adel-2020-analysis,
  title = "An Analysis of Simple Data Augmentation for Named Entity Recognition",
  author = "Dai, Xiang  and
    Adel, Heike",
  booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
  month = dec,
  year = "2020",
  address = "Barcelona, Spain (Online)",
  publisher = "International Committee on Computational Linguistics",
  url = "https://aclanthology.org/2020.coling-main.343",
  doi = "10.18653/v1/2020.coling-main.343",
  pages = "3861--3867",
  abstract = "Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare data augmentation for named entity recognition, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (i2b2-2010 and MaSciP), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.",
}

@inproceedings{rahimi-etal-2019-massively,
  title = "Massively Multilingual Transfer for {NER}",
  author = "Rahimi, Afshin  and
    Li, Yuan  and
    Cohn, Trevor",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1015",
  doi = "10.18653/v1/P19-1015",
  pages = "151--164",
  abstract = "In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a {``}massive{''} setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.",
}

@inproceedings{wu2022sustainable,
author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},
booktitle = {Proceedings of Machine Learning and Systems},
editor = {D. Marculescu and Y. Chi and C. Wu},
pages = {795--813},
title = {{Sustainable AI: Environmental Implications, Challenges and Opportunities}},
url = {https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf},
volume = {4},
year = {2022}
}
@inproceedings{Strubell:2019,
  title = "{Energy and Policy Considerations for Deep Learning in {NLP}}",
  author = "Strubell, Emma  and
    Ganesh, Ananya  and
    McCallum, Andrew",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1355",
  doi = "10.18653/v1/P19-1355",
  pages = "3645--3650",
}

@article{Schwartz:2020,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green {AI}},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
journal = {Communications of the ACM (CACM)},
month = nov,
pages = {54-63},
numpages = {10}
}

@article{thompson2020computational,
title={{The Computational Limits of Deep Learning}},
author={Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
journal={arXiv preprint arXiv:2007.05558v1},
url={https://arxiv.org/abs/2007.05558v1},
year={2020}
}

@article{ahmed2020dedemocratization,
title={{The de-democratization of AI: Deep learning and the compute divide in artificial intelligence research}},
author={Ahmed, Nur and Wahed, Muntasir},
journal={arXiv preprint arXiv:2010.15581v1},
year={2020},
url={https://arxiv.org/abs/2010.15581v1}
}

@misc{marion2023more,
    title={When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale}, 
    author={Max Marion and Ahmet Üstün and Luiza Pozzobon and Alex Wang and Marzieh Fadaee and Sara Hooker},
    year={2023},
    eprint={2309.04564},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Sambasivan2021,
author = {Sambasivan, Nithya and Arnesen, Erin and Hutchinson, Ben and Doshi, Tulsee and Prabhakaran, Vinodkumar},
title = {Re-Imagining Algorithmic Fairness in India and Beyond},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445896},
doi = {10.1145/3442188.3445896},
abstract = {Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {315–328},
numpages = {14},
keywords = {ability, India, critical algorithmic studies, religion, gender, class, feminism, caste, decoloniality, anti-caste politics, algorithmic fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@misc{prabhakaran2021releasing,
    title={On Releasing Annotator-Level Labels and Information in Datasets}, 
    author={Vinodkumar Prabhakaran and Aida Mostafazadeh Davani and Mark Díaz},
    year={2021},
    eprint={2110.05699},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{longpre2023data,
    title={The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI}, 
    author={Shayne Longpre and Robert Mahari and Anthony Chen and Naana Obeng-Marnu and Damien Sileo and William Brannon and Niklas Muennighoff and Nathan Khazam and Jad Kabbara and Kartik Perisetla and Xinyi Wu and Enrico Shippole and Kurt Bollacker and Tongshuang Wu and Luis Villa and Sandy Pentland and Sara Hooker},
    year={2023},
    eprint={2310.16787},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{lignos-etal-2022-toward,
  title = "Toward More Meaningful Resources for Lower-resourced Languages",
  author = {Lignos, Constantine  and
    Holley, Nolan  and
    Palen-Michel, Chester  and
    S{\"a}lev{\"a}, Jonne},
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.findings-acl.44",
  doi = "10.18653/v1/2022.findings-acl.44",
  pages = "523--532",
  abstract = "In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.",
}

@misc{sattigeri2018fairness,
    title={Fairness GAN}, 
    author={Prasanna Sattigeri and Samuel C. Hoffman and Vijil Chenthamarakshan and Kush R. Varshney},
    year={2018},
    eprint={1805.09910},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{caballero2023broken,
    title={Broken Neural Scaling Laws}, 
    author={Ethan Caballero and Kshitij Gupta and Irina Rish and David Krueger},
    year={2023},
    eprint={2210.14891},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{flan_wei_2021,
author       = {Jason Wei and
                Maarten Bosma and
                Vincent Y. Zhao and
                Kelvin Guu and
                Adams Wei Yu and
                Brian Lester and
                Nan Du and
                Andrew M. Dai and
                Quoc V. Le},
title        = {Finetuned Language Models Are Zero-Shot Learners},
journal      = {CoRR},
volume       = {abs/2109.01652},
year         = {2021},
url          = {https://arxiv.org/abs/2109.01652},
eprinttype    = {arXiv},
eprint       = {2109.01652},
timestamp    = {Wed, 16 Aug 2023 16:10:28 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2109-01652.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{Industrial_Union_Department,
title = {Industrial Union Department v. {American Petroleum Institute}},
author={{448 U.S. 607}},
year = {1980},
month = {June},
day = {23},
note = {448 U.S. 607}
}

@article{BAERENBOLD2023100103,
title = {Reducing risks in megaprojects: The potential of reference class forecasting},
journal = {Project Leadership and Society},
volume = {4},
pages = {100103},
year = {2023},
issn = {2666-7215},
doi = {https://doi.org/10.1016/j.plas.2023.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2666721523000248},
author = {Rebekka Baerenbold},
keywords = {Reference class forecasting, Project management, Cost estimation method, Cost overrun, Megaprojects},
abstract = {Large infrastructure projects often suffer from cost and schedule overruns, mainly due to optimism bias and strategic misrepresentation. Reference class forecasting (RCF) offers a potential remedy. This study presents a comprehensive analysis of the RCF literature with the aim of providing practitioners with key insights and identifying areas for future research. Through a review of 41 selected papers, the paper shows that the effectiveness of RCF is mainly applicable to large-scale projects and depends on the definition of the reference class. The paper calls for the development of an empirically based framework for reference class formation and urges the exploration of RCF's adaptability across industries, challenging the current one-size-fits-all approach. Theoretically, the paper critically assesses the current applications of RCF, while practically it outlines directions for future research and improvements. Overall, the study emphasises the need for detailed, data-driven methodologies and highlights their potential for risk management in projects worldwide.}
}

@misc{chiang2024chatbot,
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, 
    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
    year={2024},
    eprint={2403.04132},
    archivePrefix={arXiv},
    primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@article{Krimsky2005,
author = {Krimsky, Sheldon},
title = {The Weight of Scientific Evidence in Policy and Law},
journal = {American Journal of Public Health},
year = {2005},
volume = {95},
number = {Suppl 1},
pages = {S129--S136},
doi = {10.2105/AJPH.2004.044727},
pmid = {16030328}
}

@misc{lohn2022ai,
title={Will {AI} Make Cyber Swords or Shields?},
author={Lohn, Andrew J. and Jackson, Krystal A.},
year={2022},
month={August},
publisher={Center for Security and Emerging Technology},
doi={10.51593/2022CA002}
}

@misc{fang2024llm,
    title={LLM Agents can Autonomously Exploit One-day Vulnerabilities}, 
    author={Richard Fang and Rohan Bindu and Akul Gupta and Daniel Kang},
    year={2024},
    eprint={2404.08144},
    archivePrefix={arXiv},
    primaryClass={id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'}
}

@misc{carlini2023quantifying,
    title={Quantifying Memorization Across Neural Language Models}, 
    author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
    year={2023},
    eprint={2202.07646},
    archivePrefix={arXiv},
    primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}


@misc{kandpal2022deduplicating,
    title={Deduplicating Training Data Mitigates Privacy Risks in Language Models}, 
    author={Nikhil Kandpal and Eric Wallace and Colin Raffel},
    year={2022},
    eprint={2202.06539},
    archivePrefix={arXiv},
    primaryClass={id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'}
}
@misc{barrett2024benchmark,
    title={Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models}, 
    author={Anthony M. Barrett and Krystal Jackson and Evan R. Murphy and Nada Madkour and Jessica Newman},
    year={2024},
    eprint={2405.10986},
    archivePrefix={arXiv},
    primaryClass={id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'}
}

@misc{shankar2022operationalizing,
    title={Operationalizing Machine Learning: An Interview Study}, 
    author={Shreya Shankar and Rolando Garcia and Joseph M. Hellerstein and Aditya G. Parameswaran},
    year={2022},
    eprint={2209.09125},
    archivePrefix={arXiv},
    primaryClass={id='cs.SE' full_name='Software Engineering' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers design tools, software metrics, testing and debugging, programming environments, etc. Roughly includes material in all of ACM Subject Classes D.2, except that D.2.4 (program verification) should probably have Logics in Computer Science as the primary subject area.'}
}

@inproceedings{NIPS2015_86df7dcf,
author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran\c{c}ois and Dennison, Dan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Hidden Technical Debt in Machine Learning Systems},
url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf},
volume = {28},
year = {2015}
}


@misc{panda2024teach,
    title={Teach LLMs to Phish: Stealing Private Information from Language Models}, 
    author={Ashwinee Panda and Christopher A. Choquette-Choo and Zhengming Zhang and Yaoqing Yang and Prateek Mittal},
    year={2024},
    eprint={2403.00871},
    archivePrefix={arXiv},
    primaryClass={id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'}
}

@misc{borzunov2023petals,
    title={Petals: Collaborative Inference and Fine-tuning of Large Models}, 
    author={Alexander Borzunov and Dmitry Baranchuk and Tim Dettmers and Max Ryabinin and Younes Belkada and Artem Chumachenko and Pavel Samygin and Colin Raffel},
    year={2023},
    eprint={2209.01188},
    archivePrefix={arXiv},
    primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{donyehiya2023cold,
    title={ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning}, 
    author={Shachar Don-Yehiya and Elad Venezian and Colin Raffel and Noam Slonim and Yoav Katz and Leshem Choshen},
    year={2023},
    eprint={2212.01378},
    archivePrefix={arXiv},
    primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{davidson2023ai,
    title={AI capabilities can be significantly improved without expensive retraining}, 
    author={Tom Davidson and Jean-Stanislas Denain and Pablo Villalobos and Guillem Bas},
    year={2023},
    eprint={2312.07413},
    archivePrefix={arXiv},
    primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{schaeffer2023emergent,
    title={Are Emergent Abilities of Large Language Models a Mirage?}, 
    author={Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},
    year={2023},
    eprint={2304.15004},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{ramshaw-marcus-1995-text,
  title = "Text Chunking using Transformation-Based Learning",
  author = "Ramshaw, Lance  and
    Marcus, Mitch",
  booktitle = "Third Workshop on Very Large Corpora",
  year = "1995",
  url = "https://aclanthology.org/W95-0107",
}

@inproceedings{Tsipras2020FromIT,
title={From ImageNet to Image Classification: Contextualizing Progress on Benchmarks},
author={D. Tsipras and Shibani Santurkar and Logan Engstrom and Andrew Ilyas and A. Madry},
booktitle={ICML},
year={2020}
}
@misc{dehghani2021benchmark,
    title={The Benchmark Lottery}, 
    author={Mostafa Dehghani and Yi Tay and Alexey A. Gritsenko and Zhe Zhao and Neil Houlsby and Fernando Diaz and Donald Metzler and Oriol Vinyals},
    year={2021},
    eprint={2107.07002},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{vandeginste1988robust,
title={Robust regression and outlier detection. PJ Rousseeuw and AM Leroy, John Wiley, New York, 1987. No. of pages: 329. Price:{\pounds} 31.95. ISBN: 0 471 85233 3},
author={Vandeginste, Bernard GM},
year={1988},
publisher={Wiley Online Library}
}
@misc{beyer2020imagenet,
    title={Are we done with ImageNet?}, 
    author={Lucas Beyer and Olivier J. Hénaff and Alexander Kolesnikov and Xiaohua Zhai and Aäron van den Oord},
    year={2020},
    eprint={2006.07159},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@article{rosenblatt1956remarks,
title={Remarks on some nonparametric estimates of a density function. Annals of Mathematical Statistics},
author={Rosenblatt, M},
year={1956}
}
@article{hawkins1974detection,
title={The detection of errors in multivariate data using principal components},
author={Hawkins, Douglas M},
journal={Journal of the American Statistical Association},
volume={69},
number={346},
pages={340--344},
year={1974},
publisher={Taylor \& Francis Group}
}
@inproceedings{ruff2018deep,
title={Deep one-class classification},
author={Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and M{\"u}ller, Emmanuel and Kloft, Marius},
booktitle={International conference on machine learning},
pages={4393--4402},
year={2018},
organization={PMLR}
}

@inproceedings{Denker1990,
author = {Denker, John S. and LeCun, Yann},
title = {Transforming Neural-Net Output Levels to Probability Distributions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments of the probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "softmax" scheme.},
booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
pages = {853–859},
numpages = {7},
location = {Denver, Colorado},
series = {NIPS'90}
}


@article{pleiss2020,
author    = {Geoff Pleiss and
             Tianyi Zhang and
             Ethan R. Elenberg and
             Kilian Q. Weinberger},
title     = {Identifying Mislabeled Data using the Area Under the Margin Ranking},
journal   = {CoRR},
volume    = {abs/2001.10528},
year      = {2020},
url       = {https://arxiv.org/abs/2001.10528},
archivePrefix = {arXiv},
eprint    = {2001.10528},
timestamp = {Wed, 03 Jun 2020 10:08:34 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2001-10528.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{schroder2020survey,
    title={A Survey of Active Learning for Text Classification using Deep Neural Networks}, 
    author={Christopher Schroder and Andreas Niekler},
    year={2020},
    eprint={2008.07267},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{Liu2020PeerLF,
title={Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates},
author={Y. Liu and Hong-Yi Guo},
journal={ArXiv},
year={2020},
volume={abs/1910.03231}
}


@misc{thulasidasan2019combating,
    title={Combating Label Noise in Deep Learning Using Abstention}, 
    author={Sunil Thulasidasan and Tanmoy Bhattacharya and Jeff Bilmes and Gopinath Chennupati and Jamal Mohd-Yusof},
    year={2019},
    eprint={1905.10964},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@INPROCEEDINGS{8953408,
author={Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S.},
booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Learning to Learn From Noisy Labeled Data}, 
year={2019},
volume={},
number={},
pages={5046-5054},
doi={10.1109/CVPR.2019.00519}}
@article{Zhang2019ToBO,
title={To Balance or Not to Balance: An Embarrassingly Simple Approach for Learning with Long-Tailed Distributions},
author={J. Zhang and Lingqiao Liu and P. Wang and Chunhua Shen},
journal={ArXiv},
year={2019},
volume={abs/1912.04486}
}
@misc{shen2018deep,
    title={Deep Active Learning for Named Entity Recognition}, 
    author={Yanyao Shen and Hyokun Yun and Zachary C. Lipton and Yakov Kronrod and Animashree Anandkumar},
    year={2018},
    eprint={1707.05928},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@INPROCEEDINGS{4129456,
author={Rosenberg, Chuck and Hebert, Martial and Schneiderman, Henry},
booktitle={2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1}, 
title={Semi-Supervised Self-Training of Object Detection Models}, 
year={2005},
volume={1},
number={},
pages={29-36},
doi={10.1109/ACVMOT.2005.107}}

@InProceedings{Liu_2020_CVPR,
author = {Liu, Jialun and Sun, Yifan and Han, Chuchu and Dou, Zhaopeng and Li, Wenhui},
title = {Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{sorcher2022,
doi = {10.48550/ARXIV.2206.14486},

url = {https://arxiv.org/abs/2206.14486},

author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Beyond neural scaling laws: beating power law scaling via data pruning},

publisher = {arXiv},

year = {2022},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{hooker2019,
doi = {10.48550/ARXIV.1911.05248},

url = {https://arxiv.org/abs/1911.05248},

author = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Human-Computer Interaction (cs.HC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {What Do Compressed Deep Neural Networks Forget?},

publisher = {arXiv},

year = {2019},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{luong-manning-2015-stanford,
  title = "{S}tanford neural machine translation systems for spoken language domains",
  author = "Luong, Minh-Thang  and
    Manning, Christopher",
  booktitle = "Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign",
  month = dec # " 3-4",
  year = "2015",
  address = "Da Nang, Vietnam",
  url = "https://aclanthology.org/2015.iwslt-evaluation.11",
  pages = "76--79",
}

@inproceedings{biggio2018,
author = {Biggio, Battista and Roli, Fabio},
title = {Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3264418},
doi = {10.1145/3243734.3264418},
abstract = {Deep neural networks and machine-learning algorithms are pervasively used in several applications, ranging from computer vision to computer security. In most of these applications, the learning algorithm has to face intelligent and adaptive attackers who can carefully manipulate data to purposely subvert the learning process. As these algorithms have not been originally designed under such premises, they have been shown to be vulnerable to well-crafted, sophisticated attacks, including training-time poisoning and test-time evasion attacks (also known as adversarial examples). The problem of countering these threats and learning secure classifiers in adversarial settings has thus become the subject of an emerging, relevant research field known as adversarial machine learning. The purposes of this tutorial are: (a) to introduce the fundamentals of adversarial machine learning to the security community; (b) to illustrate the design cycle of a learning-based pattern recognition system for adversarial tasks; (c) to present novel techniques that have been recently proposed to assess performance of pattern classifiers and deep learning algorithms under attack, evaluate their vulnerabilities, and implement defense strategies that make learning algorithms more robust to attacks; and (d) to show some applications of adversarial machine learning to pattern recognition tasks like object recognition in images, biometric identity recognition, spam and malware detection.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2154–2156},
numpages = {3},
keywords = {adversarial machine learning, adversarial examples, evasion attacks, deep learning, training data poisoning},
location = {Toronto, Canada},
series = {CCS '18}
}

@misc{akhtar2018,
doi = {10.48550/ARXIV.1801.00553},

url = {https://arxiv.org/abs/1801.00553},

author = {Akhtar, Naveed and Mian, Ajmal},

keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},

publisher = {arXiv},

year = {2018},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{paul2021deep,
    title={Deep Learning on a Data Diet: Finding Important Examples Early in Training}, 
    author={Mansheej Paul and Surya Ganguli and Gintare Karolina Dziugaite},
    year={2021},
    eprint={2107.07075},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{evci2022gradmax,
    title={GradMax: Growing Neural Networks using Gradient Information}, 
    author={Utku Evci and Bart van Merriënboer and Thomas Unterthiner and Max Vladymyrov and Fabian Pedregosa},
    year={2022},
    eprint={2201.05125},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Dean202011TD,
  title          = {{1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design}},
  author         = {Dean, Jeffrey},
  year           = {2020},
  journal        = {2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
  pages          = {8--14}
}
@article{gruetzemacher20183d,
  title          = {{3D deep learning for detecting pulmonary nodules in CT scans}},
  author         = {Gruetzemacher, Ross and Gupta, Ashish and Paradice, David},
  year           = {2018},
  journal        = {Journal of the American Medical Informatics Association},
  publisher      = {Oxford University Press},
  volume         = {25},
  number         = {10},
  pages          = {1301--1310}
}
@article{2016Hendrycks,
  title          = {{A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}},
  author         = {Hendrycks, Dan and Gimpel, Kevin},
  year           = {2016},
  month          = {Oct},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1610.02136},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv161002136H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{Hooker2019ABF,
  title          = {{A Benchmark for Interpretability Methods in Deep Neural Networks}},
  author         = {Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
  year           = {2019},
  booktitle      = {NeurIPS 2019},
  url            = {https://papers.NeurIPS.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf}
}
@inproceedings{post-2018-call,
  title          = {{A Call for Clarity in Reporting BLEU Scores}},
  author         = {Post, Matt},
  year           = {2018},
  month          = oct,
  booktitle      = {Proceedings of the Third Conference on Machine Translation: Research Papers},
  publisher      = {Association for Computational Linguistics},
  address        = {Belgium, Brussels},
  pages          = {186--191},
  url            = {https://www.aclweb.org/anthology/W18-6319}
}
@article{2007legg,
  title          = {{A Collection of Definitions of Intelligence}},
  author         = {Legg, Shane and Hutter, Marcus},
  year           = {2007},
  month          = jun,
  journal        = {arXiv e-prints},
  pages          = {arXiv:0706.3639},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2007arXiv0706.3639L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Zador2019ACO,
  title          = {{A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains}},
  author         = {Anthony M. Zador},
  year           = {2019},
  journal        = {bioRxiv}
}
@article{Harwell_2019,
  title          = {{A face-scanning algorithm increasingly decides whether you deserve the job}},
  author         = {Harwell, Drew},
  year           = {2019},
  journal        = {The Washington Post},
  url            = {https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/},
  urldate        = {2019-03-12}
}
@article{Hinton06,
  title          = {{A Fast Learning Algorithm for Deep Belief Nets}},
  author         = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  year           = {2006},
  journal        = {Neural Computation},
  volume         = {18},
  pages          = {1527--1554}
}
@article{afocus,
  title          = {{A Focus on Neural Machine Translation for African Languages}},
  author         = {Laura Martinus and Jade Z. Abbott},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1906.05685},
  url            = {http://arxiv.org/abs/1906.05685},
  timestamp      = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1906-05685.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{Ando2005,
  title          = {{A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data}},
  author         = {Ando, Rie Kubota and Zhang, Tong},
  year           = {2005},
  month          = dec,
  journal        = {Journal of Machine Learning Research},
  publisher      = {JMLR.org},
  volume         = {6},
  pages          = {1817--1853},
  issn           = {1532-4435},
  acmid          = {1194905},
  issue_date     = {12/1/2005},
  numpages       = {37}
}
@inproceedings{lubana2021a,
  title          = {{A Gradient Flow Framework For Analyzing Network Pruning}},
  author         = {Ekdeep Singh Lubana and Robert Dick},
  year           = {2021},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=rumv7QmLUue}
}
@article{yang2019mean,
  title          = {{A mean field theory of batch normalization}},
  author         = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1902.08129}
}
@inproceedings{vig_multiscale_2019,
  title          = {{A Multiscale Visualization of Attention in the Transformer Model}},
  author         = {Vig, Jesse},
  year           = {2019},
  month          = jul,
  booktitle      = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {37--42},
  url            = {https://www.aclweb.org/anthology/P19-3007},
  urldate        = {2021-01-27}
}
@article{kendall1938new,
  title          = {{A new measure of rank correlation}},
  author         = {Kendall, Maurice G},
  year           = {1938},
  journal        = {Biometrika},
  publisher      = {JSTOR},
  volume         = {30},
  number         = {1/2},
  pages          = {81--93}
}
@article{Novel2009,
  title          = {{A Novel Template Reduction Approach for the$K$-Nearest Neighbor Method}},
  author         = {H. A. Fayed and A. F. Atiya},
  year           = {2009},
  month          = {May},
  journal        = {IEEE Transactions on Neural Networks},
  volume         = {20},
  number         = {5},
  pages          = {890--896},
  issn           = {1045-9227}
}
@article{rogers_primer_2020,
  title          = {{A Primer in BERTology: What We Know About How BERT Works}},
  author         = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year           = {2020},
  journal        = {Transactions of the Association for Computational Linguistics},
  volume         = {8},
  number         = {0},
  pages          = {842--866},
  issn           = {2307-387X},
  url            = {https://transacl.org/index.php/tacl/article/view/2257},
  urldate        = {2021-01-22},
  note           = {Number: 0},
}
@article{Kriegman1853,
  title          = {{A scalable pipeline for designing reconfigurable organisms}},
  author         = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
  year           = {2020},
  journal        = {Proceedings of the National Academy of Sciences},
  publisher      = {National Academy of Sciences},
  volume         = {117},
  number         = {4},
  pages          = {1853--1859},
  issn           = {0027-8424},
  url            = {https://www.pnas.org/content/117/4/1853}
}
@article{lee2019signal,
  title          = {{A signal propagation perspective for pruning neural networks at initialization}},
  author         = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1906.06307}
}
@article{karnin1990simple,
  title          = {{A simple procedure for pruning back-propagation trained neural networks}},
  author         = {Karnin, Ehud D},
  year           = {1990},
  journal        = {IEEE transactions on neural networks},
  publisher      = {IEEE},
  volume         = {1},
  number         = {2},
  pages          = {239--242}
}
@inproceedings{krogh1992simple,
  title          = {{A simple weight decay can improve generalization}},
  author         = {Krogh, Anders and Hertz, John A},
  year           = {1992},
  booktitle      = {Advances in neural information processing systems},
  pages          = {950--957}
}
@article{1993PASJ,
  title          = {{A Special-Purpose Computer forN-Body Simulations: GRAPE-2A}},
  author         = {Ito, Tomoyoshi and Makino, Junichiro and Fukushige, Toshiyuki and Ebisuzaki, Toshikazu and Okumura, Sachiko K. and Sugimoto, Daiichiro},
  year           = {1993},
  month          = jun,
  journal        = {\pasj},
  volume         = {45},
  pages          = {339--347},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/1993PASJ...45..339I},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{robbins1951stochastic,
  title          = {{A stochastic approximation method}},
  author         = {Robbins, Herbert and Monro, Sutton},
  year           = {1951},
  journal        = {The annals of mathematical statistics},
  publisher      = {JSTOR},
  pages          = {400--407}
}
@article{tan2007,
  title          = {{A synthetic biology challenge: Making cells compute}},
  author         = {Tan, Cheemeng and Song, Hao and Niemi, Jarad and You, Lingchong},
  year           = {2007},
  month          = {06},
  journal        = {Molecular bioSystems},
  volume         = {3},
  pages          = {343--53}
}
@misc{dsouza2021tale,
  title          = {{A Tale Of Two Long Tails}},
  author         = {D'souza, Daniel and Nussbaum, Zach and Agarwal, Chirag and Hooker, Sara},
  year           = {2021},
  publisher      = {arXiv},
  url            = {https://arxiv.org/abs/2107.13098}
}
@article{1954anderson_darling,
  title          = {{A Test of Goodness of Fit}},
  author         = {Anderson, T. W. and Darling, D. A.},
  year           = {1954},
  journal        = {Journal of the American Statistical Association},
  publisher      = {American Statistical Association},
  volume         = {49},
  number         = {268},
  pages          = {765--769},
  issn           = {01621459},
  url            = {http://www.jstor.org/stable/2281537},
  added-at       = {2009-05-09T19:00:52.000+0200},
  biburl         = {https://www.bibsonomy.org/bibtex/23d61214a70e110f205f8cb26b11f5381/maverick},
  timestamp      = {2009-05-09T19:00:53.000+0200}
}
@inproceedings{papadimitriou1980,
  title          = {{A worst-case analysis of nearest neighbor searching by projection}},
  author         = {Papadimitriou, Christos H. and Bentley, Jon Louis},
  year           = {1980},
  booktitle      = {Automata, Languages and Programming},
  editor         = {de Bakker, Jaco and van Leeuwen, Jan}
}

@inproceedings{ham_3_2020,
  title          = {{A\textasciicircum3: Accelerating Attention Mechanisms in Neural Networks with Approximation}},
  author         = {Ham, Tae Jun and Jung, Sung Jun and Kim, Seonghak and Oh, Young H. and Park, Yeonhong and Song, Yoonho and Park, Jung-Hun and Lee, Sanghee and Park, Kyoung and Lee, Jae W. and Jeong, Deog-Kyoon},
  year           = {2020},
  month          = feb,
  booktitle      = {2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages          = {328--341},
  note           = {ISSN: 2378-203X}
}

@misc{2018acceleratingai,
  title          = {{Accelerating AI: Past, Present, and Future}},
  author         = {Krste Asanovic},
  year           = {2018},
  url            = {https://www.youtube.com/watch?v=8n2HLp2gtYs&t=2116s}
}

@inproceedings{hsu_accelerating_2021,
  title          = {{Accelerating applications using edge tensor processing units}},
  author         = {Hsu, Kuan-Chieh and Tseng, Hung-Wei},
  year           = {2021},
  month          = nov,
  booktitle      = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and {Analysis}},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {{SC} '21},
  pages          = {1--14},
  url            = {https://doi.org/10.1145/3458817.3476177},
  urldate        = {2021-11-16}
}

@inproceedings{li_accelerating_2022,
  title          = {{Accelerating attention through gradient-based learned runtime pruning}},
  author         = {Li, Zheng and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kang, Mingu},
  year           = {2022},
  month          = jun,
  booktitle      = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {{ISCA} '22},
  pages          = {902--915},
  url            = {https://doi.org/10.1145/3470496.3527423},
  urldate        = {2022-06-22}
}

@misc{nvidiasparsecore,
  title          = {{Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT}},
  author         = {{Nvidia}},
  year           = {2021},
  month          = jul,
  journal        = {NVIDIA Technical Blog},
  url            = {https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/},
  urldate        = {2022-06-23},
  language       = {en-US}
}

@article{Goyal2017,
  title          = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
  author         = {Goyal, P. and Doll{\'a}r, P. and Girshick, R. and Noordhuis, P. and Wesolowski, L. and Kyrola, A. and Tulloch, A. and Jia, Y. and He, K.},
  year           = {2017},
  month          = jun,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170602677G},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{pasca_activation_2018,
  title          = {{Activation Function Architectures for FPGAs}},
  author         = {Pasca, Bogdan and Langhammer, Martin},
  year           = {2018},
  month          = aug,
  booktitle      = {2018 28th International Conference on Field Programmable Logic and Applications (FPL)},
  pages          = {43--437},
  note           = {ISSN: 1946-1488}
}

@mastersthesis{dubowski2020activation,
  title          = {{Activation function impact on Sparse Neural Networks}},
  author         = {Dubowski, Adam},
  year           = {2020},
  type           = {{B.S.} thesis},
  school         = {University of Twente}
}

@article{zeiler2012adadelta,
  title          = {{Adadelta: an adaptive learning rate method}},
  author         = {Zeiler, Matthew D},
  year           = {2012},
  journal        = {arXiv preprint arXiv:1212.5701}
}

@article{adam-optimizer,
  title          = {{Adam: A Method for Stochastic Optimization}},
  author         = {Diederik P. Kingma and Jimmy Ba},
  year           = {2014},
  journal        = {CoRR},
  volume         = {abs/1412.6980},
  url            = {http://arxiv.org/abs/1412.6980}
}

@article{kingma2014adam,
  title          = {{Adam: A method for stochastic optimization}},
  author         = {Kingma, Diederik P and Ba, Jimmy},
  year           = {2014},
  journal        = {arXiv preprint arXiv:1412.6980}
}
@inproceedings{zaheer2018adaptive,
  title          = {{Adaptive methods for nonconvex optimization}},
  author         = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  year           = {2018},
  booktitle      = {Advances in neural information processing systems},
  pages          = {9793--9803}
}
@article{duchi2011adaptive,
  title          = {{Adaptive subgradient methods for online learning and stochastic optimization.}},
  author         = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year           = {2011},
  journal        = {Journal of machine learning research},
  volume         = {12},
  number         = {7}
}
@inproceedings{correia_adaptively_2019,
  title          = {{Adaptively Sparse Transformers}},
  author         = {Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
  year           = {2019},
  month          = nov,
  booktitle      = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher      = {Association for Computational Linguistics},
  address        = {Hong Kong, China},
  pages          = {2174--2184},
  url            = {https://www.aclweb.org/anthology/D19-1223},
  urldate        = {2021-01-22}
}
@inproceedings{Kubat97addressingthe,
  title          = {{Addressing the Curse of Imbalanced Training Sets: One-Sided Selection}},
  author         = {Miroslav Kubat and Stan Matwin},
  year           = {1997},
  booktitle      = {In Proceedings of the Fourteenth International Conference on Machine Learning},
  publisher      = {Morgan Kaufmann},
  pages          = {179--186}
}
@article{2016alexey,
  title          = {{Adversarial examples in the physical world}},
  author         = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year           = {2016},
  month          = {Jul},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1607.02533},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv160702533K},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{2018Amodei,
  title          = {{AI and Compute}},
  author         = {Amodei, Dario and Hernandez, Danny and Sastry,Girish and Clark, Jack and Brockman, Greg and Sutskever, Ilya},
  year           = {2018},
  url            = {https://openai.com/blog/ai-and-compute/}
}
@article{Sowik2021AlgorithmicBA,
  title          = {{Algorithmic Bias and Data Bias: Understanding the Relation between Distributionally Robust Optimization and Data Curation}},
  author         = {Agnieszka Słowik and L. Bottou},
  year           = {2021},
  journal        = {ArXiv},
  volume         = {abs/2106.09467}
}
@book{Gusfield:97,
  title          = {{Algorithms on Strings, Trees and Sequences}},
  author         = {Dan Gusfield},
  year           = {1997},
  publisher      = {Cambridge University Press},
  address        = {Cambridge, UK}
}
@article{Lin1004,
  title          = {{All-optical machine learning using diffractive deep neural networks}},
  author         = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
  year           = {2018},
  journal        = {Science},
  publisher      = {American Association for the Advancement of Science},
  volume         = {361},
  number         = {6406},
  pages          = {1004--1008},
  issn           = {0036-8075},
  url            = {https://science.sciencemag.org/content/361/6406/1004}
}
@article{Chandra:81,
  title          = {{Alternation}},
  author         = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
  year           = {1981},
  journal        = {Journal of the Association for Computing Machinery},
  volume         = {28},
  number         = {1},
  pages          = {114--133}
}
@article{Dastin_2018,
  title          = {{Amazon scraps secret AI recruiting tool that showed bias against women}},
  author         = {Dastin, Jeffrey},
  year           = {2018},
  journal        = {Reuters},
  url            = {https://reut.rs/2p0ZWqe},
  urldate        = {2019-10-13}
}
@inproceedings{automatic-model-compression,
  title          = {{AMC: AutoML for Model Compression and Acceleration on Mobile Devices}},
  author         = {Yihui He and Ji Lin and Zhijian Liu and Hanrui Wang and Li{-}Jia Li and Song Han},
  year           = {2018},
  booktitle      = {Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII},
  pages          = {815--832}
}
@book{kipling1899,
  title          = {{American notes / by Rudyard Kipling}},
  author         = {Kipling, Rudyard},
  year           = {1899},
  publisher      = {Brown and Company Boston},
  pages          = {1 online resource (137 pages, 1 unnumbered leaf of plates) :},
}
@article{2016Canziani,
  title          = {{An Analysis of Deep Neural Network Models for Practical Applications}},
  author         = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
  year           = {2016},
  month          = {May},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1605.07678},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv160507678C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{Gao2021,
  title          = {{An Empirical Exploration in Quality Filtering of Text Data}},
  author         = {Gao, Leo},
  year           = {2021},
  publisher      = {arXiv},
  url            = {https://arxiv.org/abs/2109.00698}
}
@article{ruder2016overview,
  title          = {{An overview of gradient descent optimization algorithms}},
  author         = {Ruder, Sebastian},
  year           = {2016},
  journal        = {arXiv preprint arXiv:1609.04747}
}
@inproceedings{madaan2018analyze,
  title          = {{Analyze, detect and remove gender stereotyping from bollywood movies}},
  author         = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
  year           = {2018},
  booktitle      = {Conference on Fairness, Accountability and Transparency},
  pages          = {92--105}
}
@inproceedings{voita_analyzing_2019,
  title          = {{Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned}},
  author         = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  year           = {2019},
  month          = jul,
  booktitle      = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {5797--5808},
  url            = {https://www.aclweb.org/anthology/P19-1580},
  urldate        = {2021-01-22}
}
@inproceedings{vig_analyzing_2019,
  title          = {{Analyzing the Structure of Attention in a Transformer Language Model}},
  author         = {Vig, Jesse and Belinkov, Yonatan},
  year           = {2019},
  month          = aug,
  booktitle      = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {63--76},
  url            = {https://www.aclweb.org/anthology/W19-4808},
  urldate        = {2021-01-24}
}
@article{singh_article,
  title          = {{Analyzing Worldwide Research in Hardware Architecture, 1997-2011}},
  author         = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
  year           = {2015},
  month          = {01},
  journal        = {Communications of the ACM},
  volume         = {Volume 58},
  pages          = {Pages 76--85}
}
@book{tolstoy2016anna,
  title          = {{Anna Karenina}},
  author         = {Tolstoy, L. and Bartlett, R.},
  year           = {2016},
  publisher      = {Oxford University Press},
  series         = {Oxford world's classics},
  url            = {https://books.google.com/books?id=1DooDwAAQBAJ},
  lccn           = {2015943753}
}
@article{129422,
  title          = {{Application of the ANNA neural network chip to high-speed character recognition}},
  author         = {Säckinger, Eduard and Boser, Bernhard E. and Bromley, Jane and LeCun, Yann and Jackel, Larry D.},
  year           = {1992},
  journal        = {IEEE Transactions on Neural Networks},
  volume         = {3},
  number         = {3},
  pages          = {498--505}
}
@incollection{werbos1982applications,
  title          = {{Applications of advances in nonlinear sensitivity analysis}},
  author         = {Werbos, Paul J},
  year           = {1982},
  booktitle      = {System modeling and optimization},
  publisher      = {Springer},
  pages          = {762--770}
}
@inproceedings{alyafeai-ahmad-2021-arabic,
  title          = {{Arabic Compact Language Modelling for Resource Limited Devices}},
  author         = {Alyafeai, Zaid  and Ahmad, Irfan},
  year           = {2021},
  month          = apr,
  booktitle      = {Proceedings of the Sixth Arabic Natural Language Processing Workshop},
  publisher      = {Association for Computational Linguistics},
  address        = {Kyiv, Ukraine (Virtual)},
  pages          = {53--59},
  url            = {https://www.aclweb.org/anthology/2021.wanlp-1.6}
}
@article{2018Vodrahalli,
  title          = {{Are All Training Examples Created Equal? An Empirical Study}},
  author         = {Vodrahalli, Kailas and Li, Ke and Malik, Jitendra},
  year           = {2018},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1811.12569},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv181112569V},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{michel_are_2019,
  title          = {{Are Sixteen Heads Really Better than One?}},
  author         = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year           = {2019},
  booktitle      = {Advances in Neural Information Processing Systems},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  pages          = {14014--14024},
  url            = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
  editor         = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.}
}
@book{Haugeland,
  title          = {{Artificial Intelligence: The Very Idea}},
  author         = {Haugeland, John},
  year           = {1985},
  publisher      = {Massachusetts Institute of Technology},
  address        = {USA}
}
@article{MISRA2010239,
  title          = {{Artificial neural networks in hardware: A survey of two decades of progress}},
  author         = {Janardan Misra and Indranil Saha},
  year           = {2010},
  journal        = {Neurocomputing},
  volume         = {74},
  number         = {1},
  pages          = {239--255},
  issn           = {0925-2312},
  url            = {http://www.sciencedirect.com/science/article/pii/S092523121000216X},
  note           = {Artificial Brains}
}
@article{kallus2019assessing,
  title          = {{Assessing algorithmic fairness with unobserved protected class using data combination}},
  author         = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1906.00285}
}
@incollection{Stevenson2018,
  title          = {{Assessing Risk Assessment in Action}},
  author         = {{Megan}, Stevenson},
  year           = {2018},
  journal        = {Minnesota Law Review},
  volume         = {58},
  url            = {https://scholarship.law.umn.edu/mlr/58}
}
@article{vaswani2017attention,
  title          = {{Attention Is All You Need}},
  author         = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year           = {2017},
  month          = jun,
  journal        = {ArXiv e-prints},
  booktitle      = {Advances in neural information processing systems},
  pages          = {5998--6008},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170603762V},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{jain_attention_2019,
  title          = {{Attention is not Explanation}},
  author         = {Jain, Sarthak and Wallace, Byron C.},
  year           = {2019},
  month          = may,
  journal        = {arXiv:1902.10186 [cs]},
  url            = {http://arxiv.org/abs/1902.10186},
  urldate        = {2021-01-24},
  note           = {arXiv: 1902.10186}
}
@inproceedings{wiegreffe_attention_2019,
  title          = {{Attention is not not Explanation}},
  author         = {Wiegreffe, Sarah and Pinter, Yuval},
  year           = {2019},
  month          = nov,
  booktitle      = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher      = {Association for Computational Linguistics},
  address        = {Hong Kong, China},
  pages          = {11--20},
  url            = {https://www.aclweb.org/anthology/D19-1002},
  urldate        = {2021-01-24}
}
@article{orifeDiacritics,
  title          = {{Attentive Sequence-to-Sequence Learning for Diacritic Restoration of Yor\`ub\'a Language Text}},
  author         = {Iroro Orife},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1804.00832},
  url            = {http://arxiv.org/abs/1804.00832},
  timestamp      = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1804-00832.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{park2020attribution,
  title          = {{Attribution Preservation in Network Compression for Reliable Network Interpretation}},
  author         = {Geondo Park and June Yong Yang and Sung Ju Hwang and Eunho Yang},
  year           = {2020}
}
@article{autoencoding-variational-bayes,
  title          = {{Auto-Encoding Variational Bayes}},
  author         = {Diederik P. Kingma and Max Welling},
  year           = {2013},
  journal        = {CoRR},
  volume         = {abs/1312.6114}
}
@inproceedings{murray-chiang-2015-auto,
  title          = {{Auto-Sizing Neural Networks: With Applications to n-gram Language Models}},
  author         = {Murray, Kenton  and Chiang, David},
  year           = {2015},
  month          = sep,
  booktitle      = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  publisher      = {Association for Computational Linguistics},
  address        = {Lisbon, Portugal},
  pages          = {908--916},
  url            = {https://www.aclweb.org/anthology/D15-1107}
}
@inproceedings{murray-etal-2019-auto,
  title          = {{Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation}},
  author         = {Murray, Kenton  and Kinnison, Jeffery  and Nguyen, Toan Q.  and Scheirer, Walter  and Chiang, David},
  year           = {2019},
  month          = nov,
  booktitle      = {Proceedings of the 3rd Workshop on Neural Generation and Translation},
  publisher      = {Association for Computational Linguistics},
  address        = {Hong Kong},
  pages          = {231--240},
  url            = {https://www.aclweb.org/anthology/D19-5625}
}
@misc{murray2019autosizing,
  title          = {{Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation}},
  author         = {Kenton Murray and Jeffery Kinnison and Toan Q. Nguyen and Walter Scheirer and David Chiang},
  year           = {2019}
}
@article{CLINTWHALEY20013,
  title          = {{Automated empirical optimizations of software and the ATLAS project}},
  author         = {R. {Clint Whaley} and Antoine Petitet and Jack J. Dongarra},
  year           = {2001},
  journal        = {Parallel Computing},
  volume         = {27},
  number         = {1},
  pages          = {3--35},
  issn           = {0167-8191},
  url            = {http://www.sciencedirect.com/science/article/pii/S0167819100000879},
  note           = {New Trends in High Performance Computing}
}
@article{2019Hongtao,
  title          = {{Automated pulmonary nodule detection in CT images using deep convolutional neural networks}},
  author         = {Hongtao Xie and Dongbao Yang and Nannan Sun and Zhineng Chen and Yongdong Zhang},
  year           = {2019},
  journal        = {Pattern Recognition},
  volume         = {85},
  pages          = {109--119},
  issn           = {0031-3203},
  url            = {http://www.sciencedirect.com/science/article/pii/S0031320318302711}
}
@article{8476161,
  title          = {{Autotuning Numerical Dense Linear Algebra for Batched Computation With GPU Hardware Accelerators}},
  author         = {Dongarra, J. and Gates, M. and Kurzak, J. and Luszczek, P. and Tsai, Y. M.},
  year           = {2018},
  journal        = {Proceedings of the IEEE},
  volume         = {106},
  number         = {11},
  pages          = {2040--2055}
}
@misc{connecticut2008connecticut,
  title={CRIMES WITH MANDATORY MINIMUM PRISON SENTENCES—UPDATED AND REVISED},
  url={https://www.cga.ct.gov/2008/rpt/2008-R-0619.htm},
  journal={State of Connecticut},
  author={Connecticut, State of},
  year={2008}
}

@inproceedings{NEURIPS2023_42c40aff,
author = {Mince, Fraser and Dinh, Dzung and Kgomo, Jonas and Thompson, Neil and Hooker, Sara},
booktitle = {Advances in Neural Information Processing Systems},
editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
pages = {21217--21229},
publisher = {Curran Associates, Inc.},
title = {The Grand Illusion: The Myth of Software Portability and Implications for ML Progress.},
url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/42c40aff7814e9796266e12053b1c610-Paper-Conference.pdf},
volume = {36},
year = {2023}
}


@article{lundjensen2012,
author = {Lund-Jensen, Kasper},
year = {2012},
month = {06},
pages = {},
title = {Monitoring Systemic Risk Based on Dynamic Thresholds},
volume = {12},
journal = {IMF Working Papers},
doi = {10.5089/9781475504576.001}
}

@inproceedings{9087368,
  title          = {{Avoiding Unintended Bias in Toxicity Classification with Neural Networks}},
  author         = {Morzhov, Sergey},
  year           = {2020},
  booktitle      = {2020 26th Conference of Open Innovations Association (FRUCT)},
  pages          = {314--320}
}
@article{ho_axial_2019,
  title          = {{Axial Attention in Multidimensional Transformers}},
  author         = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  year           = {2019},
  month          = dec,
  journal        = {arXiv:1912.12180 [cs]},
  url            = {http://arxiv.org/abs/1912.12180},
  urldate        = {2020-10-14},
  note           = {arXiv: 1912.12180}
}
@misc{LeCun1989,
  title          = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  author         = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year           = {1989},
  publisher      = {MIT Press},
  volume         = {1},
  number         = {4},
  pages          = {541–551},
  url            = {https://doi.org/10.1162/neco.1989.1.4.541}
}
@article{LILLICRAP201982,
  title          = {{Backpropagation through time and the brain}},
  author         = {Timothy P Lillicrap and Adam Santoro},
  year           = {2019},
  journal        = {Current Opinion in Neurobiology},
  volume         = {55},
  pages          = {82--89},
  issn           = {0959-4388},
  url            = {http://www.sciencedirect.com/science/article/pii/S0959438818302009},
  note           = {Machine Learning, Big Data, and Neuroscience}
}
@article{Ioffe2015,
  title          = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  author         = {Sergey Ioffe and Christian Szegedy},
  year           = {2015},
  journal        = {CoRR},
  volume         = {abs/1502.03167},
  url            = {http://arxiv.org/abs/1502.03167},
  timestamp      = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/IoffeS15},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{2019arXiv190608158K,
  title          = {{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}},
  author         = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
  year           = {2019},
  month          = {Jun},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1906.08158},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608158K},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{bayesian-compression,
  title          = {{Bayesian Compression for Deep Learning}},
  author         = {Christos Louizos and Karen Ullrich and Max Welling},
  year           = {2017},
  booktitle      = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA},
  pages          = {3290--3300}
}
@article{spike-and-slab,
  title          = {{Bayesian Variable Selection in Linear Regression}},
  author         = {Mitchell, T. J. and Beauchamp, J. J.},
  year           = {1988},
  journal        = {Journal of the American Statistical Association},
  publisher      = {Taylor & Francis},
  volume         = {83},
  number         = {404},
  pages          = {1023--1032}
}
@inproceedings{Duh2020BenchmarkingNA,
  title          = {{Benchmarking Neural and Statistical Machine Translation on Low-Resource African Languages}},
  author         = {Kevin Duh and P. McNamee and Matt Post and Brian Thompson},
  year           = {2020},
  booktitle      = {LREC}
}
@article{2019Hendrycks_Dietterich,
  title          = {{Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}},
  author         = {Hendrycks, Dan and Dietterich, Thomas},
  year           = {2019},
  month          = {Mar},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1903.12261},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312261H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{hendrycks2018benchmarking,
  title          = {{Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}},
  author         = {Dan Hendrycks and Thomas Dietterich},
  year           = {2019},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=HJz6tiCqYm}
}
@article{2018Hendrycks,
  title          = {{Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations}},
  author         = {Hendrycks, Dan and Dietterich, Thomas G.},
  year           = {2018},
  month          = {Jul},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1807.01697},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv180701697H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{devlin2018bert,
  title          = {{Bert: Pre-training of deep bidirectional transformers for language understanding}},
  author         = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year           = {2018},
  journal        = {arXiv preprint arXiv:1810.04805}
}
@inproceedings{10.1145/2555243.2557966,
  title          = {{Beyond Parallel Programming with Domain Specific Languages}},
  author         = {Olukotun, Kunle},
  year           = {2014},
  month          = feb,
  journal        = {SIGPLAN Not.},
  booktitle      = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  location       = {Orlando, Florida, USA},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {PPoPP '14},
  volume         = {49},
  number         = {8},
  pages          = {179–180},
  issn           = {0362-1340},
  url            = {https://doi.org/10.1145/2555243.2557966},
  numpages       = {2},
  issue_date     = {August 2014}
}
@article{Hughes2017,
  title          = {{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}},
  author         = {Wu, M. and Hughes, M.~C. and Parbhoo, S. and Zazzi, M. and Roth, V. and Doshi-Velez, F.},
  year           = {2017},
  month          = nov,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv171106178W},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{Cade2018,
  title          = {{Big Bets on A.I. Open a New Frontier for Chip Start-Ups, Too}},
  author         = {Cade Metz},
  year           = {2018},
  url            = {https://www.nytimes.com/2018/01/14/technology/artificial-intelligence-chip-start-ups.html}
}
@article{li_big_2019,
  title          = {{Big Bidirectional Insertion Representations for Documents}},
  author         = {Li, Lala and Chan, William},
  year           = {2019},
  month          = oct,
  journal        = {arXiv:1910.13034 [cs]},
  url            = {http://arxiv.org/abs/1910.13034},
  urldate        = {2020-11-13},
  note           = {arXiv: 1910.13034 version: 1}
}
@article{bai_binarybert_2020,
  title          = {{BinaryBERT: Pushing the Limit of BERT Quantization}},
  author         = {Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  year           = {2020},
  month          = dec,
  journal        = {arXiv:2012.15701 [cs]},
  url            = {http://arxiv.org/abs/2012.15701},
  urldate        = {2021-01-28},
  note           = {arXiv: 2012.15701}
}
@article{Bai2020BinaryBERTPT,
  title          = {{BinaryBERT: Pushing the Limit of BERT Quantization}},
  author         = {Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and X. Jiang and Qun Liu and Michael R. Lyu and Irwin King},
  year           = {2020},
  journal        = {ArXiv},
  volume         = {abs/2012.15701}
}
@article{article,
  title          = {{Birdsong: From Behavior to Neuron}},
  author         = {Konishi, Masakazu},
  year           = {1985},
  month          = {02},
  journal        = {Annual review of neuroscience},
  volume         = {8},
  pages          = {125--70}
}
@inproceedings{Papineni2002,
  title          = {{BLEU: A Method for Automatic Evaluation of Machine Translation}},
  author         = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year           = {2002},
  booktitle      = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  location       = {Philadelphia, Pennsylvania},
  publisher      = {Association for Computational Linguistics},
  address        = {USA},
  series         = {ACL '02},
  pages          = {311–318},
  url            = {https://doi.org/10.3115/1073083.1073135},
  numpages       = {8}
}
@misc{blocksparse-gpu-kernels,
  title          = {{Block-Sparse GPU Kernels}},
  author         = {Scott Gray and Alec Radford and Diedrik P. Kingma},
  year           = {2017},
  howpublished   = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}}
}
@incollection{NeurIPS2016Cortes,
  title          = {{Boosting with Abstention}},
  author         = {Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
  year           = {2016},
  booktitle      = {Advances in Neural Information Processing Systems 29},
  publisher      = {Curran Associates, Inc.},
  pages          = {1660--1668},
  url            = {http://papers.NeurIPS.cc/paper/6336-boosting-with-abstention.pdf},
  editor         = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{zhao2018bridging,
  title          = {{Bridging the gap between deep learning and sparse matrix format selection}},
  author         = {Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng},
  year           = {2018},
  booktitle      = {Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
  pages          = {94--108}
}
@inproceedings{quoc2012,
  title          = {{Building High-Level Features Using Large Scale Unsupervised Learning}},
  author         = {Le, Quoc V. and Ranzato, Marc’Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
  year           = {2012},
  booktitle      = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
  location       = {Edinburgh, Scotland},
  publisher      = {Omnipress},
  address        = {Madison, WI, USA},
  series         = {ICML’12},
  pages          = {507–514},
  numpages       = {8}
}
@inproceedings{6909517,
  title          = {{Capturing Long-Tail Distributions of Object Subcategories}},
  author         = {Zhu,X. and Anguelov D. and Ramanan, D.},
  year           = {2014},
  booktitle      = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  pages          = {915--922}
}
@article{Zhu2014,
  title          = {{Capturing Long-Tail Distributions of Object Subcategories}},
  author         = {Zhu, Xiangxin and Anguelov, Dragomir and Ramanan, Deva},
  year           = {2014},
  month          = {09},
  booktitle      = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
  publisher      = {IEEE Computer Society},
  address        = {USA},
  series         = {CVPR ’14},
  pages          = {915--922},
  url            = {https://doi.org/10.1109/CVPR.2014.122},
  numpages       = {8}
}
@misc{patterson2021carbon,
  title          = {{Carbon Emissions and Large Neural Network Training}},
  author         = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
  year           = {2021}
}
@inproceedings{Caruana2000,
  title          = {{Case-Based Explanation for Artificial Neural Nets}},
  author         = {Caruana, Rich},
  year           = {2000},
  booktitle      = {Artificial Neural Networks in Medicine and Biology},
  publisher      = {Springer London},
  address        = {London},
  pages          = {303--308},
  editor         = {Malmgren, Helge and Borga, Magnus and Niklasson, Lars}
}
@misc{MCCLOSKEY1989109,
  title          = {{Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem}},
  author         = {Michael McCloskey and Neal J. Cohen},
  year           = {1989},
  publisher      = {Academic Press},
  series         = {Psychology of Learning and Motivation},
  volume         = {24},
  pages          = {109--165},
  issn           = {0079-7421},
  editor         = {Gordon H. Bower}
}
@article{2014certifying_removing_disparate_impact,
  title          = {{Certifying and removing disparate impact}},
  author         = {Feldman, Michael and Friedler, Sorelle and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year           = {2014},
  month          = {Dec},
  journal        = {arXiv e-prints},
  booktitle      = {proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages          = {arXiv:1412.3756},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3756F},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2020arXiv201003058H,
  title          = {{Characterising Bias in Compressed Models}},
  author         = {Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  year           = {2020},
  month          = oct,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2010.03058},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv201003058H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{hooker2020characterising,
  title          = {{Characterising Bias in Compressed Models}},
  author         = {Sara Hooker and Nyalleng Moorosi and Gregory Clark and Samy Bengio and Emily Denton},
  year           = {2020}
}
@article{2020arXiv200203206J,
  title          = {{Characterizing Structural Regularities of Labeled Data in Overparameterized Models}},
  author         = {Jiang, Ziheng and Zhang, Chiyuan and Talwar, Kunal and Mozer, Michael C.},
  year           = {2020},
  month          = feb,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2002.03206},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200203206J},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{labatie2019characterizing,
  title          = {{Characterizing well-behaved vs. pathological deep neural networks}},
  author         = {Labatie, Antoine},
  year           = {2019},
  booktitle      = {International Conference on Machine Learning},
  pages          = {3611--3621},
  organization   = {PMLR}
}
@article{Suriyakumar2021ChasingYL,
  title          = {{Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings}},
  author         = {V. Suriyakumar and Nicolas Papernot and A. Goldenberg and M. Ghassemi},
  year           = {2021},
  journal        = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency}
}
@misc{Kubota2018,
  title          = {{China Plans $47$ Billion Fund to Boost Its Semiconductor Industry}},
  author         = {Yoko Kubota},
  year           = {2018},
  url            = {https://on.wsj.com/32L7Kwn}
}
@article{2020Mirhoseini,
  title          = {{Chip Placement with Deep Reinforcement Learning}},
  author         = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Bae, Sungmin and Nazi, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Babu, Anand and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
  year           = {2020},
  month          = apr,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2004.10746},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200410746M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{mirhoseini2020chip,
  title          = {{Chip Placement with Deep Reinforcement Learning}},
  author         = {Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Jiang and Ebrahim Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Sungmin Bae and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and William Hang and Emre Tuncer and Anand Babu and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
  year           = {2020}
}
@article{krizhevsky2009cifar,
  title          = {{Cifar-10 and cifar-100 datasets}},
  author         = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  year           = {2009},
  journal        = {URl: https://www. cs. toronto. edu/kriz/cifar. html},
  volume         = {6},
  pages          = {1}
}
@article{Bartlett2008,
  title          = {{Classification with a Reject Option Using a Hinge Loss}},
  author         = {Bartlett, Peter L. and Wegkamp, Marten H.},
  year           = {2008},
  month          = jun,
  journal        = {J. Mach. Learn. Res.},
  publisher      = {JMLR.org},
  volume         = {9},
  pages          = {1823--1840},
  issn           = {1532-4435},
  url            = {http://dl.acm.org/citation.cfm?id=1390681.1442792},
  issue_date     = {6/1/2008},
  numpages       = {18},
  acmid          = {1442792}
}
@inproceedings{albericio_cnvlutin_2016,
  title          = {{Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing}},
  author         = {Albericio, J. and Judd, P. and Hetherington, T. and Aamodt, T. and Jerger, N. E. and Moshovos, A.},
  year           = {2016},
  month          = jun,
  booktitle      = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {1--13},
  note           = {ISSN: 1063-6897}
}
@inproceedings{Chatterjee2020Coherent,
  title          = {{Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization}},
  author         = {Satrajit Chatterjee},
  year           = {2020},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=ryeFY0EFwS}
}
@inproceedings{7459430,
  title          = {{Collective Knowledge: Towards R D sustainability}},
  author         = {Fursin, G. and Lokhmotov, A. and Plowman, E.},
  year           = {2016},
  booktitle      = {2016 Design, Automation   Test in Europe Conference   Exhibition (DATE)},
  pages          = {864--869}
}
@inproceedings{10.5555/2969735.2969756,
  title          = {{Comparing Biases for Minimal Network Construction with Back-Propagation}},
  author         = {Hanson, Stephen Jos\'{e} and Pratt, Lorien Y.},
  year           = {1988},
  booktitle      = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
  publisher      = {MIT Press},
  address        = {Cambridge, MA, USA},
  series         = {NeurIPS'88},
  pages          = {177–185},
  numpages       = {9}
}
@article{Cha2007,
  title          = {{Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions}},
  author         = {Cha, Sung-Hyuk},
  year           = {2007},
  month          = {01},
  journal        = {Int. J. Math. Model. Meth. Appl. Sci.},
  volume         = {1}
}
@article{gordon_compressing_2020,
  title          = {{Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}},
  author         = {Gordon, Mitchell A. and Duh, Kevin and Andrews, Nicholas},
  year           = {2020},
  month          = may,
  journal        = {arXiv:2002.08307 [cs]},
  url            = {http://arxiv.org/abs/2002.08307},
  urldate        = {2021-01-22},
  note           = {arXiv: 2002.08307}
}
@inproceedings{aji-heafield-2020-compressing,
  title          = {{Compressing Neural Machine Translation Models with 4-bit Precision}},
  author         = {Aji, Alham Fikri  and Heafield, Kenneth},
  year           = {2020},
  month          = jul,
  booktitle      = {Proceedings of the Fourth Workshop on Neural Generation and Translation},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {35--42},
  url            = {https://www.aclweb.org/anthology/2020.ngt-1.4}
}
@article{variational-information-bottleneck,
  title          = {{Compressing Neural Networks using the Variational Information Bottleneck}},
  author         = {Bin Dai and Chen Zhu and David P. Wipf},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1802.10399}
}
@article{2016abigail,
  title          = {{Compression of Neural Machine Translation Models via Pruning}},
  author         = {See, Abigail and Luong, Minh-Thang and Manning, Christopher D.},
  year           = {2016},
  month          = {Jun},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1606.09274},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv160609274S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{sun_computation_sparse,
  title          = {{Computation on Sparse Neural Networks and Its Implications for Future Hardware: Invited}},
  author         = {Sun, Fei and Qin, Minghai and Zhang, Tianyun and Liu, Liu and Chen, Yen-Kuang and Xie, Yuan},
  year           = {2020},
  booktitle      = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
  location       = {Virtual Event, USA},
  publisher      = {IEEE Press},
  series         = {DAC '20},
  articleno      = {32},
  numpages       = {6}
}
@article{2020sun,
  title          = {{Computation on Sparse Neural Networks: an Inspiration for Future Hardware}},
  author         = {Sun, Fei and Qin, Minghai and Zhang, Tianyun and Liu, Liu and Chen, Yen-Kuang and Xie, Yuan},
  year           = {2020},
  month          = apr,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2004.11946},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200411946S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@phdthesis{kearns89,
  title          = {{Computational Complexity of Machine Learning}},
  author         = {M. J. Kearns},
  year           = {1989},
  school         = {Department of Computer Science, Harvard University}
}
@article{benna2016,
  title          = {{Computational principles of synaptic memory consolidation}},
  author         = {Benna, Marcus and Fusi, Stefano},
  year           = {2016},
  month          = {10},
  journal        = {Nature Neuroscience},
  volume         = {19}
}
@misc{2018vacuum,
  title          = {{Computer History 1949 - 1960 Early Vacuum Tube Computers Overview}},
  author         = {Computer History Archives Project},
  year           = {2018},
  url            = {https://www.youtube.com/watch?v=WnNm_uJYWhA}
}
@article{2016amodei,
  title          = {{Concrete Problems in AI Safety}},
  author         = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year           = {2016},
  month          = {Jun},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1606.06565},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv160606565A},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{amodei2016,
  title          = {{Concrete Problems in AI Safety}},
  author         = {Dario Amodei and Chris Olah and Jacob Steinhardt and Paul F. Christiano and John Schulman and Dan Man{\'{e}}},
  year           = {2016},
  journal        = {CoRR},
  volume         = {abs/1606.06565},
  url            = {http://arxiv.org/abs/1606.06565},
  timestamp      = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/AmodeiOSCSM16},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{2017Huang,
  title          = {{CondenseNet: An Efficient DenseNet using Learned Group Convolutions}},
  author         = {Huang, Gao and Liu, Shichen and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year           = {2017},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1711.09224},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109224H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{congolese,
  title          = {{Congolese Swahili Machine Translation for Humanitarian Response}},
  author         = {Alp {\"{O}}ktem and Eric DeLuca and Rodrigue Bashizi and Eric Paquin and Grace Tang},
  year           = {2021},
  journal        = {AfricaNLP Workshop},
  url            = {https://arxiv.org/abs/2103.10734}
}
@article{neal1992connectionist,
  title          = {{Connectionist learning of belief networks}},
  author         = {Neal, Radford M},
  year           = {1992},
  journal        = {Artificial intelligence},
  publisher      = {Elsevier},
  volume         = {56},
  number         = {1},
  pages          = {71--113}
}
@inproceedings{voita_context-aware_2018,
  title          = {{Context-Aware Neural Machine Translation Learns Anaphora Resolution}},
  author         = {Voita, Elena and Serdyukov, Pavel and Sennrich, Rico and Titov, Ivan},
  year           = {2018},
  month          = jul,
  booktitle      = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher      = {Association for Computational Linguistics},
  address        = {Melbourne, Australia},
  pages          = {1264--1274},
  url            = {https://www.aclweb.org/anthology/P18-1117},
  urldate        = {2021-01-22}
}
@article{2018Parisi,
  title          = {{Continual Lifelong Learning with Neural Networks: A Review}},
  author         = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year           = {2018},
  month          = feb,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1802.07569},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv180207569P},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2017Stock_Cisse,
  title          = {{ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases}},
  author         = {Stock, Pierre and Cisse, Moustapha},
  year           = {2017},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1711.11443},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111443S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{stock2018,
  title          = {{ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases}},
  author         = {Pierre Stock and Moustapha Ciss{\'{e}}},
  year           = {2018},
  booktitle      = {Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part {VI}},
  pages          = {504--519},
  url            = {https://doi.org/10.1007/978-3-030-01231-1\_31},
  crossref       = {DBLP:conf/eccv/2018-6},
  timestamp      = {Tue, 14 May 2019 10:00:45 +0200},
  biburl         = {https://dblp.org/rec/bib/conf/eccv/StockC18},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{spelke2007,
  title          = {{Core knowledge}},
  author         = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
  year           = {2007},
  journal        = {Developmental Science},
  volume         = {10},
  number         = {1},
  pages          = {89--96},
  url            = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00569.x}
}
@misc{lispcode,
  title          = {{Course: 15-880(A) -- Introduction to Neural Networks}},
  author         = {Dave Touretzky and Alex Waibel},
  year           = {1995},
  url            = {shorturl.at/evKX9}
}
@inproceedings{langley00,
  title          = {{Crafting Papers on Machine Learning}},
  author         = {P. Langley},
  year           = {2000},
  booktitle      = {Proceedings of the 17th International Conference on Machine Learning (ICML 2000)},
  publisher      = {Morgan Kaufmann},
  address        = {Stanford, CA},
  pages          = {1207--1216},
  editor         = {Pat Langley}
}
@article{moore1965,
  title          = {{Cramming more components onto integrated circuits}},
  author         = {Moore, Gordon},
  year           = {1965},
  month          = {April},
  journal        = {Electronics},
  volume         = {38},
  number         = {8},
  url            = {https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf}
}
@article{10.1001/archopht.116.4.502,
  title          = {{Critical Periods and Amblyopia}},
  author         = {Daw, Nigel W.},
  year           = {1998},
  month          = {04},
  journal        = {Archives of Ophthalmology},
  volume         = {116},
  number         = {4},
  pages          = {502--505},
  issn           = {0003-9950},
  url            = {https://doi.org/10.1001/archopht.116.4.502}
}
@article{castro2020,
  title          = {{CryptoCredit: Securely Training Fair Models}},
  author         = {Leo de Castro and Jiahao Chen and Antigoni Polychroniadou},
  year           = {2020},
  journal        = {CoRR},
  volume         = {abs/2010.04840},
  url            = {https://arxiv.org/abs/2010.04840},
  eprinttype     = {arXiv},
  timestamp      = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2010-04840.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Bengio2009,
  title          = {{Curriculum Learning}},
  author         = {Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Collobert, Ronan and Weston, Jason},
  year           = {2009},
  booktitle      = {Proceedings of the 26th Annual International Conference on Machine Learning},
  location       = {Montreal, Quebec, Canada},
  publisher      = {ACM},
  address        = {New York, NY, USA},
  series         = {ICML '09},
  pages          = {41--48},
  url            = {http://doi.acm.org/10.1145/1553374.1553380},
  numpages       = {8},
  acmid          = {1553380}
}
@article{Cong2011,
  title          = {{Customizable Domain-Specific Computing}},
  author         = {Cong, J. and Sarkar, V. and Reinman, G. and Bui, A.},
  year           = {2011},
  journal        = {IEEE Design   Test of Computers},
  volume         = {28},
  number         = {2},
  pages          = {6--15}
}
@misc{DARPA018,
  title          = {{DARPA Announces Next Phase of Electronics Resurgence Initiative}},
  author         = {DARPA},
  year           = {2018},
  url            = {https://www.darpa.mil/news-events/2018-11-01a}
}
@article{2018Wang,
  title          = {{Data Dropout: Optimizing Training Data for Convolutional Neural Networks}},
  author         = {Wang, Tianyang and Huan, Jun and Li, Bo},
  year           = {2018},
  month          = {Sep},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1809.00193},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv180900193W},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{loshchilov2017decoupled,
  title          = {{Decoupled weight decay regularization}},
  author         = {Loshchilov, Ilya and Hutter, Frank},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1711.05101}
}
@article{claudiu2010,
  title          = {{Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition}},
  author         = {{Claudiu Ciresan}, Dan and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Juergen},
  year           = {2010},
  month          = mar,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1003.0358},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2010arXiv1003.0358C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2015Han,
  title          = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
  author         = {Han, S. and Mao, H. and Dally, W.~J.},
  year           = {2015},
  month          = oct,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2015arXiv151000149H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{han2016deep,
  title          = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
  author         = {Song Han and Huizi Mao and William J. Dally},
  year           = {2016}
}
@inproceedings{liu2015faceattributes,
  title          = {{Deep Learning Face Attributes in the Wild.}},
  author         = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year           = {2015},
  month          = {December},
  booktitle      = {ICCV},
  publisher      = {IEEE Computer Society},
  pages          = {3730--3738},
  url            = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
  added-at       = {2018-10-09T00:00:00.000+0200},
  biburl         = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
  ee             = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
  timestamp      = {2018-10-11T11:43:28.000+0200}
}
@article{2019badgeley,
  title          = {{Deep learning predicts hip fracture using confounding patient and healthcare variables}},
  author         = {Badgeley, Marcus and Zech, John and Oakden-Rayner, Luke and Glicksberg, Benjamin and Liu, Manway and Gale, William and McConnell, Michael and Percha, Bethany and Snyder, Thomas},
  year           = {2019},
  month          = {04},
  journal        = {npj Digital Medicine},
  volume         = {2},
  pages          = {31}
}
@article{deep-learning-scaling,
  title          = {{Deep Learning Scaling is Predictable, Empirically}},
  author         = {Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory F. Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1712.00409}
}
@article{DBLP:journals/corr/abs-2106-09647,
  title          = {{Deep Learning Through the Lens of Example Difficulty}},
  author         = {Robert J. N. Baldock and Hartmut Maennel and Behnam Neyshabur},
  year           = {2021},
  journal        = {CoRR},
  volume         = {abs/2106.09647},
  url            = {https://arxiv.org/abs/2106.09647},
  timestamp      = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2106-09647.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{coates13,
  title          = {{Deep learning with COTS HPC systems}},
  author         = {Adam Coates and Brody Huval and Tao Wang and David Wu and Bryan Catanzaro and Ng Andrew},
  year           = {2013},
  month          = {17--19 Jun},
  booktitle      = {Proceedings of the 30th International Conference on Machine Learning},
  publisher      = {PMLR},
  address        = {Atlanta, Georgia, USA},
  series         = {Proceedings of Machine Learning Research},
  volume         = {28},
  pages          = {1337--1345},
  url            = {http://proceedings.mlr.press/v28/coates13.html},
  editor         = {Sanjoy Dasgupta and David McAllester},
  pdf            = {http://proceedings.mlr.press/v28/coates13.pdf}
}

@misc{epoch2023trendsinthedollartrainingcostofmachinelearningsystems,
title={Trends in the Dollar Training Cost of Machine Learning Systems},
author={Ben Cottier},
year={2023},
url={https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems},
note={Accessed: 2024-05-28}
}


@inproceedings{davies2024,
author = {Davies, Michael and McDougall, Ian and Anandaraj, Selvaraj and Machchhar, Deep and Jain, Rithik and Sankaralingam, Karthikeyan},
title = {A Journey of a 1,000 Kernels Begins with a Single Step: A Retrospective of Deep Learning on GPUs},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640367},
doi = {10.1145/3620665.3640367},
abstract = {We are in age of AI, with rapidly changing algorithms and a somewhat synergistic change in hardware. MLPerf is a recent benchmark suite that serves as a way to compare and evaluate hardware. However it has several drawbacks - it is dominated by CNNs and does a poor job of capturing the diversity of AI use cases, and only represents a sliver of production AI use cases. This paper performs a longitudinal study of state-of-art AI applications spanning vision, physical simulation, vision synthesis, language and speech processing, and tabular data processing, across three generations of hardware to understand how the AI revolution has panned out. We call this collection of applications and execution scaffolding the CaSiO suite. The paper reports on data gathered at the framework level, device API level, and hardware and microarchitecture level. The paper provides insights on the hardware-software revolution with pointers to future trends.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {20–36},
numpages = {17},
location = {, La Jolla, CA, USA, },
series = {ASPLOS '24}
}



@ARTICLE{thompson2021,
author={Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
journal={IEEE Spectrum}, 
title={Deep Learning's Diminishing Returns: The Cost of Improvement is Becoming Unsustainable}, 
year={2021},
volume={58},
number={10},
pages={50-55},
keywords={Deep learning;Semiconductor device modeling;Proteins;Knowledge engineering;Moore's Law;Neurons;Tools},
doi={10.1109/MSPEC.2021.9563954}}
@misc{lepikhin2020gshard,
    title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, 
    author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    year={2020},
    eprint={2006.16668},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{epoch2023trendsinmachinelearninghardware,
title={Trends in Machine Learning Hardware},
author={Marius Hobbhahn and Lennart Heim and Gökçe Aydos},
year={2023},
url={https://epochai.org/blog/trends-in-machine-learning-hardware},
note={Accessed: 2024-05-28}
}

@misc{erdil2023algorithmic,
    title={Algorithmic progress in computer vision}, 
    author={Ege Erdil and Tamay Besiroglu},
    year={2023},
    eprint={2212.05153},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{ho2024algorithmic,
    title={Algorithmic progress in language models}, 
    author={Anson Ho and Tamay Besiroglu and Ege Erdil and David Owen and Robi Rahman and Zifan Carl Guo and David Atkinson and Neil Thompson and Jaime Sevilla},
    year={2024},
    eprint={2403.05812},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yoo2022scalable,
    title={Scalable Training of Language Models using JAX pjit and TPUv4}, 
    author={Joanna Yoo and Kuba Perlin and Siddhartha Rao Kamalakara and João G. M. Araújo},
    year={2022},
    eprint={2204.06514},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{fawzi2022discovering,
title={Discovering faster matrix multiplication algorithms with reinforcement learning},
author={Fawzi, Ali and Balog, Miklos and Huang, Alex and Song, Ziwei and Song, Yang and Vinyals, Oriol},
journal={Nature},
volume={610},
pages={47--53},
year={2022},
publisher={Nature Publishing Group}
}

@misc{uchicago_library,
title = {The Soviet Imaginary: {I}ndustrialization and Collectivization},
author= {Kathryn Duda},
url ={https://www.lib.uchicago.edu/collex/exhibits/soviet-imaginary/technology/industrialization-and-collectivization/},
journal = {University of Chicago Library},
publisher = {University of Chicago},
year = {2023}
}

@inbook{NBERc1425,
title = "Development Strategy and Planning: The Soviet Experience",
author = "Alexander Erlich",
BookTitle = "National Economic Planning",
Publisher = "NBER",
pages = "233-278",
year = "1967",
URL = "http://www.nber.org/chapters/c1425",
}

@book{Goodfellow-et-al-2016,
  title={Deep Learning},
  author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher={MIT Press},
  note={\url{http://www.deeplearningbook.org}},
  year={2016}
}

@article{2015_gupta,
  title          = {{Deep Learning with Limited Numerical Precision}},
  author         = {Suyog Gupta and Ankur Agrawal and Kailash Gopalakrishnan and Pritish Narayanan},
  year           = {2015},
  journal        = {CoRR},
  volume         = {abs/1502.02551},
  url            = {http://arxiv.org/abs/1502.02551},
  timestamp      = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/GuptaAGN15},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{jin2015deep,
  title          = {{Deep learning with s-shaped rectified linear activation units}},
  author         = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
  year           = {2015},
  journal        = {arXiv preprint arXiv:1512.07030}
}
@book{goodfellow2016deep,
  title          = {{Deep learning}},
  author         = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  year           = {2016},
  publisher      = {MIT Press},
  volume         = {1}
}
@article{2014Nguyen,
  title          = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
  author         = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year           = {2014},
  month          = {Dec},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1412.1897},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.1897N},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{He_2015,
  title          = {{Deep Residual Learning for Image Recognition}},
  author         = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year           = {2015},
  month          = dec,
  journal        = {ArXiv e-prints},
  booktitle      = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages          = {770--778},
  adsurl         = {http://adsabs.harvard.edu/abs/2015arXiv151203385H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{rn50,
  title          = {{Deep Residual Learning for Image Recognition}},
  author         = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year           = {2016},
  booktitle      = {2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages          = {770--778}
}
@article{bellec2017deep,
  title          = {{Deep rewiring: Training very sparse deep networks}},
  author         = {Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1711.05136}
}
@article{deep-rewiring,
  title          = {{Deep Rewiring: Training Very Sparse Deep Networks}},
  author         = {Guillaume Bellec and David Kappel and Wolfgang Maass and Robert A. Legenstein},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1711.05136}
}
@inproceedings{glorot2011deep,
  title          = {{Deep sparse rectifier neural networks}},
  author         = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year           = {2011},
  booktitle      = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages          = {315--323}
}
@inproceedings{he2015delving,
  title          = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
  author         = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year           = {2015},
  booktitle      = {Proceedings of the IEEE international conference on computer vision},
  pages          = {1026--1034}
}
@inproceedings{caoDeQAOnDeviceQuestion2019,
  title          = {{DeQA: On-Device Question Answering}},
  author         = {Cao, Qingqing and Weber, Noah and Balasubramanian, Niranjan and Balasubramanian, Aruna},
  year           = {2019},
  month          = jun,
  booktitle      = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
  publisher      = {Association for Computing Machinery},
  address        = {Seoul, Republic of Korea},
  series         = {{MobiSys} '19},
  pages          = {27--40},
  url            = {https://doi.org/10.1145/3307334.3326071},
  urldate        = {2020-05-07}
}
@article{2017Andre,
  title          = {{Dermatologist-level classification of skin cancer with deep neural networks}},
  author         = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto and Ko, Justin and M Swetter, Susan and M Blau, Helen and Thrun, Sebastian},
  year           = {2017},
  month          = {01},
  journal        = {Nature},
  volume         = {542}
}
@article{1050511,
  title          = {{Design of ion-implanted MOSFET's with very small physical dimensions}},
  author         = {Dennard, R. H. and Gaensslen, F. H. and Yu, H. and Rideout, V. L. and Bassous, E. and LeBlanc, A. R.},
  year           = {1974},
  journal        = {IEEE Journal of Solid-State Circuits},
  volume         = {9},
  number         = {5},
  pages          = {256--268}
}
@inproceedings{Bagdasaryan2019,
  title          = {{Differential Privacy Has Disparate Impact on Model Accuracy}},
  author         = {Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
  year           = {2019},
  booktitle      = {Advances in Neural Information Processing Systems},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  url            = {https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@misc{bagdasaryan2019differential,
  title          = {{Differential Privacy Has Disparate Impact on Model Accuracy}},
  author         = {Eugene Bagdasaryan and Vitaly Shmatikov},
  year           = {2019}
}
@article{1971Naturdistance_sets,
  title          = {{Distance between Sets}},
  author         = {{Levandowsky}, M.},
  year           = {1971},
  month          = nov,
  journal        = {\nat},
  volume         = {234},
  pages          = {34--35},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/1971Natur.234...34L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{sanh_distilbert_2019,
  title          = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
  author         = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year           = {2019},
  month          = aug,
  journal        = {arXiv:1910.01108 [cs]},
  url            = {http://arxiv.org/abs/1910.01108},
  urldate        = {2021-01-22},
  note           = {arXiv: 1910.01108}
}
@article{2015hinton,
  title          = {{Distilling the Knowledge in a Neural Network}},
  author         = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year           = {2015},
  month          = {Mar},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1503.02531},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2015arXiv150302531H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{zhang2019dive,
  title          = {{Dive into deep learning}},
  author         = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year           = {2019},
  journal        = {Unpublished Draft. Retrieved},
  volume         = {19},
  pages          = {2019}
}
@article{DBLPKornblith,
  title          = {{Do Better ImageNet Models Transfer Better?}},
  author         = {Simon Kornblith and Jonathon Shlens and Quoc V. Le},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1805.08974},
  url            = {http://arxiv.org/abs/1805.08974},
  timestamp      = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1805-08974},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{2018Nalisnick,
  title          = {{Do Deep Generative Models Know What They Don't Know?}},
  author         = {Nalisnick, Eric and Matsukawa, Akihiro and {Whye Teh}, Yee and Gorur, Dilan and Lakshminarayanan, Balaji},
  year           = {2018},
  month          = {Oct},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1810.09136},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009136N},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Recht2019,
  title          = {{Do ImageNet Classifiers Generalize to ImageNet?}},
  author         = {Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1902.10811},
  url            = {http://arxiv.org/abs/1902.10811},
  timestamp      = {Tue, 21 May 2019 18:03:38 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1902-10811},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{welling2019,
  title          = {{Do we still need models or just more data and compute?}},
  author         = {Max Welling},
  year           = {2019},
  url            = {shorturl.at/qABIY}
}
@article{feldman2019does,
  title          = {{Does learning require memorization? A short tale about a long tail}},
  author         = {Feldman, Vitaly},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1906.05271}
}
@article{DeVries2019,
  title          = {{Does Object Recognition Work for Everyone?}},
  author         = {Terrance DeVries and Ishan Misra and Changhan Wang and Laurens van der Maaten},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1906.02659}
}
@article{Tapo2021,
  title          = {{Domain-specific MT for Low-resource Languages: The case of Bambara-French}},
  author         = {Allahsera Auguste Tapo and Michael Leventhal and Sarah Luger and Christopher M. Homan and Marcos Zampieri},
  year           = {2021},
  journal        = {AfricaNLP Workshop},
  url            = {https://arxiv.org/abs/2104.00041}
}
@inproceedings{qu_dota_2022,
  title          = {{DOTA: detect and omit weak attentions for scalable transformer acceleration}},
  author         = {Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
  year           = {2022},
  month          = feb,
  booktitle      = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {{ASPLOS} 2022},
  pages          = {14--26},
  url            = {https://doi.org/10.1145/3503222.3507738},
  urldate        = {2022-05-12}
}
@article{false_discovery_rates,
  title          = {{Doubly Robust Internal Benchmarking and False Discovery Rates for Detecting Racial Bias in Police Stops}},
  author         = {Ridgeway, Greg and MacDonald, John},
  year           = {2009},
  month          = {06},
  journal        = {Journal of the American Statistical Association},
  publisher      = {Taylor \& Francis},
  volume         = {104},
  number         = {486},
  pages          = {661--668}
}
@article{2016DropNeuron,
  title          = {{DropNeuron: Simplifying the Structure of Deep Neural Networks}},
  author         = {Pan, W. and Dong, H. and Guo, Y.},
  year           = {2016},
  month          = jun,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2016arXiv160607326P},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{dropout-journal,
  title          = {{Dropout: a simple way to prevent neural networks from overfitting}},
  author         = {Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  year           = {2014},
  journal        = {Journal of Machine Learning Research},
  volume         = {15},
  number         = {1},
  pages          = {1929--1958}
}
@article{Hou2020DynaBERTDB,
  title          = {{DynaBERT: Dynamic BERT with Adaptive Width and Depth}},
  author         = {Lu Hou and Lifeng Shang and X. Jiang and Qun Liu},
  year           = {2020},
  journal        = {ArXiv},
  volume         = {abs/2004.04037}
}
@article{Xu2020DynamicCL,
  title          = {{Dynamic Curriculum Learning for Low-Resource Neural Machine Translation}},
  author         = {Chen Xu and Bojie Hu and Yu-fan Jiang and Kai Feng and Zeyang Wang and Shen Huang and Q. Ju and Tong Xiao and Jingbo Zhu},
  year           = {2020},
  journal        = {ArXiv},
  volume         = {abs/2011.14608}
}
@inproceedings{Guo2016Dynamic,
  title          = {{Dynamic Network Surgery for Efficient DNNs}},
  author         = {Yiwen Guo and Anbang Yao and Yurong Chen},
  year           = {2016},
  journal        = {CoRR},
  booktitle      = {NeurIPS},
  volume         = {abs/1608.04493},
  url            = {http://arxiv.org/abs/1608.04493},
  timestamp      = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/GuoYC16},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{NeurIPS2017_6975,
  title          = {{Dynamic Routing Between Capsules}},
  author         = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  year           = {2017},
  booktitle      = {Advances in Neural Information Processing Systems 30},
  pages          = {3856--3866},
  url            = {http://papers.NeurIPS.cc/paper/6975-dynamic-routing-between-capsules.pdf}
}
@article{xiao2018dynamical,
  title          = {{Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks}},
  author         = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
  year           = {2018},
  journal        = {arXiv preprint arXiv:1806.05393}
}
@inproceedings{williamson1991dynamically,
  title          = {{Dynamically scaled fixed point arithmetic}},
  author         = {Williamson, Darrell},
  year           = {1991},
  booktitle      = {[1991] IEEE Pacific Rim Conference on Communications, Computers and Signal Processing Conference Proceedings},
  pages          = {315--318},
  organization   = {IEEE}
}
@article{Chen2021EarlyBERTEB,
  title          = {{EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets}},
  author         = {Xiao-Han Chen and Yu Cheng and Shuohang Wang and Zhe Gan and Zhangyang Wang and Jing-jing Liu},
  year           = {2021},
  journal        = {ArXiv},
  volume         = {abs/2101.00063}
}
@article{tambe_edgebert_2021,
  title          = {{EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP}},
  author         = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Yang, En-Yu and Donato, Marco and Sanh, Victor and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
  year           = {2021},
  month          = feb,
  journal        = {arXiv:2011.14203 [cs]},
  url            = {http://arxiv.org/abs/2011.14203},
  urldate        = {2021-02-16},
  note           = {arXiv: 2011.14203 version: 3}
}
@misc{geEdgeFormerParameterEfficientTransformer2022,
  title          = {{EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation}},
  author         = {Ge, Tao and Wei, Furu},
  year           = {2022},
  month          = mar,
  publisher      = {arXiv},
  url            = {http://arxiv.org/abs/2202.07959},
  urldate        = {2022-07-04}
}
@article{bhandare_efficient_2019,
  title          = {{Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model}},
  author         = {Bhandare, Aishwarya and Sripathi, Vamsi and Karkada, Deepthi and Menon, Vivek and Choi, Sun and Datta, Kushal and Saletore, Vikram},
  year           = {2019},
  month          = jun,
  journal        = {arXiv:1906.00532 [cs]},
  url            = {http://arxiv.org/abs/1906.00532},
  urldate        = {2021-01-25},
  note           = {arXiv: 1906.00532}
}
@article{2017Gurumoorthy,
  title          = {{Efficient Data Representation by Selecting Prototypes with Importance Weights}},
  author         = {Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
  year           = {2017},
  month          = {Jul},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1707.01212},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv170701212G},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{li_efficient_2021,
  title          = {{Efficient Methods for Mapping Neural Machine Translator on FPGAs}},
  author         = {Li, Qin and Zhang, Xiaofan and Xiong, Jinjun and Hwu, Wen-Mei and Chen, Deming},
  year           = {2021},
  month          = jul,
  journal        = {IEEE Transactions on Parallel and Distributed Systems},
  volume         = {32},
  number         = {7},
  pages          = {1866--1877},
  issn           = {1558-2183},
  note           = {Conference Name: IEEE Transactions on Parallel and Distributed Systems}
}
@inproceedings{wavernn,
  title          = {{Efficient Neural Audio Synthesis}},
  author         = {Nal Kalchbrenner and Erich Elsen and Karen Simonyan and Seb Noury and Norman Casagrande and Edward Lockhart and Florian Stimberg and A{\"{a}}ron van den Oord and Sander Dieleman and Koray Kavukcuoglu},
  year           = {2018},
  booktitle      = {Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  pages          = {2415--2424}
}
@article{DBLP_Sze,
  title          = {{Efficient Processing of Deep Neural Networks: A Tutorial and Survey}},
  author         = {Vivienne Sze and Yu{-}Hsin Chen and Tien{-}Ju Yang and Joel S. Emer},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1703.09039},
  url            = {http://arxiv.org/abs/1703.09039}
}
@article{tay_efficient_2020,
  title          = {{Efficient Transformers: A Survey}},
  author         = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year           = {2020},
  month          = sep,
  journal        = {arXiv:2009.06732 [cs]},
  url            = {http://arxiv.org/abs/2009.06732},
  urldate        = {2020-09-16},
  note           = {arXiv: 2009.06732 version: 1}
}
@misc{2019EdgeTpu,
  title          = {{EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML}},
  author         = {Gupta, Suyog and Tan, Mingxing},
  year           = {2019},
  url            = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html}
}
@inproceedings{ham_elsa_2021,
  title          = {{ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks}},
  author         = {Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W.},
  year           = {2021},
  month          = jun,
  booktitle      = {2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {692--705},
  note           = {ISSN: 2575-713X}
}
@misc{Wei2022,
  title          = {{Emergent Abilities of Large Language Models}},
  author         = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year           = {2022},
  publisher      = {arXiv},
  url            = {https://arxiv.org/abs/2206.07682}
}
@article{zhou_energon_2022,
  title          = {{Energon: Towards Efficient Acceleration of Transformers Using Dynamic Sparse Attention}},
  author         = {Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and Sun, Guangyu},
  year           = {2022},
  journal        = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  pages          = {1--1},
  issn           = {1937-4151},
  note           = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}
}

@misc{2020cortexm,
  title          = {{Enhancing AI Performance for IoT Endpoint Devices}},
  author         = {ARM},
  year           = {2020},
  url            = {https://www.arm.com/company/news/2020/02/new-ai-technology-from-arm}
}
@inproceedings{liang2018enhancing,
  title          = {{Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}},
  author         = {Shiyu Liang and Yixuan Li and R. Srikant},
  year           = {2018},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=H1VGkIxRZ}
}
@article{Tulving2002,
  title          = {{Episodic Memory: From Mind to Brain}},
  author         = {Tulving, Endel},
  year           = {2002},
  journal        = {Annual Review of Psychology},
  volume         = {53},
  number         = {1},
  pages          = {1--25},
  url            = {https://doi.org/10.1146/annurev.psych.53.100901.135114},
  note           = {PMID: 11752477}
}
@inproceedings{Hardt2016,
  title          = {{Equality of Opportunity in Supervised Learning}},
  author         = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  year           = {2016},
  booktitle      = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  location       = {Barcelona, Spain},
  publisher      = {Curran Associates Inc.},
  address        = {USA},
  series         = {NeurIPS'16},
  pages          = {3323--3331},
  url            = {http://dl.acm.org/citation.cfm?id=3157382.3157469},
  numpages       = {9},
  acmid          = {3157469}
}
@article{ambrogio2018,
  title          = {{Equivalent-accuracy accelerated neural-network training using analogue memory}},
  author         = {Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert and Boybat, Irem and Nolfo, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan and Killeen, Benjamin and Cheng, Christina and Jaoudi, Yassine and Burr, Geoffrey},
  year           = {2018},
  month          = {06},
  journal        = {Nature},
  volume         = {558}
}
@article{2020arXiv200811600A,
  title          = {{Estimating Example Difficulty using Variance of Gradients}},
  author         = {Agarwal, Chirag and Hooker, Sara},
  year           = {2020},
  month          = aug,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2008.11600},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200811600A},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{ravula_etc_2020,
  title          = {{ETC: Encoding Long and Structured Inputs in Transformers}},
  author         = {Ravula, Anirudh and Alberti, Chris and Ainslie, Joshua and Yang, Li and Pham, Philip Minh and Wang, Qifan and Ontanon, Santiago and Sanghai, Sumit Kumar and Cvicek, Vaclav and Fisher, Zach},
  year           = {2020},
  booktitle      = {2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)}
}
@article{Goodman2016,
  title          = {{European Union regulations on algorithmic decision-making and a ``right to explanation''}},
  author         = {Goodman, B. and Flaxman, S.},
  year           = {2016},
  month          = jun,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2016arXiv160608813G},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{basta-etal-2019-evaluating,
  title          = {{Evaluating the Underlying Gender Bias in Contextualized Word Embeddings}},
  author         = {Basta, Christine  and Costa-juss{\`a}, Marta R.  and Casas, Noe},
  year           = {2019},
  month          = aug,
  booktitle      = {Proceedings of the First Workshop on Gender Bias in Natural Language Processing},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {33--39},
  url            = {https://aclanthology.org/W19-3805}
}
@article{samek2017,
  title          = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
  author         = {W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. Müller},
  year           = {2017},
  month          = {Nov},
  journal        = {IEEE Transactions on Neural Networks and Learning Systems},
  volume         = {28},
  number         = {11},
  pages          = {2660--2673},
  issn           = {2162-237X}
}
@article{Samala_2018,
  title          = {{Evolutionary pruning of transfer learned deep convolutional neural network for breast cancer diagnosis in digital breast tomosynthesis}},
  author         = {Ravi K Samala and Heang-Ping Chan and Lubomir M Hadjiiski and Mark A Helvie and Caleb Richter and Kenny Cha},
  year           = {2018},
  month          = {may},
  journal        = {Physics in Medicine {\&} Biology},
  publisher      = {{IOP} Publishing},
  volume         = {63},
  number         = {9},
  pages          = {095005}
}
@article{saxe2013exact,
  title          = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
  author         = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  year           = {2013},
  journal        = {arXiv preprint arXiv:1312.6120}
}
@incollection{NeurIPS2016_6300,
  title          = {{Examples are not enough, learn to criticize! Criticism for Interpretability}},
  author         = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  year           = {2016},
  booktitle      = {Advances in Neural Information Processing Systems 29},
  publisher      = {Curran Associates, Inc.},
  pages          = {2280--2288},
  url            = {http://papers.NeurIPS.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf},
  editor         = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{hoover_exbert_2020,
  title          = {{exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models}},
  author         = {Hoover, Benjamin and Strobelt, Hendrik and Gehrmann, Sebastian},
  year           = {2020},
  month          = jul,
  booktitle      = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {187--196},
  url            = {https://www.aclweb.org/anthology/2020.acl-demos.22},
  urldate        = {2021-01-27}
}
@article{Montavon2017,
  title          = {{Explaining nonlinear classification decisions with deep taylor decomposition}},
  author         = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year           = {2017},
  journal        = {Pattern Recognition},
  publisher      = {Elsevier},
  volume         = {65},
  pages          = {211--222},
  date-modified  = {2017-02-10 22:17:33 +0000}
}
@inproceedings{denton2014,
  title          = {{Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation}},
  author         = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  year           = {2014},
  booktitle      = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
  location       = {Montreal, Canada},
  publisher      = {MIT Press},
  address        = {Cambridge, MA, USA},
  series         = {NIPS'14},
  pages          = {1269–1277},
  numpages       = {9}
}
@inproceedings{xie2019exploring,
  title          = {{Exploring randomly wired neural networks for image recognition}},
  author         = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  year           = {2019},
  booktitle      = {Proceedings of the IEEE International Conference on Computer Vision},
  pages          = {1284--1293}
}
@book{tani2016exploring,
  title          = {{Exploring Robotic Minds: Actions, Symbols, and Consciousness as Self-organizing Dynamic Phenomena}},
  author         = {Tani, J. and Oxford University Press},
  year           = {2016},
  publisher      = {Oxford University Press},
  series         = {Oxford series on cognitive models and architectures},
  url            = {https://books.google.com/books?id=QswnnQAACAAJ},
  lccn           = {2016014889}
}
@article{2017Narang,
  title          = {{Exploring Sparsity in Recurrent Neural Networks}},
  author         = {Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  year           = {2017},
  month          = {Apr},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1704.05119},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv170405119N},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{exploring-sparsity-rnn,
  title          = {{Exploring Sparsity in Recurrent Neural Networks}},
  author         = {Sharan Narang and Gregory F. Diamos and Shubho Sengupta and Erich Elsen},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1704.05119}
}
@misc{raffel2020exploring,
  title          = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
  author         = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year           = {2020}
}
@article{2020arXiv200907453C,
  title          = {{Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation}},
  author         = {Chung, Insoo and Kim, Byeongwook and Choi, Yoonjung and Kwon, Se Jung and Jeon, Yongkweon and Park, Baeseong and Kim, Sangha and Lee, Dongsoo},
  year           = {2020},
  month          = sep,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2009.07453},
  url            = {http://arxiv.org/abs/2009.07453},
  urldate        = {2020-09-18},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200907453C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Uono2015,
  title          = {{Eye Contact Perception in the West and East: A Cross-Cultural Study}},
  author         = {Uono, Shota AND Hietanen, Jari K.},
  year           = {2015},
  month          = {02},
  journal        = {PLOS ONE},
  publisher      = {Public Library of Science},
  volume         = {10},
  number         = {2},
  pages          = {1--15},
  url            = {https://doi.org/10.1371/journal.pone.0118094}
}
@inproceedings{7551407Chen,
  title          = {{Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks}},
  author         = {Chen, Y. and Emer, J. and Sze, V.},
  year           = {2016},
  month          = {June},
  booktitle      = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {367--379},
  issn           = {1063-6897}
}
@inproceedings{Chen2018d,
  title          = {{Fair lending needs explainable models for responsible recommendation}},
  author         = {Jiahao Chen},
  year           = {2018},
  month          = {October 8},
  booktitle      = {FATREC'18 Proceedings of the Second Workshop on Responsible Recommendation},
  address        = {Vancouver, British Columbia, Canada}
}
@article{2016fair_prediction,
  title          = {{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}},
  author         = {Chouldechova, Alexandra},
  year           = {2016},
  month          = {Oct},
  journal        = {arXiv e-prints},
  publisher      = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
  volume         = {5},
  number         = {2},
  pages          = {arXiv:1610.07524},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv161007524C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2019arXiv190110566Z,
  title          = {{Fair Regression for Health Care Spending}},
  author         = {Zink, Anna and Rose, Sherri},
  year           = {2019},
  month          = {Jan},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1901.10566},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190110566Z},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{veale_2017,
  title          = {{Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data}},
  author         = {Michael Veale and Reuben Binns},
  year           = {2017},
  journal        = {Big Data \& Society},
  volume         = {4},
  number         = {2},
  pages          = {2053951717743530},
  url            = {https://doi.org/10.1177/2053951717743530}
}
@article{Sengupta2021FairlyPT,
  title          = {{Fairly Private Through Group Tagging and Relation Impact}},
  author         = {Poushali Sengupta and Subhankar Mishra},
  year           = {2021},
  journal        = {ArXiv},
  volume         = {abs/2105.07244}
}
@book{barocas-hardt-narayanan,
  title          = {{Fairness and Machine Learning}},
  author         = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  year           = {2019},
  publisher      = {fairmlbook.org},
  note           = {\url{http://www.fairmlbook.org}}
}
@inproceedings{hashimoto18a,
  title          = {{Fairness Without Demographics in Repeated Loss Minimization}},
  author         = {Hashimoto, Tatsunori and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
  year           = {2018},
  month          = {10--15 Jul},
  booktitle      = {Proceedings of the 35th International Conference on Machine Learning},
  publisher      = {PMLR},
  address        = {Stockholmsmässan, Stockholm Sweden},
  series         = {Proceedings of Machine Learning Research},
  volume         = {80},
  pages          = {1929--1938},
  url            = {http://proceedings.mlr.press/v80/hashimoto18a.html},
  editor         = {Dy, Jennifer and Krause, Andreas},
  pdf            = {http://proceedings.mlr.press/v80/hashimoto18a/hashimoto18a.pdf}
}
@inproceedings{langhammer_faithful_2013,
  title          = {{Faithful single-precision floating-point tangent for FPGAs}},
  author         = {Langhammer, Martin and Pasca, Bogdan},
  year           = {2013},
  month          = feb,
  booktitle      = {Proceedings of the ACM/SIGDA international symposium on Field programmable gate arrays},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {{FPGA} '13},
  pages          = {39--42},
  url            = {http://doi.org/10.1145/2435264.2435274},
  urldate        = {2021-10-31}
}
@article{jiang2019fantastic,
  title          = {{Fantastic generalization measures and where to find them}},
  author         = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1912.02178}
}
@article{xiao2017fashion,
  title          = {{Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms}},
  author         = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1708.07747}
}
@article{clevert2015fast,
  title          = {{Fast and accurate deep network learning by exponential linear units (elus)}},
  author         = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year           = {2015},
  journal        = {arXiv preprint arXiv:1511.07289}
}
@inproceedings{Elsen_2020_CVPR,
  title          = {{Fast Sparse ConvNets}},
  author         = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  year           = {2020},
  month          = {June},
  booktitle      = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@misc{elsen2019fast,
  title          = {{Fast Sparse ConvNets}},
  author         = {Erich Elsen and Marat Dukhan and Trevor Gale and Karen Simonyan},
  year           = {2019}
}
@article{2018Theis,
  title          = {{Faster gaze prediction with dense networks and Fisher pruning}},
  author         = {Theis, L. and Korshunova, I. and Tejani, A. and Husz{\'a}r, F.},
  year           = {2018},
  month          = jan,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2018arXiv180105787T},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{fisher-pruning,
  title          = {{Faster gaze prediction with dense networks and Fisher pruning}},
  author         = {Lucas Theis and Iryna Korshunova and Alykhan Tejani and Ferenc Husz{\'{a}}r},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1801.05787},
  url            = {http://arxiv.org/abs/1801.05787}
}
@article{fbnet,
  title          = {{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search}},
  author         = {Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1812.03443},
  url            = {http://arxiv.org/abs/1812.03443}
}
@article{olah2017feature,
  title          = {{Feature Visualization}},
  author         = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year           = {2017},
  journal        = {Distill},
  note           = {https://distill.pub/2017/feature-visualization}
}
@inproceedings{Chierichetti2010,
  title          = {{Finding the Jaccard Median}},
  author         = {Chierichetti, Flavio and Kumar, Ravi and Pandey, Sandeep and Vassilvitskii, Sergei},
  year           = {2010},
  month          = {01},
  journal        = {Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms},
  booktitle      = {Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages          = {293--311}
}
@article{liu2020finding,
  title          = {{Finding trainable sparse networks through Neural Tangent Transfer}},
  author         = {Liu, Tianlin and Zenke, Friedemann},
  year           = {2020},
  journal        = {arXiv preprint arXiv:2006.08228}
}
@article{holi210171,
  title          = {{Finite precision error analysis of neural network hardware implementations}},
  author         = {Holi, J. L. and Hwang, J.-N.},
  year           = {1993},
  journal        = {IEEE Transactions on Computers},
  volume         = {42},
  number         = {3},
  pages          = {281--290}
}
@article{ciresan2011,
  title          = {{Flexible, High Performance Convolutional Neural Networks for Image Classification.}},
  author         = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
  year           = {2011},
  month          = {07},
  journal        = {International Joint Conference on Artificial Intelligence IJCAI-2011},
  volume         = {abs/1102.0183},
  pages          = {1237--1242},
  url            = {http://arxiv.org/abs/1102.0183},
  timestamp      = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1102-0183},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{Lapedus2017,
  title          = {{Foundry Challenges In 2018}},
  author         = {Mark Lapedus},
  year           = {2017},
  url            = {https://semiengineering.com/foundry-challenges-in-2018/}
}
@article{7866802,
  title          = {{FPGAs versus GPUs in Data centers}},
  author         = {Falsafi, B. and Dally, B. and Singh, D. and Chiou, D. and Yi, J. J. and Sendag, R.},
  year           = {2017},
  journal        = {IEEE Micro},
  volume         = {37},
  number         = {1},
  pages          = {60--72}
}
@misc{1986Rosenblatt,
  title          = {{Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms}},
  author         = {Van Der Malsburg, C},
  year           = {1986},
  booktitle      = {Brain Theory},
  publisher      = {Springer Berlin Heidelberg},
  address        = {Berlin, Heidelberg},
  pages          = {245--248},
  editor         = {Palm, G{\"u}nther and Aertsen, Ad}
}
@inproceedings{li_ftrans_2020,
  title          = {{FTRANS: energy-efficient acceleration of transformers using FPGA}},
  author         = {Li, Bingbing and Pandey, Santosh and Fang, Haowen and Lyv, Yanjun and Li, Ji and Chen, Jieyang and Xie, Mimi and Wan, Lipeng and Liu, Hang and Ding, Caiwen},
  year           = {2020},
  month          = aug,
  booktitle      = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {ISLPED '20},
  pages          = {175--180},
  url            = {http://doi.org/10.1145/3370748.3406567},
  urldate        = {2020-12-01}
}
@article{prato_fully_2020,
  title          = {{Fully Quantized Transformer for Machine Translation}},
  author         = {Prato, Gabriele and Charlaix, Ella and Rezagholizadeh, Mehdi},
  year           = {2020},
  month          = mar,
  journal        = {arXiv:1910.10485 [cs, stat]},
  url            = {http://arxiv.org/abs/1910.10485},
  urldate        = {2021-01-03},
  note           = {arXiv: 1910.10485 version: 3}
}
@book{kolb2009fundamentals,
  title          = {{Fundamentals of Human Neuropsychology}},
  author         = {Kolb, B. and Whishaw, I.Q.},
  year           = {2009},
  publisher      = {Worth Publishers},
  series         = {A series of books in psychology},
  url            = {https://books.google.com/books?id=z0DThNQqdL4C},
  lccn           = {2007924870}
}
@inproceedings{zhao-etal-2018-gender,
  title          = {{Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods}},
  author         = {Zhao, Jieyu  and Wang, Tianlu  and Yatskar, Mark  and Ordonez, Vicente  and Chang, Kai-Wei},
  year           = {2018},
  month          = jun,
  booktitle      = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  publisher      = {Association for Computational Linguistics},
  address        = {New Orleans, Louisiana},
  pages          = {15--20},
  url            = {https://aclanthology.org/N18-2003}
}
@inproceedings{Hamidi2018,
  title          = {{Gender Recognition or Gender Reductionism? The Social Implications of Embedded Gender Recognition Systems}},
  author         = {Hamidi, Foad and Scheuerman, Morgan Klaus and Branham, Stacy M.},
  year           = {2018},
  booktitle      = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  location       = {Montreal QC, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {CHI '18},
  pages          = {1–13},
  url            = {https://doi.org/10.1145/3173574.3173582},
  numpages       = {13}
}
@inproceedings{buolamwini2018gender,
  title          = {{Gender shades: Intersectional accuracy disparities in commercial gender classification}},
  author         = {Buolamwini, Joy and Gebru, Timnit},
  year           = {2018},
  booktitle      = {Conference on fairness, accountability and transparency},
  pages          = {77--91}
}
@inproceedings{pmlrbuolamwini18a,
  title          = {{Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification}},
  author         = {Joy Buolamwini and Timnit Gebru},
  year           = {2018},
  month          = {23--24 Feb},
  booktitle      = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  publisher      = {PMLR},
  address        = {New York, NY, USA},
  series         = {Proceedings of Machine Learning Research},
  volume         = {81},
  pages          = {77--91},
  url            = {http://proceedings.mlr.press/v81/buolamwini18a.html},
  editor         = {Sorelle A. Friedler and Christo Wilson}
}
@article{Florenta2014,
  title          = {{Generalists, Specialists, and the Direction of Inventive Activity}},
  author         = {Teodoridis, Florenta},
  year           = {2014},
  month          = {01},
  journal        = {SSRN Electronic Journal}
}
@incollection{NeurIPS1990_Andreas_weight_elimination,
  title          = {{Generalization by Weight-Elimination with Application to Forecasting}},
  author         = {Andreas S. Weigend and David E. Rumelhart and Bernardo A. Huberman},
  year           = {1991},
  booktitle      = {Advances in Neural Information Processing Systems 3},
  publisher      = {Morgan-Kaufmann},
  pages          = {875--882},
  editor         = {R. P. Lippmann and J. E. Moody and D. S. Touretzky}
}
@article{child_generating_2019,
  title          = {{Generating Long Sequences with Sparse Transformers}},
  author         = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year           = {2019},
  month          = apr,
  journal        = {arXiv:1904.10509 [cs, stat]},
  url            = {http://arxiv.org/abs/1904.10509},
  urldate        = {2020-09-17},
  note           = {arXiv: 1904.10509 version: 1}
}
@inproceedings{huangGhostBERTGenerateMore2021,
  title          = {{GhostBERT: Generate More Features with Cheap Operations for BERT}},
  author         = {Huang, Zhiqi and Hou, Lu and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  year           = {2021},
  month          = aug,
  booktitle      = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {6512--6523},
  url            = {https://aclanthology.org/2021.acl-long.509},
  urldate        = {2022-07-04}
}
@inproceedings{zadeh_gobo_2020,
  title          = {{GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference}},
  author         = {Zadeh, A. H. and Edo, I. and Awad, O. M. and Moshovos, A.},
  year           = {2020},
  month          = oct,
  booktitle      = {2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages          = {811--824}
}
@inproceedings{7298594,
  title          = {{Going deeper with convolutions}},
  author         = {Szegedy, C. and  Liu, Wei and Jia, Yangqing and Sermanet, P. and Reed, S. and Anguelov, D. and Erhan, D. and Vanhoucke, V. and Rabinovich, A.},
  year           = {2015},
  booktitle      = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages          = {1--9}
}
@book{1986agostino,
  title          = {{Goodness-of-fit Techniques}},
  year           = {1986},
  publisher      = {Marcel Dekker, Inc.},
  address        = {New York, NY, USA},
  editor         = {D'Agostino, Ralph B and Stephens, Michael A}
}

@book{2002huber,
  title          = {{Goodness-of-Fit Tests and Model Validity}},
  author         = {Huber-Carol, C. and Balakrishnan, N. and Nikulin, M. and Mesbah, M.},
  year           = {2002},
  publisher      = {Birkh{\"a}user Boston},
  series         = {Goodness-of-fit Tests and Model Validity},
  url            = {https://books.google.com/books?id=gUMcv2\_NrhkC},
  lccn           = {2002022647}
}

@misc{Gray2017GPUKF,
  title          = {{GPU Kernels for Block-Sparse Weights}},
  author         = {Scott Gray and A. Radford and Diederik P. Kingma},
  year           = {2017}
}

@article{2014Mark1,
  title          = {{Grace Hopper, computing pioneer}},
  author         = {Walter Isaacson},
  year           = {2014},
  journal        = {The Harvard Gazette},
  url            = {https://news.harvard.edu/gazette/story/2014/12/grace-hopper-computing-pioneer/}
}

@inproceedings{gradcam2017,
  title          = {{Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization}},
  author         = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year           = {2017},
  month          = {Oct},
  booktitle      = {The IEEE International Conference on Computer Vision (ICCV)}
}

@misc{hochreiter2001gradient,
  title          = {{Gradient flow in recurrent nets: the difficulty of learning long-term dependencies}},
  author         = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
  year           = {2001},
  publisher      = {A field guide to dynamical recurrent neural networks. IEEE Press}
}

@article{evci2020gradient,
  title          = {{Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win}},
  author         = {Evci, Utku and Ioannou, Yani A and Keskin, Cem and Dauphin, Yann},
  year           = {2020},
  journal        = {arXiv preprint arXiv:2010.03533}
}

@article{lenet,
  title          = {{Gradient-based learning applied to document recognition}},
  author         = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  year           = {1998},
  month          = {Nov},
  journal        = {Proceedings of the IEEE},
  volume         = {86},
  number         = {11},
  pages          = {2278--2324},
  issn           = {0018-9219}
}

@inproceedings{chen2018gradnorm,
  title          = {{Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks}},
  author         = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  year           = {2018},
  booktitle      = {International Conference on Machine Learning},
  pages          = {794--803},
  organization   = {PMLR}
}

@article{taiji_1998,
  title          = {{Grape-4: A Teraflops Machine for N-Body Simulations}},
  author         = {Taiji, Makoto},
  year           = {1998},
  journal        = {Highlights of Astronomy},
  publisher      = {Cambridge University Press},
  volume         = {11},
  number         = {2},
  pages          = {600–602}
}

@book{diamond98,
  title          = {{Guns, Germs, and Steel: The Fates of Human Societies}},
  author         = {Diamond, J.M. and Diamond, P.G.J. and Bernard Hames Collection},
  year           = {1999},
  publisher      = {W.W. Norton},
  series         = {National bestseller / W.W. Norton \& Company},
  url            = {https://books.google.com/books?id=1lBu\_bqSsSMC},
  lccn           = {96037068}
}

@book{mcdonald2009handbook,
  title          = {{Handbook of biological statistics}},
  author         = {McDonald, John H},
  year           = {2009},
  publisher      = {sparky house publishing Baltimore, MD},
  volume         = {2}
}

@article{Wang2018HAQHA,
  title          = {{HAQ: Hardware-Aware Automated Quantization}},
  author         = {Kuan Wang and Zhijian Liu and Yujun Lin and Ji Lin and Song Han},
  year           = {2018},
  journal        = {ArXiv},
  volume         = {abs/1811.08886}
}

@article{liu_hardware_2021,
  title          = {{Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing}},
  author         = {Liu, Zejian and Li, Gang and Cheng, Jian},
  year           = {2021},
  month          = mar,
  journal        = {arXiv:2103.02800 [cs]},
  url            = {http://arxiv.org/abs/2103.02800},
  urldate        = {2021-06-28},
  note           = {arXiv: 2103.02800}
}

@article{lu_hardware_2020,
  title          = {{Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer}},
  author         = {Lu, Siyuan and Wang, Meiqi and Liang, Shuang and Lin, Jun and Wang, Zhongfeng},
  year           = {2020},
  month          = sep,
  journal        = {arXiv:2009.08605 [cs, eess]},
  url            = {http://arxiv.org/abs/2009.08605},
  urldate        = {2020-12-01},
  note           = {arXiv: 2009.08605 version: 1}
}

@misc{Dong2019,
  title          = {{HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision}},
  author         = {Zhen, Dong and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Keutzer, Kurt},
  year           = {2019},
  month          = {10},
  pages          = {293--302}
}

@article{2019oakden,
  title          = {{Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging}},
  author         = {Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  year           = {2019},
  month          = {Sep},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1909.12475},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190912475O},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{srivastava2015highway,
  title          = {{Highway networks}},
  author         = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  year           = {2015},
  journal        = {arXiv preprint arXiv:1505.00387}
}
@misc{kingsburyhipnet,
  title          = {{HiPNeT-1: A Highly Pipelined Architecture for Neural Network Training}},
  author         = {Kingsbury, Brian and Morgan, Nelson and Wawrzynek, John},
  year           = {1998},
  month          = {03}
}
@article{ramsauer_hopfield_2020,
  title          = {{Hopfield Networks is All You Need}},
  author         = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  year           = {2020},
  month          = dec,
  journal        = {arXiv:2008.02217 [cs, stat]},
  url            = {http://arxiv.org/abs/2008.02217},
  urldate        = {2021-01-22},
  note           = {arXiv: 2008.02217}
}
@article{scheuerman2019how,
  title          = {{How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services}},
  author         = {Scheuerman, Morgan Klaus and Paul, Jacob M. and Brubaker, Jed R.},
  year           = {2019},
  month          = nov,
  journal        = {Proc. ACM Hum.-Comput. Interact.},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  volume         = {3},
  number         = {CSCW},
  url            = {https://doi.org/10.1145/3359246},
  issue_date     = {November 2019},
  articleno      = {144},
  numpages       = {33}
}
@article{roberts2020,
  title          = {{How Much Knowledge Can You Pack Into the Parameters of a Language Model?}},
  author         = {Adam Roberts and Colin Raffel and Noam Shazeer},
  year           = {2020},
  journal        = {CoRR},
  volume         = {abs/2002.08910},
  url            = {https://arxiv.org/abs/2002.08910},
  eprinttype     = {arXiv},
  timestamp      = {Mon, 02 Mar 2020 16:46:06 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2002-08910.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{PMID15632230,
  title          = {{How the brain decides what we see}},
  author         = {Smythies, John},
  year           = {2005},
  month          = {January},
  journal        = {Journal of the Royal Society of Medicine},
  volume         = {98},
  number         = {1},
  pages          = {18—20},
  issn           = {0141-0768},
  url            = {https://europepmc.org/articles/PMC1079232}
}
@article{macia2014,
  title          = {{How to Make a Synthetic Multicellular Computer}},
  author         = {Macía, Javier and Sole, Ricard},
  year           = {2014},
  month          = {02},
  journal        = {PloS one},
  volume         = {9},
  pages          = {e81248}
}
@inbook{2003Gitelman,
  title          = {{How Users Define New Media: A History of the Amusement Phonograph}},
  author         = {Lisa Gitelman},
  year           = {2003},
  booktitle      = {Rethinking Media Change},
  publisher      = {MIT Press},
  language       = {English (US)},
  editor         = {Thorburn, {David } and Jenkins, {Henry }}
}
@article{kim_i-bert_2021,
  title          = {{I-BERT: Integer-only BERT Quantization}},
  author         = {Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  year           = {2021},
  month          = jan,
  journal        = {arXiv:2101.01321 [cs]},
  url            = {http://arxiv.org/abs/2101.01321},
  urldate        = {2021-01-06},
  note           = {arXiv: 2101.01321}
}

@misc{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset}, 
    author={Sara Beery and Elijah Cole and Arvi Gjoka},
    year={2020},
    eprint={2004.10340},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{li2017deeper,
    title={Deeper, Broader and Artier Domain Generalization}, 
    author={Da Li and Yongxin Yang and Yi-Zhe Song and Timothy M. Hospedales},
    year={2017},
    eprint={1710.03077},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{hendrycks2019deep,
    title={Deep Anomaly Detection with Outlier Exposure}, 
    author={Dan Hendrycks and Mantas Mazeika and Thomas Dietterich},
    year={2019},
    eprint={1812.04606},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{tan2024scattered,
    title={Scattered Mixture-of-Experts Implementation}, 
    author={Shawn Tan and Yikang Shen and Rameswar Panda and Aaron Courville},
    year={2024},
    eprint={2403.08245},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hendrycks2018baseline,
    title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}, 
    author={Dan Hendrycks and Kevin Gimpel},
    year={2018},
    eprint={1610.02136},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{Ahmed_Courville_2020, title={Detecting Semantic Anomalies}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5712}, DOI={10.1609/aaai.v34i04.5712}, abstractNote={&lt;p&gt;We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that out-distributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ahmed, Faruk and Courville, Aaron}, year={2020}, month={Apr.}, pages={3154-3162} }

@article{Carlini2018,
author       = {Nicholas Carlini and
                David A. Wagner},
title        = {Towards Evaluating the Robustness of Neural Networks},
journal      = {CoRR},
volume       = {abs/1608.04644},
year         = {2016},
url          = {http://arxiv.org/abs/1608.04644},
eprinttype    = {arXiv},
eprint       = {1608.04644},
timestamp    = {Mon, 13 Aug 2018 16:46:14 +0200},
biburl       = {https://dblp.org/rec/journals/corr/CarliniW16a.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{daume-iii-2007-frustratingly,
  title = "Frustratingly Easy Domain Adaptation",
  author = "Daum{\'e} III, Hal",
  editor = "Zaenen, Annie  and
    van den Bosch, Antal",
  booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
  month = jun,
  year = "2007",
  address = "Prague, Czech Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P07-1033",
  pages = "256--263",
}

@inproceedings{
Sagawa*2020Distributionally,
title={Distributionally Robust Neural Networks},
author={Shiori Sagawa* and Pang Wei Koh* and Tatsunori B. Hashimoto and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryxGuJrFvS}
}

@article{2019arXiv190802900M,
  title          = {{iCassava 2019Fine-Grained Visual Categorization Challenge}},
  author         = {Mwebaze, Ernest and Gebru, Timnit and Frome, Andrea and Nsumba, Solomon and Tusubira, Jeremy},
  year           = {2019},
  month          = {Aug},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1908.02900},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190802900M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{parmar_image_2018,
  title          = {{Image Transformer}},
  author         = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year           = {2018},
  month          = jul,
  booktitle      = {Proceedings of the 35th International Conference on Machine Learning},
  publisher      = {PMLR},
  address        = {Stockholmsmässan, Stockholm Sweden},
  series         = {Proceedings of Machine Learning Research},
  volume         = {80},
  pages          = {4055--4064},
  url            = {http://proceedings.mlr.press/v80/parmar18a.html},
  editor         = {Dy, Jennifer and Krause, Andreas}
}

@misc{NeurIPS2012_4824,
  title          = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  author         = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  year           = {2012},
  booktitle      = {Advances in Neural Information Processing Systems 25},
  pages          = {1097--1105},
  url            = {https://bit.ly/2GneDwp}
}

@article{Russakovsky2015,
  title          = {{ImageNet Large Scale Visual Recognition Challenge}},
  author         = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  year           = {2015},
  month          = {Dec},
  day            = {01},
  journal        = {International Journal of Computer Vision},
  volume         = {115},
  number         = {3},
  pages          = {211--252},
  issn           = {1573-1405},
  url            = {https://doi.org/10.1007/s11263-015-0816-y}
}

@inproceedings{imagenet_cvpr09,
  title          = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  author         = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  year           = {2009},
  booktitle      = {CVPR09},
  bibsource      = {http://www.image-net.org/papers/imagenet_cvpr09.bib}
}

@article{Lee2011,
  title          = {{Implementing Domain-Specific Languages for Heterogeneous Parallel Computing}},
  author         = {Lee, H. and Brown, K. and Sujeeth, A. and Chafi, H. and Rompf, T. and Odersky, M. and Olukotun, K.},
  year           = {2011},
  journal        = {IEEE Micro},
  volume         = {31},
  number         = {5},
  pages          = {42--53}
}

@article{RossFinale2017,
  title          = {{Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients}},
  author         = {Andrew Slavin Ross and Finale Doshi{-}Velez},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1711.09404}
}

@inproceedings{Vanhoucke_2011,
  title          = {{Improving the speed of neural networks on CPUs}},
  author         = {Vincent Vanhoucke and Andrew Senior and Mark Z. Mao},
  year           = {2011},
  booktitle      = {Deep Learning and Unsupervised Feature Learning Workshop, NeurIPS 2011}
}

@inproceedings{10.1145/3079856.3080246,
  title          = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
  author         = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year           = {2017},
  month          = jun,
  journal        = {SIGARCH Comput. Archit. News},
  booktitle      = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  location       = {Toronto, ON, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {ISCA '17},
  volume         = {45},
  number         = {2},
  pages          = {1–12},
  issn           = {0163-5964},
  url            = {https://doi.org/10.1145/3079856.3080246},
  numpages       = {12},
  issue_date     = {May 2017}
}

@article{2016Szegedy,
  title          = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
  author         = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
  year           = {2016},
  month          = {Feb},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1602.07261},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207261S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{wilcoxon1945individual,
  title          = {{Individual Comparisons by Ranking Methods}},
  author         = {Wilcoxon, Frank},
  year           = {1945},
  journal        = {Biometrics Bulletin},
  publisher      = {JSTOR},
  volume         = {1},
  number         = {6},
  pages          = {80--83}
}

@article{kleinberg2016inherent,
  title          = {{Inherent trade-offs in the fair determination of risk scores}},
  author         = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  year           = {2016},
  journal        = {arXiv preprint arXiv:1609.05807}
}

@inproceedings{CameronJones1995InstanceSB,
  title          = {{Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing}},
  author         = {R. Mike Cameron-Jones},
  year           = {1995}
}

@article{Aha1991,
  title          = {{Instance-based learning algorithms}},
  author         = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
  year           = {1991},
  month          = {Jan},
  day            = {01},
  journal        = {Machine Learning},
  volume         = {6},
  number         = {1},
  pages          = {37--66},
  url            = {https://doi.org/10.1007/BF00153759}
}

@misc{intelavxbf16,
  title          = {{Intel® Deep Learning Boost New Deep Learning Instruction bfloat16}},
  author         = {Intel},
  year           = {2020},
  month          = jun,
  journal        = {Intel},
  url            = {https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html},
  urldate        = {2022-06-23},
  language       = {en}
}

@article{kim2017,
  title          = {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
  author         = {Kim, B. and Wattenberg, M. and Gilmer, J. and Cai, C. and Wexler, J. and Viegas, F. and Sayres, R.},
  year           = {2017},
  month          = nov,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv171111279K},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Fong2017,
  title          = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
  author         = {Ruth C. Fong and Andrea Vedaldi},
  year           = {2017},
  booktitle      = {{ICCV}},
  publisher      = {{IEEE} Computer Society},
  pages          = {3449--3457}
}

@article{mobilenetv2,
  title          = {{Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation}},
  author         = {Mark Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang{-}Chieh Chen},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1801.04381}
}

@inproceedings{agic-vulic-2019-jw300,
  title          = {{JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages}},
  author         = {Agi{\'c}, {\v{Z}}eljko  and Vuli{\'c}, Ivan},
  year           = {2019},
  month          = jul,
  booktitle      = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {3204--3210},
  url            = {https://www.aclweb.org/anthology/P19-1310}
}

@article{tessera2021keep,
  title          = {{Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization}},
  author         = {Tessera, Kale{-}ab and Hooker, Sara and Rosman, Benjamin},
  year           = {2021},
  journal        = {arXiv e-prints},
  pages          = {arXiv:2102.01670}
}

@misc{keras_pruning,
  title          = {{Keras Tensorflow Magnitude Pruning Open Source Code}},
  author         = {{Keras TensorFlow}},
  note           = {Accessed: 2019-07-10},
  howpublished   = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}}
}

@article{Montavon2011,
  title          = {{Kernel analysis of deep networks}},
  author         = {Montavon, Gregoire and Braun, Mikio L and M{\"u}ller, Klaus-Robert},
  year           = {2011},
  journal        = {Journal of Machine Learning Research},
  volume         = {12},
  number         = {Sep},
  pages          = {2563--2581},
  date-added     = {2016-10-12 11:13:21 +0000},
  date-modified  = {2016-10-19 16:36:27 +0000}
}

@article{2020brown,
  title          = {{Language Models are Few-Shot Learners}},
  author         = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year           = {2020},
  month          = may,
  journal        = {arXiv e-prints}
}

@article{DBLP:journals/corr/abs-2005-14165,
  title          = {{Language Models are Few-Shot Learners}},
  author         = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert{-}Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year           = {2020},
  journal        = {CoRR},
  volume         = {abs/2005.14165},
  url            = {https://arxiv.org/abs/2005.14165},
  eprinttype     = {arXiv},
  timestamp      = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}

@article{2019arXiv190405160L,
  title          = {{Large-Scale Long-Tailed Recognition in an Open World}},
  author         = {Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X.},
  year           = {2019},
  month          = apr,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1904.05160},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190405160L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{ba2016layer,
  title          = {{Layer normalization}},
  author         = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  year           = {2016},
  journal        = {arXiv preprint arXiv:1607.06450}
}

@inproceedings{lwac,
  title          = {{Learning both Weights and Connections for Efficient Neural Network}},
  author         = {Song Han and Jeff Pool and John Tran and William J. Dally},
  year           = {2015},
  booktitle      = {{NeurIPS}},
  pages          = {1135--1143}
}

@inproceedings{Han2015,
  title          = {{Learning Both Weights and Connections for Efficient Neural Networks}},
  author         = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  year           = {2015},
  booktitle      = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  location       = {Montreal, Canada},
  publisher      = {MIT Press},
  address        = {Cambridge, MA, USA},
  series         = {NeurIPS'15},
  pages          = {1135--1143},
  url            = {http://dl.acm.org/citation.cfm?id=2969239.2969366},
  numpages       = {9},
  acmid          = {2969366}
}

@misc{han2015learning,
  title          = {{Learning both Weights and Connections for Efficient Neural Networks}},
  author         = {Song Han and Jeff Pool and John Tran and William J. Dally},
  year           = {2015}
}
@inproceedings{shenLearningCompressedSentence2019,
  title          = {{Learning Compressed Sentence Representations for On-Device Text Processing}},
  author         = {Shen, Dinghan and Cheng, Pengyu and Sundararaman, Dhanasekar and Zhang, Xinyuan and Yang, Qian and Tang, Meng and Celikyilmaz, Asli and Carin, Lawrence},
  year           = {2019},
  month          = jul,
  booktitle      = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {107--116},
  url            = {https://aclanthology.org/P19-1011},
  urldate        = {2022-07-05}
}
@article{2017Liu,
  title          = {{Learning Efficient Convolutional Networks through Network Slimming}},
  author         = {Liu, Z. and Li, J. and Shen, Z. and Huang, G. and Yan, S. and Zhang, C.},
  year           = {2017},
  month          = aug,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170806519L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{network-slimming,
  title          = {{Learning Efficient Convolutional Networks through Network Slimming}},
  author         = {Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng Yan and Changshui Zhang},
  year           = {2017},
  booktitle      = {IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017},
  pages          = {2755--2763}
}
@article{Li2020LearningLT,
  title          = {{Learning Light-Weight Translation Models from Deep Transformer}},
  author         = {Bei Li and Ziyang Wang and H. Liu and Quan Du and Tong Xiao and Chunliang Zhang and Jingbo Zhu},
  year           = {2020},
  journal        = {ArXiv},
  volume         = {abs/2012.13866}
}
@article{bengio1994learning,
  title          = {{Learning long-term dependencies with gradient descent is difficult}},
  author         = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  year           = {1994},
  journal        = {IEEE transactions on neural networks},
  publisher      = {IEEE},
  volume         = {5},
  number         = {2},
  pages          = {157--166}
}
@article{1963steinbuch,
  title          = {{Learning matrices and their applications}},
  author         = {K, Steinbuch and U. Piske},
  year           = {1963},
  journal        = {IEEE Transactions on Electronic Computers},
  volume         = {EC-12},
  number         = {6},
  pages          = {846--862}
}
@article{Krizhevsky09learningmultiple,
  title          = {{Learning Multiple Layers of Features from Tiny Images}},
  author         = {Krizhevsky, Alex},
  year           = {2012},
  month          = {05},
  journal        = {University of Toronto}
}
@inbook{1988rumelhart,
  title          = {{Learning Representations by Back-Propagating Errors}},
  author         = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year           = {1988},
  booktitle      = {Neurocomputing: Foundations of Research},
  publisher      = {MIT Press},
  pages          = {696–699},
  numpages       = {4}
}
@article{Louizos2017Learning,
  title          = {{Learning Sparse Neural Networks through $L_0$ Regularization}},
  author         = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  year           = {2017},
  month          = dec,
  journal        = {ArXiv e-prints},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{tartaglione2018learning,
  title          = {{Learning sparse neural networks via sensitivity-driven regularization}},
  author         = {Tartaglione, Enzo and Leps{\o}y, Skjalg and Fiandrotti, Attilio and Francini, Gianluca},
  year           = {2018},
  booktitle      = {Advances in neural information processing systems},
  pages          = {3878--3888}
}
@article{2016learnedSparsity,
  title          = {{Learning Structured Sparsity in Deep Neural Networks}},
  author         = {Wen, W. and Wu, C. and Wang, Y. and Chen, Y. and Li, H.},
  year           = {2016},
  month          = aug,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2016arXiv160803665W},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{wang2017learning,
  title          = {{Learning to model the tail}},
  author         = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
  year           = {2017},
  booktitle      = {Advances in Neural Information Processing Systems},
  pages          = {7029--7039}
}
@inproceedings{Corinna2016,
  title          = {{Learning with Rejection}},
  author         = {Corinna Cortes and Giulia DeSalvo and Mehryar Mohri},
  year           = {2016},
  booktitle      = {ALT}
}
@article{2018Liu,
  title          = {{Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks}},
  author         = {Liu, Y. and Chen, J. and Chen, H.},
  year           = {2018},
  month          = jan,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2018arXiv180102850L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Leibig2017,
  title          = {{Leveraging uncertainty information from deep neural networks for disease detection}},
  author         = {Leibig, Christian and Allken, Vaneeda and Ayhan, Murat Seckin and Berens, Philipp and Wahl, Siegfried},
  year           = {2017},
  month          = {12},
  journal        = {Scientific Reports},
  volume         = {7}
}
@misc{desaiLightweightConvolutionalRepresentations2020,
  title          = {{Lightweight Convolutional Representations for On-Device Natural Language Processing}},
  author         = {Desai, Shrey and Goh, Geoffrey and Babu, Arun and Aly, Ahmed},
  year           = {2020},
  month          = feb,
  publisher      = {arXiv},
  url            = {http://arxiv.org/abs/2002.01535},
  urldate        = {2022-07-04},
  annote         = {Comment: Accepted to MLSys 2020}
}
@article{frankle2019linear,
  title          = {{Linear mode connectivity and the lottery ticket hypothesis}},
  author         = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1912.05671}
}
@inproceedings{wu*LiteTransformerLongShort2022,
  title          = {{Lite Transformer with Long-Short Range Attention}},
  author         = {Wu*, Zhanghao and Liu*, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  year           = {2022},
  month          = feb,
  url            = {https://openreview.net/forum?id=ByeMPlHKPH},
  urldate        = {2022-07-04},
  language       = {en}
}
@book{bruce1991,
  title          = {{Little Engines That Could’ve: The Calculating Machines of Charles Babbage}},
  author         = {Collier, Bruce},
  year           = {1991},
  publisher      = {Garland Publishing, Inc.},
  address        = {USA}
}
@article{beltagy_longformer_2020,
  title          = {{Longformer: The Long-Document Transformer}},
  author         = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year           = {2020},
  month          = apr,
  journal        = {arXiv:2004.05150 [cs]},
  url            = {http://arxiv.org/abs/2004.05150},
  urldate        = {2020-11-13},
  note           = {arXiv: 2004.05150}
}
@article{Sowell8223,
  title          = {{Longitudinal Mapping of Cortical Thickness and Brain Growth in Normal Children}},
  author         = {Sowell, Elizabeth R. and Thompson, Paul M. and Leonard, Christiana M. and Welcome, Suzanne E. and Kan, Eric and Toga, Arthur W.},
  year           = {2004},
  journal        = {Journal of Neuroscience},
  publisher      = {Society for Neuroscience},
  volume         = {24},
  number         = {38},
  pages          = {8223--8231},
  url            = {https://www.jneurosci.org/content/24/38/8223}
}
@article{liebenwein,
  title          = {{Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}},
  author         = {Lucas Liebenwein and Cenk Baykal and Brandon Carter and David Gifford and Daniela Rus},
  year           = {2021},
  journal        = {CoRR},
  volume         = {abs/2103.03014},
  url            = {https://arxiv.org/abs/2103.03014},
  timestamp      = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2103-03014.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{liebenwein2021lost,
  title          = {{Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}},
  author         = {Liebenwein, Lucas and Baykal, Cenk and Carter, Brandon and Gifford, David and Rus, Daniela},
  year           = {2021},
  journal        = {Proceedings of Machine Learning and Systems},
  volume         = {3}
}
@misc{kuwanto2021lowresource,
  title          = {{Low-Resource Machine Translation for Low-Resource Languages: Leveraging Comparable Data, Code-Switching and Compute Resources}},
  author         = {Garry Kuwanto and Afra Feyza Akyürek and Isidora Chara Tourni and Siyang Li and Derry Wijaya},
  year           = {2021}
}
@article{lpcnet,
  title          = {{LPCNet: Improving Neural Speech Synthesis Through Linear Prediction}},
  author         = {Jean{-}Marc Valin and Jan Skoglund},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1810.11846},
  url            = {http://arxiv.org/abs/1810.11846}
}
@inproceedings{2019barham,
  title          = {{Machine Learning Systems Are Stuck in a Rut}},
  author         = {Barham, Paul and Isard, Michael},
  year           = {2019},
  booktitle      = {Proceedings of the Workshop on Hot Topics in Operating Systems},
  location       = {Bertinoro, Italy},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {HotOS ’19},
  pages          = {177–183},
  url            = {https://doi.org/10.1145/3317550.3321441},
  numpages       = {7}
}
@book{MachineLearningI,
  title          = {{Machine Learning: An Artificial Intelligence Approach, Vol. I}},
  year           = {1983},
  publisher      = {Tioga},
  address        = {Palo Alto, CA},
  editor         = {R. S. Michalski and J. G. Carbonell and T. M. Mitchell}
}
@inproceedings{NIPS2016_a486cd07,
  title          = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
  author         = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  year           = {2016},
  booktitle      = {Advances in Neural Information Processing Systems},
  publisher      = {Curran Associates, Inc.},
  volume         = {29},
  url            = {https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
  editor         = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@article{massiveWild,
  title          = {{Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges}},
  author         = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and George F. Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1907.05019},
  url            = {http://arxiv.org/abs/1907.05019},
  timestamp      = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1907-05019.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{webster2021measuring,
  title          = {{Measuring and Reducing Gendered Correlations in Pre-trained Models}},
  author         = {Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Ed Chi and Slav Petrov},
  year           = {2021}
}
@article{2017jo,
  title          = {{Measuring the tendency of CNNs to Learn Surface Statistical Regularities}},
  author         = {Jo, Jason and Bengio, Yoshua},
  year           = {2017},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1711.11561},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111561J},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{Newell81,
  title          = {{Mechanisms of Skill Acquisition and the Law of Practice}},
  author         = {A. Newell and P. S. Rosenbloom},
  year           = {1981},
  booktitle      = {Cognitive Skills and Their Acquisition},
  publisher      = {Lawrence Erlbaum Associates, Inc.},
  address        = {Hillsdale, NJ},
  pages          = {1--51},
  editor         = {J. R. Anderson},
  chapter        = {1}
}
@misc{Gallistel2009,
  title          = {{Memory and the Computational Brain: Why Cognitive Science Will Transform Neuroscience}},
  author         = {Gallistel, Charles and King, Adam},
  year           = {2009},
  month          = {04},
  pages          = {288--298}
}
@article{Collins2014memorybounded,
  title          = {{Memory Bounded Deep Convolutional Networks}},
  author         = {Collins, Maxwell~D. and Kohli, Pushmeet},
  year           = {2014},
  month          = dec,
  journal        = {ArXiv e-prints},
  url            = {http://arxiv.org/abs/1412.1442},
  adsurl         = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{zhao-etal-2017-men,
  title          = {{Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints}},
  author         = {Zhao, Jieyu  and Wang, Tianlu  and Yatskar, Mark  and Ordonez, Vicente  and Chang, Kai-Wei},
  year           = {2017},
  month          = sep,
  booktitle      = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}
}
@article{Zhao2017,
  title          = {{Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints}},
  author         = {Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai{-}Wei Chang},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1707.09457}
}
@inproceedings{merrill2016merge,
  title          = {{Merge-based parallel sparse matrix-vector multiplication}},
  author         = {Merrill, Duane and Garland, Michael},
  year           = {2016},
  booktitle      = {SC'16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages          = {678--689},
  organization   = {IEEE}
}
@article{2018Masana,
  title          = {{Metric Learning for Novelty and Anomaly Detection}},
  author         = {Masana, Marc and Ruiz, Idoia and Serrat, Joan and {van de Weijer}, Joost and Lopez, Antonio M.},
  year           = {2018},
  month          = {Aug},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1808.05492},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2018arXiv180805492M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{Reagen_7551399,
  title          = {{Minerva: Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators}},
  author         = {Reagen, B. and Whatmough, P. and Adolf, R. and Rama, S. and Lee, H. and Lee, S. K. and Hernández-Lobato, J. M. and Wei, G. and Brooks, D.},
  year           = {2016},
  month          = {June},
  booktitle      = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {267--278},
  issn           = {1063-6897}
}
@article{2017Micikevicius,
  title          = {{Mixed Precision Training}},
  author         = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year           = {2017},
  month          = oct,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1710.03740},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171003740M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{micikevicius2017mixed,
  title          = {{Mixed Precision Training}},
  author         = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  year           = {2017}
}
=
@misc{lattner2020mlir,
  title          = {{MLIR: A Compiler Infrastructure for the End of Moore's Law}},
  author         = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
  year           = {2020},
  journal        = {ArXiv},
  volume         = {abs/2002.11054}
}
@inproceedings{Reddi2020,
  title          = {{MLPerf Inference Benchmark}},
  author         = {Reddi,V. J. and Cheng, C. and Kanter, D. and Mattson, P. and Schmuelling, G. and Wu, C. and Anderson, B. and Breughe, M. and Charlebois, M. and Chou, W. and Chukka, R. and Coleman, C. and Davis, S. and Deng, P. and Diamos, G. and Duke, J. and Fick, D. and Gardner, J. S. and Hubara, I. and Idgunji, S. and Jablin, T. B. and Jiao, J. and John, T. S. and Kanwar, P. and Lee, D. and Liao, J. and Lokhmotov, A. and Massa, F. and Meng, P. and Micikevicius, P. and Osborne, C. and Pekhimenko, G. and Rajan. A. T. R. and Sequeira, D. and Sirasao, A. and Sun, F. and Tang, H. and Thomson, M. and Wei, F. and Wu, E. and Xu, L. and Yamada, K. and Yu, B. and Yuan, G. and Zhong, A. and Zhang, P. and Zhou, Y.},
  year           = {2020},
  booktitle      = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {446--459}
}
@article{2017Howard,
  title          = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
  author         = {Howard, A.~G. and Zhu, M. and Chen, B. and Kalenichenko, D. and Wang, W. and Weyand, T. and Andreetto, M. and Adam, H.},
  year           = {2017},
  month          = apr,
  journal        = {ArXiv e-prints},
  url            = {http://arxiv.org/abs/1704.04861},
  urldate        = {2020-06-05},
  note           = {arXiv: 1704.04861},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170404861H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{mitchell2019model,
  title          = {{Model cards for model reporting}},
  author         = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year           = {2019},
  booktitle      = {Proceedings of the conference on fairness, accountability, and transparency},
  pages          = {220--229}
}
@incollection{NeurIPS2019_8410,
  title          = {{Model Compression with Adversarial Robustness: A Unified Optimization Framework}},
  author         = {Gui, Shupeng and Wang, Haotao N and Yang, Haichuan and Yu, Chen and Wang, Zhangyang and Liu, Ji},
  year           = {2019},
  booktitle      = {Advances in Neural Information Processing Systems 32},
  publisher      = {Curran Associates, Inc.},
  pages          = {1285--1296},
  url            = {http://papers.NeurIPS.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework.pdf},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{zadeh_mokey_2022,
  title          = {{Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models}},
  author         = {Zadeh, Ali Hadi and Mahmoud, Mostafa and Abdelhadi, Ameer and Moshovos, Andreas},
  year           = {2022},
  month          = jun,
  booktitle      = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {ISCA '22},
  pages          = {888--901},
  url            = {https://doi.org/10.1145/3470496.3527438},
  urldate        = {2022-06-22}
}
@misc{computerhistorymuseum,
  title          = {{Moore's Law}},
  author         = {CHM},
  year           = {2020},
  url            = {https://www.computerhistory.org/revolution/digital-logic/12/267}
}
@article{Gordon_2018,
  title          = {{MorphNet: Fast \& Simple Resource-Constrained Structure Learning of Deep Networks}},
  author         = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
  year           = {2018},
  month          = {Jun},
  journal        = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher      = {IEEE},
  url            = {http://dx.doi.org/10.1109/CVPR.2018.00171}
}
@article{Eagleman2036,
  title          = {{Motion Integration and Postdiction in Visual Awareness}},
  author         = {Eagleman, David M. and Sejnowski, Terrence J.},
  year           = {2000},
  journal        = {Science},
  publisher      = {American Association for the Advancement of Science},
  volume         = {287},
  number         = {5460},
  pages          = {2036--2038},
  issn           = {0036-8075},
  url            = {https://science.sciencemag.org/content/287/5460/2036}
}
@article{sanh_movement_2020,
  title          = {{Movement Pruning: Adaptive Sparsity by Fine-Tuning}},
  author         = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M.},
  year           = {2020},
  month          = oct,
  journal        = {arXiv:2005.07683 [cs]},
  url            = {http://arxiv.org/abs/2005.07683},
  urldate        = {2021-01-22},
  note           = {arXiv: 2005.07683}
}
@misc{sanh2020movement,
  title          = {{Movement Pruning: Adaptive Sparsity by Fine-Tuning}},
  author         = {Victor Sanh and Thomas Wolf and Alexander M. Rush},
  year           = {2020}
}
@article{hornik1989multilayer,
  title          = {{Multilayer feedforward networks are universal approximators.}},
  author         = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
  year           = {1989},
  journal        = {Neural networks},
  volume         = {2},
  number         = {5},
  pages          = {359--366}
}
@misc{bremner2013,
  title          = {{Multisensory Development}},
  author         = {Bremner, Andrew and Lewkowicz, David and Spence, Charles},
  year           = {2013},
  month          = {11}
}
@article{2019Hendrycks,
  title          = {{Natural Adversarial Examples}},
  author         = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  year           = {2019},
  month          = {Jul},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1907.07174},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv190707174H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Farrand2020NeitherPN,
  title          = {{Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy}},
  author         = {Tom Farrand and FatemehSadat Mireshghallah and Sahib Singh and Andrew Trask},
  year           = {2020},
  journal        = {Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice}
}
@article{FUKUSHIMA1982455,
  title          = {{Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position}},
  author         = {Kunihiko Fukushima and Sei Miyake},
  year           = {1982},
  journal        = {Pattern Recognition},
  volume         = {15},
  number         = {6},
  pages          = {455--469},
  issn           = {0031-3203},
  url            = {http://www.sciencedirect.com/science/article/pii/0031320382900243}
}
@misc{Demuth93neuralnetwork,
  title          = {{Neural Network Toolbox For Use with Matlab - User Guide Verion 3.0}},
  author         = {Howard Demuth and Mark Beale},
  year           = {1993}
}
@article{hinton2012neural,
  title          = {{Neural networks for machine learning lecture 6a overview of mini-batch gradient descent}},
  author         = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year           = {2012},
  journal        = {Cited on},
  volume         = {14},
  number         = {8}
}
@article{shankar2017no,
  title          = {{No classification without representation: Assessing geodiversity issues in open data sets for the developing world}},
  author         = {Shankar, Shreya and Halpern, Yoni and Breck, Eric and Atwood, James and Wilson, Jimbo and Sculley, D},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1711.08536}
}
@inproceedings{Aha_1989,
  title          = {{Noise-tolerant Instance-based Learning Algorithms}},
  author         = {Aha, David W. and Kibler, Dennis},
  year           = {1989},
  booktitle      = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
  location       = {Detroit, Michigan},
  publisher      = {Morgan Kaufmann Publishers Inc.},
  address        = {San Francisco, CA, USA},
  series         = {IJCAI'89},
  pages          = {794--799},
  url            = {http://dl.acm.org/citation.cfm?id=1623755.1623881},
  numpages       = {6},
  acmid          = {1623881}
}
@inproceedings{Zhou2019NonvacuousGB,
  title          = {{Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach}},
  author         = {Wenda Zhou and Victor Veitch and Morgane Austern and Ryan P. Adams and Peter Orbanz},
  year           = {2019},
  booktitle      = {ICLR}
}
@article{LRP2016,
  title          = {{Not Just a Black Box: Learning Important Features Through Propagating Activation Differences}},
  author         = {Avanti Shrikumar and Peyton Greenside and Anna Shcherbina and Anshul Kundaje},
  year           = {2016},
  journal        = {CoRR},
  volume         = {abs/1605.01713},
  url            = {http://arxiv.org/abs/1605.01713},
  biburl         = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShrikumarGSK16},
  bibsource      = {dblp computer science bibliography, http://dblp.org}
}
@article{khan_npe_2021,
  title          = {{NPE: An FPGA-based Overlay Processor for Natural Language Processing}},
  author         = {Khan, Hamza and Khan, Asma and Khan, Zainab and Huang, Lun Bin and Wang, Kun and He, Lei},
  year           = {2021},
  month          = feb,
  journal        = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages          = {227--227},
  url            = {http://arxiv.org/abs/2104.06535},
  urldate        = {2021-07-01},
  note           = {arXiv: 2104.06535}
}
@article{Borkan2019,
  title          = {{Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification}},
  author         = {Daniel Borkan and Lucas Dixon and Jeffrey Sorensen and Nithum Thain and Lucy Vasserman},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1903.04561},
  url            = {http://arxiv.org/abs/1903.04561},
  timestamp      = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{nvidia2020,
  title          = {{NVIDIA Ampere Architecture In-Depth.}},
  author         = {Krashinsky, Ronny and Giroux, Olivier and Jones, Stephen and Stam, Nick and Ramaswamy,Sridhar},
  year           = {2020},
  url            = {https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}
}
@misc{volta,
  title          = {{NVIDIA TESLA V100 GPU ARCHITECTURE}},
  author         = {{Nvidia}},
  year           = {2017},
  url            = {https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}
}
@article{OkwuGbe,
  title          = {{OkwuGb\'e: End-to-End Speech Recognition for Fon and Igbo}},
  author         = {Bonaventure F. P. Dossou and Chris C. Emezue},
  year           = {2021},
  journal        = {AfricaNLP Workshop},
  url            = {https://arxiv.org/abs/2103.07762}
}
@article{2017Guo,
  title          = {{On Calibration of Modern Neural Networks}},
  author         = {Guo, C. and Pleiss, G. and Sun, Y. and Weinberger, K.~Q.},
  year           = {2017},
  month          = jun,
  journal        = {ArXiv e-prints},
  pages          = {arXiv:1706.04599},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170604599G},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{brunner_identifiability_2019,
  title          = {{On Identifiability in Transformers}},
  author         = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  year           = {2019},
  month          = sep,
  url            = {https://openreview.net/forum?id=BJg1f6EFDB},
  urldate        = {2021-01-23},
  language       = {en}
}
@inproceedings{raunak-etal-2020-long,
  title          = {{On Long-Tailed Phenomena in Neural Machine Translation}},
  author         = {Raunak, Vikas  and Dalmia, Siddharth  and Gupta, Vivek  and Metze, Florian},
  year           = {2020},
  month          = nov,
  booktitle      = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {3088--3095},
  url            = {https://aclanthology.org/2020.findings-emnlp.276}
}
@article{Biljon2020OnOT,
  title          = {{On Optimal Transformer Depth for Low-Resource Language Translation}},
  author         = {Elan Van Biljon and Arnu Pretorius and Julia Kreutzer},
  year           = {2020},
  journal        = {AfricaNLP Workshop}
}
@article{Bach2015,
  title          = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
  author         = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year           = {2015},
  journal        = {PloS one},
  publisher      = {Public Library of Science},
  volume         = {10},
  number         = {7},
  pages          = {e0130140},
  date-added     = {2016-10-12 11:12:26 +0000},
  date-modified  = {2016-10-12 11:16:21 +0000}
}
@misc{sehwag2020pruning,
  title          = {{On Pruning Adversarially Robust Neural Networks}},
  author         = {Vikash Sehwag and Shiqi Wang and Prateek Mittal and Suman Jana},
  year           = {2020}
}
@article{funahashi1989approximate,
  title          = {{On the approximate realization of continuous mappings by neural networks}},
  author         = {Funahashi, Ken-Ichi},
  year           = {1989},
  journal        = {Neural networks},
  publisher      = {Elsevier},
  volume         = {2},
  number         = {3},
  pages          = {183--192}
}
@article{nocedal2002behavior,
  title          = {{On the behavior of the gradient norm in the steepest descent method}},
  author         = {Nocedal, Jorge and Sartenaer, Annick and Zhu, Ciyou},
  year           = {2002},
  journal        = {Computational Optimization and Applications},
  publisher      = {Springer},
  volume         = {22},
  number         = {1},
  pages          = {5--35}
}
@inproceedings{Cummings2019,
  title          = {{On the Compatibility of Privacy and Fairness}},
  author         = {Cummings, Rachel and Gupta, Varun and Kimpara, Dhamma and Morgenstern, Jamie},
  year           = {2019},
  location       = {Larnaca, Cyprus},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {UMAP'19 Adjunct},
  pages          = {309–315},
  url            = {https://doi.org/10.1145/3314183.3323847},
  numpages       = {7}
}
@inproceedings{bender_gebru_2021,
  title          = {{On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?}},
  author         = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  year           = {2021},
  booktitle      = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  location       = {Virtual Event, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {FAccT '21},
  pages          = {610–623},
  url            = {https://doi.org/10.1145/3442188.3445922},
  numpages       = {14}
}
@inproceedings{pascanu2013difficulty,
  title          = {{On the difficulty of training recurrent neural networks}},
  author         = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year           = {2013},
  booktitle      = {International conference on machine learning},
  pages          = {1310--1318}
}
@inproceedings{ji_distribution_2021,
  title          = {{On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers}},
  author         = {Ji, Tianchu and Jain, Shraddhan and Ferdman, Michael and Milder, Peter and Schwartz, H. Andrew and Balasubramanian, Niranjan},
  year           = {2021},
  month          = aug,
  booktitle      = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {4147--4157},
  url            = {https://aclanthology.org/2021.findings-acl.363},
  urldate        = {2021-08-04}
}
@article{Alvarez_2016,
  title          = {{On the Efficient Representation and Execution of Deep Acoustic Models}},
  author         = {Alvarez, Raziel and Prabhavalkar, Rohit and Bakhtin, Anton},
  year           = {2016},
  month          = {Sep},
  journal        = {Interspeech 2016},
  publisher      = {ISCA},
  url            = {http://dx.doi.org/10.21437/Interspeech.2016-128}
}
@article{2017neurons,
  title          = {{On the frontiers of biomedicine with professor Rahul Sarpeshkar}},
  author         = {Kristin Sainani},
  year           = {2017},
  journal        = {Dartmouth Magazine},
  url            = {https://dartmouthalumnimagazine.com/articles/cell-power}
}
@inproceedings{sutskever2013importance,
  title          = {{On the importance of initialization and momentum in deep learning}},
  author         = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year           = {2013},
  booktitle      = {International conference on machine learning},
  pages          = {1139--1147}
}
@article{singledirection2018,
  title          = {{On the importance of single directions for generalization}},
  author         = {Morcos, A.~S. and Barrett, D.~G.~T. and Rabinowitz, N.~C. and Botvinick, M.},
  year           = {2018},
  month          = mar,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2018arXiv180306959M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2017hosseini,
  title          = {{On the Limitation of Convolutional Neural Networks in Recognizing Negative Images}},
  author         = {Hosseini, Hossein and Xiao, Baicen and Jaiswal, Mayoore and Poovendran, Radha},
  year           = {2017},
  month          = {Mar},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1703.06857},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv170306857H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{bommasani2021opportunities,
  title          = {{On the Opportunities and Risks of Foundation Models}},
  author         = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  year           = {2021}
}
@inproceedings{Khan2021,
  title          = {{One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision}},
  author         = {Khan, Zaid and Fu, Yun},
  year           = {2021},
  booktitle      = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  location       = {Virtual Event, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {FAccT '21},
  pages          = {587–597},
  url            = {https://doi.org/10.1145/3442188.3445920},
  numpages       = {11}
}
@article{2017Cortes,
  title          = {{Online Learning with Abstention}},
  author         = {Cortes, Corinna and DeSalvo, Giulia and Gentile, Claudio and Mohri, Mehryar and Yang, Scott},
  year           = {2017},
  month          = {Mar},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1703.03478},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303478C},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{wigger2020,
  title          = {{OpenAI launches an API to commercialize its research}},
  author         = {Kyle Wiggers},
  year           = {2020},
  url            = {https://bit.ly/31NAJQB}
}
@inproceedings{Ansel2014,
  title          = {{OpenTuner: An Extensible Framework for Program Autotuning}},
  author         = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
  year           = {2014},
  booktitle      = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
  location       = {Edmonton, AB, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {PACT '14},
  pages          = {303–316},
  url            = {https://doi.org/10.1145/2628071.2628092},
  numpages       = {14}
}
@inproceedings{Cun90optimalbrain,
  title          = {{Optimal Brain Damage}},
  author         = {Yann LeCun and John S. Denker and Sara A. Solla},
  year           = {1990},
  booktitle      = {Advances in Neural Information Processing Systems},
  publisher      = {Morgan Kaufmann},
  volume         = {2},
  pages          = {598--605},
  language       = {English (US)},
  editor         = {David Touretzky}
}
@incollection{LeCun1990,
  title          = {{Optimal Brain Damage}},
  author         = {{LeCun}, Yann and John S. Denker and Sara A. Solla},
  year           = {1990},
  booktitle      = {Advances in Neural Information Processing Systems 2},
  publisher      = {Morgan-Kaufmann},
  pages          = {598--605},
  url            = {http://papers.NeurIPS.cc/paper/250-optimal-brain-damage.pdf},
  editor         = {D. S. Touretzky}
}
@inproceedings{1993optimalbrain,
  title          = {{Optimal Brain Surgeon and general network pruning}},
  author         = {B. Hassibi and D. G. Stork and G. J. Wolff},
  year           = {1993},
  month          = {March},
  booktitle      = {IEEE International Conference on Neural Networks},
  pages          = {293--299 vol.1}
}
@book{villani2008optimal,
  title          = {{Optimal Transport: Old and New}},
  author         = {Villani, C.},
  year           = {2008},
  publisher      = {Springer Berlin Heidelberg},
  series         = {Grundlehren der mathematischen Wissenschaften},
  url            = {https://books.google.com/books?id=hV8o5R7\_5tkC},
  lccn           = {2008932183}
}
@inproceedings{araabi-monz-2020-optimizing,
  title          = {{Optimizing Transformer for Low-Resource Neural Machine Translation}},
  author         = {Araabi, Ali  and Monz, Christof},
  year           = {2020},
  month          = dec,
  booktitle      = {Proceedings of the 28th International Conference on Computational Linguistics},
  publisher      = {International Committee on Computational Linguistics},
  address        = {Barcelona, Spain (Online)},
  pages          = {3429--3435},
  url            = {https://www.aclweb.org/anthology/2020.coling-main.304}
}
@article{2014arXiv1409.1257P,
  title          = {{Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation}},
  author         = {Pouget-Abadie, Jean and Bahdanau, Dzmitry and {van Merrienboer}, Bart and Cho, Kyunghyun and Bengio, Yoshua},
  year           = {2014},
  month          = sep,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1409.1257},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2014arXiv1409.1257P},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{6527325,
  title          = {{Overview of Beyond-CMOS Devices and a Uniform Methodology for Their Benchmarking}},
  author         = {Nikonov, D. E. and Young, I. A.},
  year           = {2013},
  journal        = {Proceedings of the IEEE},
  volume         = {101},
  number         = {12},
  pages          = {2498--2533}
}
@inproceedings{TIEDEMANN12.463,
  title          = {{Parallel Data, Tools and Interfaces in OPUS}},
  author         = {Jörg Tiedemann},
  year           = {2012},
  month          = {may},
  booktitle      = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  publisher      = {European Language Resources Association (ELRA)},
  address        = {Istanbul, Turkey},
  date           = {23-25},
  editor         = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  language       = {english}
}
@book{rumelhart1987,
  title          = {{Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations}},
  year           = {1986},
  publisher      = {MIT Press},
  address        = {Cambridge, MA, USA},
  editor         = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE}
}
@misc{jeff1990,
  title          = {{Parallel Implementations of neural network training: two back-propagation approaches.}},
  author         = {Jeff Dean},
  year           = {1990},
  url            = {https://drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view}
}
@book{Hinton1989,
  title          = {{Parallel Models of Associative Memory}},
  author         = {Hinton, Geoffrey E. and Anderson, J. A.},
  year           = {1989},
  publisher      = {L. Erlbaum Associates Inc.},
  address        = {USA}
}
@article{mostafa2019parameter,
  title          = {{Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization}},
  author         = {Mostafa, Hesham and Wang, Xin},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1902.05967}
}
@inproceedings{nekoto-etal-2020-participatory,
  title          = {{Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages}},
  author         = {Nekoto, Wilhelmina  and Marivate, Vukosi  and Matsila, Tshinondiwa  and Fasubaa, Timi  and Fagbohungbe, Taiwo  and Akinola, Solomon Oluwole  and Muhammad, Shamsuddeen  and Kabongo Kabenamualu, Salomon  and Osei, Salomey  and Sackey, Freshia  and Niyongabo, Rubungo Andre  and Macharm, Ricky  and Ogayo, Perez  and Ahia, Orevaoghene  and Berhe, Musie Meressa  and Adeyemi, Mofetoluwa  and Mokgesi-Selinga, Masabata  and Okegbemi, Lawrence  and Martinus, Laura  and Tajudeen, Kolawole  and Degila, Kevin  and Ogueji, Kelechi  and Siminyu, Kathleen  and Kreutzer, Julia  and Webster, Jason  and Ali, Jamiil Toure  and Abbott, Jade  and Orife, Iroro  and Ezeani, Ignatius  and Dangana, Idris Abdulkadir  and Kamper, Herman  and Elsahar, Hady  and Duru, Goodness  and Kioko, Ghollah  and Espoir, Murhabazi  and van Biljon, Elan  and Whitenack, Daniel  and Onyefuluchi, Christopher  and Emezue, Chris Chinenye  and Dossou, Bonaventure F. P.  and Sibanda, Blessing  and Bassey, Blessing  and Olabiyi, Ayodele  and Ramkilowan, Arshath  and {\"O}ktem, Alp  and Akinfaderin, Adewale  and Bashir, Abdallah},
  year           = {2020},
  month          = nov,
  booktitle      = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {2144--2160},
  url            = {https://aclanthology.org/2020.findings-emnlp.195}
}
@misc{nekoto2020participatory,
  title          = {{Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages}},
  author         = {Nekoto, Wilhelmina  and Marivate, Vukosi  and Matsila, Tshinondiwa  and Fasubaa, Timi  and Fagbohungbe, Taiwo  and Akinola, Solomon Oluwole  and Muhammad, Shamsuddeen  and Kabongo Kabenamualu, Salomon  and Osei, Salomey  and Sackey, Freshia  and Niyongabo, Rubungo Andre  and Macharm, Ricky  and Ogayo, Perez  and Ahia, Orevaoghene  and Berhe, Musie Meressa  and Adeyemi, Mofetoluwa  and Mokgesi-Selinga, Masabata  and Okegbemi, Lawrence  and Martinus, Laura  and Tajudeen, Kolawole  and Degila, Kevin  and Ogueji, Kelechi  and Siminyu, Kathleen  and Kreutzer, Julia  and Webster, Jason  and Ali, Jamiil Toure  and Abbott, Jade  and Orife, Iroro  and Ezeani, Ignatius  and Dangana, Idris Abdulkadir  and Kamper, Herman  and Elsahar, Hady  and Duru, Goodness  and Kioko, Ghollah  and Espoir, Murhabazi  and van Biljon, Elan  and Whitenack, Daniel  and Onyefuluchi, Christopher  and Emezue, Chris Chinenye  and Dossou, Bonaventure F. P.  and Sibanda, Blessing  and Bassey, Blessing  and Olabiyi, Ayodele  and Ramkilowan, Arshath  and {\"O}ktem, Alp  and Akinfaderin, Adewale  and Bashir, Abdallah},
  year           = {2020}
}
@book{DudaHart2nd,
  title          = {{Pattern Classification}},
  author         = {R. O. Duda and P. E. Hart and D. G. Stork},
  year           = {2000},
  publisher      = {John Wiley and Sons},
  edition        = {2nd}
}
@article{Northcutt2021,
  title          = {{Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks}},
  author         = {Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  journal        = {arXiv preprint arXiv:2103.14749},
  year           = {2021}
}
@article{wang2020picking,
  title          = {{Picking winning tickets before training by preserving gradient flow}},
  author         = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  year           = {2020},
  journal        = {arXiv preprint arXiv:2002.07376}
}
@inproceedings{quinn-ballesteros-2018-pieces,
  title          = {{Pieces of Eight: 8-bit Neural Machine Translation}},
  author         = {Quinn, Jerry  and Ballesteros, Miguel},
  year           = {2018},
  month          = jun,
  booktitle      = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)},
  publisher      = {Association for Computational Linguistics},
  address        = {New Orleans - Louisiana},
  pages          = {114--120},
  url            = {https://aclanthology.org/N18-3014}
}
@misc{lazaridou2021pitfalls,
  title          = {{Pitfalls of Static Language Modelling}},
  author         = {Angeliki Lazaridou and Adhiguna Kuncoro and Elena Gribovskaya and Devang Agrawal and Adam Liska and Tayfun Terzi and Mai Gimenez and Cyprien de Masson d'Autume and Sebastian Ruder and Dani Yogatama and Kris Cao and Tomas Kocisky and Susannah Young and Phil Blunsom},
  year           = {2021}
}
@inproceedings{8192487,
  title          = {{Plasticine: A reconfigurable architecture for parallel patterns}},
  author         = {Prabhakar, R. and Zhang, Y. and Koeplinger, D. and Feldman, M. and Zhao, T. and Hadjis, S. and Pedram, A. and Kozyrakis, C. and Olukotun, K.},
  year           = {2017},
  booktitle      = {2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)},
  pages          = {389--402}
}
@article{oughton2021,
  title          = {{Policy options for digital infrastructure strategies: A simulation model for broadband universal service in Africa}},
  author         = {Edward J. Oughton},
  year           = {2021},
  journal        = {CoRR},
  volume         = {abs/2102.03561},
  url            = {https://arxiv.org/abs/2102.03561},
  timestamp      = {Wed, 10 Feb 2021 15:24:32 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2102-03561.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{sajjad_poor_2020,
  title          = {{Poor Man's BERT: Smaller and Faster Transformer Models}},
  author         = {Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  year           = {2020},
  month          = apr,
  journal        = {arXiv:2004.03844 [cs]},
  url            = {http://arxiv.org/abs/2004.03844},
  urldate        = {2021-01-22},
  note           = {arXiv: 2004.03844}
}
@article{2013denil,
  title          = {{Predicting Parameters in Deep Learning}},
  author         = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and {de Freitas}, Nando},
  year           = {2013},
  month          = {Jun},
  journal        = {arXiv e-prints},
  publisher      = {arXiv},
  pages          = {arXiv:1306.0543},
  url            = {https://arxiv.org/abs/1306.0543},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2013arXiv1306.0543D},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{bubic2010,
  title          = {{Prediction, cognition and the brain}},
  author         = {Bubic, Andreja and Von Cramon, D. Yves and Schubotz, Ricarda},
  year           = {2010},
  journal        = {Frontiers in Human Neuroscience},
  volume         = {4},
  pages          = {25},
  issn           = {1662-5161},
  url            = {https://www.frontiersin.org/article/10.3389/fnhum.2010.00025}
}
@article{2017Kearns,
  title          = {{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}},
  author         = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year           = {2017},
  month          = nov
}
@misc{lucas1991,
  title          = {{Principles of Expert Systems}},
  author         = {Lucas, Peter and van der Gaag, Linda},
  year           = {1991},
  address        = {USA}
}
@inproceedings{sankarProFormerOnDeviceLSH2021,
  title          = {{ProFormer: Towards On-Device LSH Projection Based Transformers}},
  author         = {Sankar, Chinnadhurai and Ravi, Sujith and Kozareva, Zornitsa},
  year           = {2021},
  month          = apr,
  booktitle      = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {2823--2828},
  url            = {https://aclanthology.org/2021.eacl-main.246},
  urldate        = {2022-07-04}
}
@inproceedings{8741810,
  title          = {{Progress in Neuromorphic Computing : Drawing Inspiration from Nature for Gains in AI and Computing}},
  author         = {Davies, M.},
  year           = {2019},
  booktitle      = {2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)},
  pages          = {1--1}
}
@article{2012Prototype,
  title          = {{Prototype selection for interpretable classification}},
  author         = {Bien, J. and Tibshirani, R.},
  year           = {2012},
  month          = feb,
  journal        = {ArXiv e-prints},
  publisher      = {Institute of Mathematical Statistics},
  volume         = {5},
  number         = {4},
  pages          = {2403–2424},
  issn           = {1932-6157},
  url            = {http://dx.doi.org/10.1214/11-AOAS495},
  adsurl         = {http://adsabs.harvard.edu/abs/2012arXiv1202.5933B},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{Carlini2018PrototypicalEI,
  title          = {{Prototypical Examples in Deep Learning: Metrics, Characteristics, and Utility}},
  author         = {Nicholas Carlini and Ulfar Erlingsson and Nicolas Papernot},
  year           = {2019},
  url            = {https://openreview.net/forum?id=r1xyx3R9tQ}
}
@article{gupta2018proxy,
  title          = {{Proxy fairness}},
  author         = {Gupta, Maya and Cotter, Andrew and Fard, Mahdi Milani and Wang, Serena},
  year           = {2018},
  journal        = {arXiv preprint arXiv:1806.11212}
}
@misc{paganini2020prune,
  title          = {{Prune Responsibly}},
  author         = {Michela Paganini},
  year           = {2020}
}
@article{1993pruningsurvey,
  title          = {{Pruning algorithms-a survey}},
  author         = {R. Reed},
  year           = {1993},
  month          = {Sept},
  journal        = {IEEE Transactions on Neural Networks},
  volume         = {4},
  number         = {5},
  pages          = {740--747},
  issn           = {1045-9227}
}
@article{248452,
  title          = {{Pruning algorithms-a survey}},
  author         = {Reed, R.},
  year           = {1993},
  journal        = {IEEE Transactions on Neural Networks},
  volume         = {4},
  number         = {5},
  pages          = {740--747}
}
@article{2016Molchanov,
  title          = {{Pruning Convolutional Neural Networks for Resource Efficient Inference}},
  author         = {Molchanov, P. and Tyree, S. and Karras, T. and Aila, T. and Kautz, J.},
  year           = {2016},
  month          = nov,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2016arXiv161106440M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{pruning-convnet-nvidia,
  title          = {{Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning}},
  author         = {Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
  year           = {2016},
  journal        = {CoRR},
  volume         = {abs/1611.06440}
}
@misc{li2017pruning,
  title          = {{Pruning Filters for Efficient ConvNets}},
  author         = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
  year           = {2017}
}
@misc{2020FrankleMissingMark,
  title          = {{Pruning Neural Networks at Initialization: Why are We Missing the Mark?}},
  author         = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  year           = {2020}
}
@article{tanaka2020pruning,
  title          = {{Pruning neural networks without any data by iteratively conserving synaptic flow}},
  author         = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  year           = {2020},
  journal        = {arXiv preprint arXiv:2006.05467}
}
@book{APA:83,
  title          = {{Publications Manual}},
  author         = {{American Psychological Association}},
  year           = {1983},
  publisher      = {American Psychological Association},
  address        = {Washington, DC}
}
@inproceedings{rouhani_pushing_2020,
  title          = {{Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point}},
  author         = {Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and Forin, Alessandro and Zhu, Haishan and Na, Taesik and Patel, Prerak and Che, Shuai and Koppaka, Lok Chand and Song, Xia and Som, Subhojit and Das, Kaustav and Tiwary, Saurabh and Reinhardt, Steve and Lanka, Sitaram and Chung, Eric and Burger, Doug},
  year           = {2020},
  month          = nov,
  booktitle      = {NeurIPS 2020},
  publisher      = {ACM},
  url            = {https://www.microsoft.com/en-us/research/publication/pushing-the-limits-of-narrow-precision-inferencing-at-cloud-scale-with-microsoft-floating-point/}
}
@article{shen_q-bert_2020,
  title          = {{Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT}},
  author         = {Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  year           = {2020},
  month          = apr,
  journal        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume         = {34},
  number         = {05},
  pages          = {8815--8821},
  issn           = {2374-3468},
  url            = {https://ojs.aaai.org/index.php/AAAI/article/view/6409},
  urldate        = {2021-01-25},
  note           = {Number: 05},
  language       = {en}
}
@article{zafrir_q8bert_2019,
  title          = {{Q8BERT: Quantized 8Bit BERT}},
  author         = {Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  year           = {2019},
  month          = oct,
  journal        = {arXiv:1910.06188 [cs]},
  url            = {http://arxiv.org/abs/1910.06188},
  urldate        = {2021-01-25},
  note           = {arXiv: 1910.06188}
}
@misc{Nguyen2022,
  title          = {{Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP}},
  author         = {Nguyen, Thao and Ilharco, Gabriel and Wortsman, Mitchell and Oh, Sewoong and Schmidt, Ludwig},
  year           = {2022},
  publisher      = {arXiv},
  url            = {https://arxiv.org/abs/2208.05516}
}
@article{2017arXivBenoit,
  title          = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
  author         = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  year           = {2017},
  month          = dec,
  journal        = {arXiv e-prints},
  publisher      = {IEEE},
  pages          = {arXiv:1712.05877},
  url            = {http://dx.doi.org/10.1109/CVPR.2018.00286},
  urldate        = {2020-12-17},
  note           = {arXiv: 1712.05877},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Benoit2017,
  title          = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
  author         = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew G. Howard and Hartwig Adam and Dmitry Kalenichenko},
  year           = {2017},
  journal        = {CoRR},
  volume         = {abs/1712.05877},
  url            = {http://arxiv.org/abs/1712.05877},
  timestamp      = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1712-05877.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{black-etal-2022-gpt,
  title = "{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",
  author = "Black, Sidney  and
    Biderman, Stella  and
    Hallahan, Eric  and
    Anthony, Quentin  and
    Gao, Leo  and
    Golding, Laurence  and
    He, Horace  and
    Leahy, Connor  and
    McDonell, Kyle  and
    Phang, Jason  and
    Pieler, Michael  and
    Prashanth, Usvsn Sai  and
    Purohit, Shivanshu  and
    Reynolds, Laria  and
    Tow, Jonathan  and
    Wang, Ben  and
    Weinbach, Samuel",
  booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
  month = may,
  year = "2022",
  address = "virtual+Dublin",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.bigscience-1.9",
  doi = "10.18653/v1/2022.bigscience-1.9",
  pages = "95--136",
  abstract = "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B{'}s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",
}

@misc{zhou2023lima,
    title={LIMA: Less Is More for Alignment}, 
    author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
    year={2023},
    eprint={2305.11206},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% Related Works
@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


%%start added papers
@inproceedings{
park2022active,
title={Active Learning is a Strong Baseline for Data Subset Selection},
author={Dongmin Park and Dimitris Papailiopoulos and Kangwook Lee},
booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
year={2022},
url={https://openreview.net/forum?id=PAgpyQ5rGS}
}

@misc{touvron2023llama,
    title={LLaMA: Open and Efficient Foundation Language Models}, 
    author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
    year={2023},
    eprint={2302.13971},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{schirrmeister2022more,
    title={When less is more: Simplifying inputs aids neural network understanding}, 
    author={Robin Tibor Schirrmeister and Rosanne Liu and Sara Hooker and Tonio Ball},
    year={2022},
    eprint={2201.05610},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{zhao2022dataset,
    title={Dataset Condensation with Distribution Matching}, 
    author={Bo Zhao and Hakan Bilen},
    year={2022},
    eprint={2110.04181},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hernandez2022scaling,
    title={Scaling Laws and Interpretability of Learning from Repeated Data}, 
    author={Danny Hernandez and Tom Brown and Tom Conerly and Nova DasSarma and Dawn Drain and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Tom Henighan and Tristan Hume and Scott Johnston and Ben Mann and Chris Olah and Catherine Olsson and Dario Amodei and Nicholas Joseph and Jared Kaplan and Sam McCandlish},
    year={2022},
    eprint={2205.10487},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{fayyaz2022bert,
    title={BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning}, 
    author={Mohsen Fayyaz and Ehsan Aghazadeh and Ali Modarressi and Mohammad Taher Pilehvar and Yadollah Yaghoobzadeh and Samira Ebrahimi Kahou},
    year={2022},
    eprint={2211.05610},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{coleman2020selection,
    title={Selection via Proxy: Efficient Data Selection for Deep Learning}, 
    author={Cody Coleman and Christopher Yeh and Stephen Mussmann and Baharan Mirzasoleiman and Peter Bailis and Percy Liang and Jure Leskovec and Matei Zaharia},
    year={2020},
    eprint={1906.11829},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{he2023largescale,
    title={Large-scale Dataset Pruning with Dynamic Uncertainty}, 
    author={Muyang He and Shuo Yang and Tiejun Huang and Bo Zhao},
    year={2023},
    eprint={2306.05175},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
%% end added papers

% Fine tuning
@misc{attendu2023nlu,
    title={NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks}, 
    author={Jean-Michel Attendu and Jean-Philippe Corbeil},
    year={2023},
    eprint={2306.03208},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{cao2023instruction,
    title={Instruction Mining: High-Quality Instruction Data Selection for Large Language Models}, 
    author={Yihan Cao and Yanbin Kang and Lichao Sun},
    year={2023},
    eprint={2307.06290},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  year={2023},
  eprint={2304.01373},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@misc{chen2021evaluating,
    title={Evaluating Large Language Models Trained on Code}, 
    author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
    year={2021},
    eprint={2107.03374},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lewkowycz2022solving,
    title={Solving Quantitative Reasoning Problems with Language Models}, 
    author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
    year={2022},
    eprint={2206.14858},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yang2020xlnet,
    title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
    author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
    year={2020},
    eprint={1906.08237},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{10.1162/tacl_a_00447,
  author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife, Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and Müller, Mathias and Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and de Silva, Nisansa and Çabuk Ballı, Sakine and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
  title = "{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {50-72},
  year = {2022},
  month = {01},
  abstract = "{With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00447},
  url = {https://doi.org/10.1162/tacl\_a\_00447},
  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00447/1986585/tacl\_a\_00447.pdf},
}

@inproceedings{bane-etal-2022-comparison,
  title = "A Comparison of Data Filtering Methods for Neural Machine Translation",
  author = "Bane, Fred  and
    Uguet, Celia Soler  and
    Stribi{\.z}ew, Wiktor  and
    Zaretskaya, Anna",
  booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
  month = sep,
  year = "2022",
  address = "Orlando, USA",
  publisher = "Association for Machine Translation in the Americas",
  url = "https://aclanthology.org/2022.amta-upg.22",
  pages = "313--325",
  abstract = "With the increasing availability of large-scale parallel corpora derived from web crawling and bilingual text mining, data filtering is becoming an increasingly important step in neural machine translation (NMT) pipelines. This paper applies several available tools to the task of data filtration, and compares their performance in filtering out different types of noisy data. We also study the effect of filtration with each tool on model performance in the downstream task of NMT by creating a dataset containing a combination of clean and noisy data, filtering the data with each tool, and training NMT engines using the resulting filtered corpora. We evaluate the performance of each engine with a combination of direct assessment (DA) and automated metrics. Our best results are obtained by training for a short time on all available data then filtering the corpus with cross-entropy filtering and training until convergence.",
}

@inproceedings{lee-etal-2022-deduplicating,
  title = "Deduplicating Training Data Makes Language Models Better",
  author = "Lee, Katherine  and
    Ippolito, Daphne  and
    Nystrom, Andrew  and
    Zhang, Chiyuan  and
    Eck, Douglas  and
    Callison-Burch, Chris  and
    Carlini, Nicholas",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.acl-long.577",
  doi = "10.18653/v1/2022.acl-long.577",
  pages = "8424--8445",
  abstract = "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1{%} of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets{---}for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4{%} of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
}

@misc{dodge2021documenting,
    title={Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus}, 
    author={Jesse Dodge and Maarten Sap and Ana Marasović and William Agnew and Gabriel Ilharco and Dirk Groeneveld and Margaret Mitchell and Matt Gardner},
    year={2021},
    eprint={2104.08758},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{qin2023infobatch,
    title={InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning}, 
    author={Ziheng Qin and Kai Wang and Zangwei Zheng and Jianyang Gu and Xiangyu Peng and Daquan Zhou and Yang You},
    year={2023},
    eprint={2303.04947},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@INPROCEEDINGS{5206848,
author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={ImageNet: A large-scale hierarchical image database}, 
year={2009},
volume={},
number={},
pages={248-255},
doi={10.1109/CVPR.2009.5206848}}

@article{DBLP:journals/corr/abs-2101-00027,
author       = {Leo Gao and
                Stella Biderman and
                Sid Black and
                Laurence Golding and
                Travis Hoppe and
                Charles Foster and
                Jason Phang and
                Horace He and
                Anish Thite and
                Noa Nabeshima and
                Shawn Presser and
                Connor Leahy},
title        = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
journal      = {CoRR},
volume       = {abs/2101.00027},
year         = {2021},
url          = {https://arxiv.org/abs/2101.00027},
eprinttype    = {arXiv},
eprint       = {2101.00027},
timestamp    = {Thu, 14 Oct 2021 09:16:12 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% DSIR
@misc{xie2023data,
    title={Data Selection for Language Models via Importance Resampling}, 
    author={Sang Michael Xie and Shibani Santurkar and Tengyu Ma and Percy Liang},
    year={2023},
    eprint={2302.03169},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{xie2023doremi,
title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
journal={arXiv preprint arXiv:2305.10429},
year={2023}
}

@misc{sorscher2023neural,
    title={Beyond neural scaling laws: beating power law scaling via data pruning}, 
    author={Ben Sorscher and Robert Geirhos and Shashank Shekhar and Surya Ganguli and Ari S. Morcos},
    year={2023},
    eprint={2206.14486},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{gao2021empirical,
    title={An Empirical Exploration in Quality Filtering of Text Data}, 
    author={Leo Gao},
    year={2021},
    eprint={2109.00698},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{wenzek-etal-2020-ccnet,
  title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
  author = "Wenzek, Guillaume  and
    Lachaux, Marie-Anne  and
    Conneau, Alexis  and
    Chaudhary, Vishrav  and
    Guzm{\'a}n, Francisco  and
    Joulin, Armand  and
    Grave, Edouard",
  booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
  month = may,
  year = "2020",
  address = "Marseille, France",
  publisher = "European Language Resources Association",
  url = "https://aclanthology.org/2020.lrec-1.494",
  pages = "4003--4012",
  abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
  language = "English",
  ISBN = "979-10-95546-34-4",
}

@misc{laurençon2023bigscience,
    title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset}, 
    author={Hugo Laurençon and Lucile Saulnier and Thomas Wang and Christopher Akiki et al.},
    year={2023},
    eprint={2303.03915},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{Reuel2024GenerativeAN,
title={Generative AI Needs Adaptive Governance},
author={Anka Reuel and Trond Arne Undheim},
year={2024},
url={https://api.semanticscholar.org/CorpusID:270357796}
}

@misc{brown2020language,
    title={Language Models are Few-Shot Learners}, 
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{ahia-ogueji-towards,
author       = {Orevaoghene Ahia and
                Kelechi Ogueji},
editor       = {Kathleen Siminyu and
                Laura Martinus and
                Vukosi Marivate},
title        = {Towards Supervised and Unsupervised Neural Machine Translation Baselines
                for Nigerian Pidgin},
booktitle    = {1st AfricaNLP Workshop Proceedings, AfricaNLP@ICLR 2020, Virtual Conference,
                Formerly Addis Ababa Ethiopia, 26th April 2020},
year         = {2020},
url          = {https://arxiv.org/abs/2003.12660},
timestamp    = {Wed, 06 Sep 2023 09:56:29 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2003-12660.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v139-hubara21a,
title = 	 {Accurate Post Training Quantization With Small Calibration Sets},
author =       {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
pages = 	 {4466--4475},
year = 	 {2021},
editor = 	 {Meila, Marina and Zhang, Tong},
volume = 	 {139},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {18--24 Jul},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v139/hubara21a/hubara21a.pdf},
url = 	 {https://proceedings.mlr.press/v139/hubara21a.html},
abstract = 	 {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations’ dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations’ dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy degradation — with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code <i>https://github.com/papers-submission/CalibTIP</i>.}
}


@misc{diaz2023scaling,
    title={Scaling Laws Do Not Scale}, 
    author={Fernando Diaz and Michael Madaio},
    year={2023},
    eprint={2307.03201},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{treviso2022,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}


@inproceedings{NIPS2016_6300,
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
booktitle = {Advances in Neural Information Processing Systems},
editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
volume = {29},
year = {2016}
}


@misc{boubdir2023prompts,
    title={Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation}, 
    author={Meriem Boubdir and Edward Kim and Beyza Ermis and Marzieh Fadaee and Sara Hooker},
    year={2023},
    eprint={2310.14424},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{radford2018improving,
title={Improving language understanding by generative pre-training},
author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
year={2018},
publisher={OpenAI}
}


@misc{siddiqui2022metadata,
    title={Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics}, 
    author={Shoaib Ahmed Siddiqui and Nitarshan Rajkumar and Tegan Maharaj and David Krueger and Sara Hooker},
    year={2022},
    eprint={2209.10015},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{mindermann2022prioritized,
    title={Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt}, 
    author={Sören Mindermann and Jan Brauner and Muhammed Razzak and Mrinank Sharma and Andreas Kirsch and Winnie Xu and Benedikt Höltgen and Aidan N. Gomez and Adrien Morisot and Sebastian Farquhar and Yarin Gal},
    year={2022},
    eprint={2206.07137},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{luccioni-viviano-2021-whats,
  title = "What{'}s in the Box? An Analysis of Undesirable Content in the {C}ommon {C}rawl Corpus",
  author = "Luccioni, Alexandra  and
    Viviano, Joseph",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  month = aug,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.acl-short.24",
  doi = "10.18653/v1/2021.acl-short.24",
  pages = "182--189",
  abstract = "Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.",
}

%Memorization
@misc{biderman2023emergent,
    title={Emergent and Predictable Memorization in Large Language Models}, 
    author={Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff},
    year={2023},
    eprint={2304.11158},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{toneva2019empirical,
    title={An Empirical Study of Example Forgetting during Deep Neural Network Learning}, 
    author={Mariya Toneva and Alessandro Sordoni and Remi Tachet des Combes and Adam Trischler and Yoshua Bengio and Geoffrey J. Gordon},
    year={2019},
    eprint={1812.05159},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{carlini2023quantifying,
    title={Quantifying Memorization Across Neural Language Models}, 
    author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
    year={2023},
    eprint={2202.07646},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{carlini2021extracting,
    title={Extracting Training Data from Large Language Models}, 
    author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
    year={2021},
    eprint={2012.07805},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}

@misc{kim2023odim,
    title={ODIM: an efficient method to detect outliers via inlier-memorization effect of deep generative models}, 
    author={Dongha Kim and Jaesung Hwang and Jongjin Lee and Kunwoong Kim and Yongdai Kim},
    year={2023},
    eprint={2301.04257},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{zhang2022opt,
    title={OPT: Open Pre-trained Transformer Language Models}, 
    author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
    year={2022},
    eprint={2205.01068},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{rae2022scaling,
    title={Scaling Language Models: Methods, Analysis and Insights from Training Gopher}, 
    author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
    year={2022},
    eprint={2112.11446},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@InProceedings{Agarwal_2022_CVPR,
  author    = {Agarwal, Chirag and D'souza, Daniel and Hooker, Sara},
  title     = {Estimating Example Difficulty Using Variance of Gradients},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {10368-10378}
}

@misc{anil2023palm,
    title={PaLM 2 Technical Report}, 
    author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
    year={2023},
    eprint={2305.10403},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% Examples of CV Pruning
@misc{gururangan2022language,
    title={Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection}, 
    author={Suchin Gururangan and Dallas Card and Sarah K. Dreier and Emily K. Gade and Leroy Z. Wang and Zeyu Wang and Luke Zettlemoyer and Noah A. Smith},
    year={2022},
    eprint={2201.10474},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{raju2021accelerating,
    title={Accelerating Deep Learning with Dynamic Data Pruning}, 
    author={Ravi S Raju and Kyle Daruwalla and Mikko Lipasti},
    year={2021},
    eprint={2111.12621},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{paul2023deep,
    title={Deep Learning on a Data Diet: Finding Important Examples Early in Training}, 
    author={Mansheej Paul and Surya Ganguli and Gintare Karolina Dziugaite},
    year={2023},
    eprint={2107.07075},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

% Language pruning
@misc{penedo2023refinedweb,
    title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
    author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
    year={2023},
    eprint={2306.01116},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{abbas2023semdedup,
    title={SemDeDup: Data-efficient learning at web-scale through semantic deduplication}, 
    author={Amro Abbas and Kushal Tirumala and Dániel Simig and Surya Ganguli and Ari S. Morcos},
    year={2023},
    eprint={2303.09540},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{gunasekar2023textbooks,
    title={Textbooks Are All You Need}, 
    author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},
    year={2023},
    eprint={2306.11644},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{muennighoff2023scaling,
    title={Scaling Data-Constrained Language Models}, 
    author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
    year={2023},
    eprint={2305.16264},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{mitchell2023measuring,
    title={Measuring Data}, 
    author={Margaret Mitchell and Alexandra Sasha Luccioni and Nathan Lambert and Marissa Gerchick and Angelina McMillan-Major and Ezinwanne Ozoani and Nazneen Rajani and Tristan Thrush and Yacine Jernite and Douwe Kiela},
    year={2023},
    eprint={2212.05129},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

% Deduplication / ngram 
@misc{taylor2022galactica,
    title={Galactica: A Large Language Model for Science}, 
    author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
    year={2022},
    eprint={2211.09085},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{kocetkov2022stack,
    title={The Stack: 3 TB of permissively licensed source code}, 
    author={Denis Kocetkov and Raymond Li and Loubna Ben Allal and Jia Li and Chenghao Mou and Carlos Muñoz Ferrandis and Yacine Jernite and Margaret Mitchell and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro von Werra and Harm de Vries},
    year={2022},
    eprint={2211.15533},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{sennrich2016neural,
    title={Neural Machine Translation of Rare Words with Subword Units}, 
    author={Rico Sennrich and Barry Haddow and Alexandra Birch},
    year={2016},
    eprint={1508.07909},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{vaswani2023attention,
    title={Attention Is All You Need}, 
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2023},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@software{together2023redpajama,
author = {Together Computer},
title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
month = April,
year = 2023,
url = {https://github.com/togethercomputer/RedPajama-Data}
}

@misc{vicuna2023,
  title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality},
  url = {https://lmsys.org/blog/2023-03-30-vicuna/},
  author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  month = {March},
  year = {2023}
}
@misc{koala_blogpost_2023,
author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
title = {Koala: A Dialogue Model for Academic Research},
howpublished = {Blog post},
month = {April},
year = {2023},
url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
urldate = {2023-04-03}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{Yaseen2021DataAF,
title={Data Augmentation for Low-Resource Named Entity Recognition Using Backtranslation},
author={Usama Yaseen and Stefan Langer},
journal={ArXiv},
year={2021},
volume={abs/2108.11703}
}
@article{Mishra2020AssessingDB,
title={Assessing Demographic Bias in Named Entity Recognition},
author={Shubhanshu Mishra and Sijun He and Luca Belli},
journal={ArXiv},
year={2020},
volume={abs/2008.03415}
}

@article{dhole2021nl,
title={Nl-augmenter: A framework for task-sensitive natural language augmentation},
author={Dhole, Kaustubh D and Gangal, Varun and Gehrmann, Sebastian and Gupta, Aadesh and Li, Zhenhao and Mahamood, Saad and Mahendiran, Abinaya and Mille, Simon and Srivastava, Ashish and Tan, Samson and others},
journal={arXiv preprint arXiv:2112.02721},
year={2021}
}
@misc{seqeval,
title={{seqeval}: A Python framework for sequence labeling evaluation},
url={https://github.com/chakki-works/seqeval},
note={Software available from https://github.com/chakki-works/seqeval},
author={Hiroki Nakayama},
year={2018},
}
@inproceedings{Loshchilov2019DecoupledWD,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={ICLR},
year={2019}
}


@article{Ogueji2022IntriguingPO,
title={Intriguing Properties of Compression on Multilingual Models},
author={Kelechi Ogueji and Orevaoghene Ahia and Gbemileke Onilude and Sebastian Gehrmann and Sara Hooker and Julia Kreutzer},
journal={ArXiv},
year={2022},
volume={abs/2211.02738},
url={https://api.semanticscholar.org/CorpusID:253383782}
}
@article{Tran2022PruningHA,
title={Pruning has a disparate impact on model accuracy},
author={Cuong D. Tran and Ferdinando Fioretto and Jung-Eun Kim and Rakshit Naidu},
journal={ArXiv},
year={2022},
volume={abs/2205.13574},
url={https://api.semanticscholar.org/CorpusID:249152254}
}

@article{Holste2023HowDP,
title={How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?},
author={Greg Holste and Ziyu Jiang and Ajay Jaiswal and Maria Hanna and Shlomo Minkowitz and Alan C. Legasto and Joanna G. Escalon and Sharon Steinberger and Mark E. Bittman and Thomas C. Shen and Ying Ding and Ronald M. Summers and George L. Shih and Yifan Peng and Zhangyang Wang},
journal={ArXiv},
year={2023},
url={https://api.semanticscholar.org/CorpusID:261030169}
}

@article{Iofinova2023BiasIP,
title={Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures},
author={Eugenia Iofinova and Alexandra Peste and Dan Alistarh},
journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2023},
pages={24364-24373},
url={https://api.semanticscholar.org/CorpusID:258309395}
}

@misc{northcutt2021pervasive,
    title={Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks}, 
    author={Curtis G. Northcutt and Anish Athalye and Jonas Mueller},
    year={2021},
    eprint={2103.14749},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@ARTICLE{2020arXiv200607159B,
     author = {{Beyer}, Lucas and {H{\'e}naff}, Olivier J. and {Kolesnikov}, Alexander and {Zhai}, Xiaohua and {van den Oord}, A{\"a}ron},
      title = "{Are we done with ImageNet?}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
       year = 2020,
      month = jun,
        eid = {arXiv:2006.07159},
      pages = {arXiv:2006.07159},
        doi = {10.48550/arXiv.2006.07159},
archivePrefix = {arXiv},
     eprint = {2006.07159},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200607159B},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{ZHANG2024101468,
title = {Delayed citation impact of interdisciplinary research},
journal = {Journal of Informetrics},
volume = {18},
number = {1},
pages = {101468},
year = {2024},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2023.101468},
url = {https://www.sciencedirect.com/science/article/pii/S1751157723000937},
author = {Yang Zhang and Yang Wang and Haifeng Du and Shlomo Havlin},
keywords = {Interdisciplinary research, Citation peak, Delayed citation impact, The Matthew effect, Science of science},
abstract = {Interdisciplinary research increasingly fuels innovation, and is a key input for future breakthroughs. Yet the timing of when interdisciplinary research achieves its highest citation impact remains unclear. Here, we use the time of a paper to reach its citation peak to quantify citation dynamics, and examine its relationship with paper interdisciplinarity. Using large scale publication datasets spanning over 37 years, our results suggest that interdisciplinary papers show significant delayed citation impact both at the individual paper level and collectively, as it takes longer for highly interdisciplinary papers to reach their citation peak as well as their half citations. Such relationships are nearly universal across various scientific disciplines and time periods. Furthermore, we study the underlying forces behind this delayed impact, finding that the effect goes beyond the Matthew effect (i.e., the rich-get-richer effect). Although team size and content conventionality are partly related to the citation delay, they cannot fully explain this effect. Overall, our results suggest that governments, research administrators, and funding agencies should be aware of this general feature of interdisciplinary science, which may have broad policy implications.}
}

@misc{yang2023cautious,
    title={Cautious explorers generate more future academic impact}, 
    author={Xingsheng Yang and Zhaoru Ke and Qing Ke and Haipeng Zhang and Fengnan Gao},
    year={2023},
    eprint={2306.16643},
    archivePrefix={arXiv},
    primaryClass={cs.DL}
}

@article{Park2023PapersAP,
title={Papers and patents are becoming less disruptive over time},
author={Michael Park and Erin Leahey and Russell J. Funk},
journal={Nature},
year={2023},
volume={613},
pages={138-144},
url={https://api.semanticscholar.org/CorpusID:255466666}
}

@misc{nelaturu2023fairness,
    title={On The Fairness Impacts of Hardware Selection in Machine Learning}, 
    author={Sree Harsha Nelaturu and Nishaanth Kanna Ravichandran and Cuong Tran and Sara Hooker and Ferdinando Fioretto},
    year={2023},
    eprint={2312.03886},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{gruber2023sources,
    title={Sources of Uncertainty in Machine Learning -- A Statisticians' View}, 
    author={Cornelia Gruber and Patrick Oliver Schenk and Malte Schierholz and Frauke Kreuter and Göran Kauermann},
    year={2023},
    eprint={2305.16703},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{Hllermeier2019AleatoricAE,
title={Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
author={Eyke H{\"u}llermeier and Willem Waegeman},
journal={Machine Learning},
year={2019},
volume={110},
pages={457 - 506},
url={https://api.semanticscholar.org/CorpusID:216465307}
}

@misc{hu2021does,
    title={When does loss-based prioritization fail?}, 
    author={Niel Teng Hu and Xinyu Hu and Rosanne Liu and Sara Hooker and Jason Yosinski},
    year={2021},
    eprint={2107.07741},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{thakkar2023selfinfluence,
  title={Self-Influence Guided Data Reweighting for Language Model Pre-training},
  author={Megh Thakkar and Tolga Bolukbasi and Sriram Ganapathy and Shikhar Vashishth and Sarath Chandar and Partha Talukdar},
  year={2023},
  eprint={2311.00913},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@misc{chowdhery2022palm,
    title={PaLM: Scaling Language Modeling with Pathways}, 
    author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
    year={2022},
    eprint={2204.02311},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{loshchilov2016online,
    title={Online Batch Selection for Faster Training of Neural Networks}, 
    author={Ilya Loshchilov and Frank Hutter},
    year={2016},
    eprint={1511.06343},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Gonalves2023UnderstandingTE,
title={Understanding the Effect of Model Compression on Social Bias in Large Language Models},
author={Gustavo Gonçalves and Emma Strubell},
booktitle={Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://api.semanticscholar.org/CorpusID:266163873}
}

@article{Hernandez2021ScalingLF,
title={Scaling Laws for Transfer},
author={Danny Hernandez and Jared Kaplan and T. J. Henighan and Sam McCandlish},
journal={ArXiv},
year={2021},
volume={abs/2102.01293},
url={https://api.semanticscholar.org/CorpusID:231749962}
}


@inproceedings{Dhariwal2021DataAP,
title={Data and Parameter Scaling Laws for Neural Machine Translation},
author={Prafulla Dhariwal and Girish Sastry and Mark Chen and Dan I. Moldovan and Alex and Beutel and Jonathan Deaton},
year={2021},
url={https://api.semanticscholar.org/CorpusID:235415752}
}

@article{kaplan2020,
author       = {Jared Kaplan and
                Sam McCandlish and
                Tom Henighan and
                Tom B. Brown and
                Benjamin Chess and
                Rewon Child and
                Scott Gray and
                Alec Radford and
                Jeffrey Wu and
                Dario Amodei},
title        = {Scaling Laws for Neural Language Models},
journal      = {CoRR},
volume       = {abs/2001.08361},
year         = {2020},
url          = {https://arxiv.org/abs/2001.08361},
eprinttype    = {arXiv},
eprint       = {2001.08361},
timestamp    = {Wed, 03 Jun 2020 10:55:13 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ramesh2023ACS,
title={A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models},
author={Krithika Ramesh and Arnav Chavan and Shrey Pandit and Sunayana Sitaram},
booktitle={Annual Meeting of the Association for Computational Linguistics},
year={2023},
url={https://api.semanticscholar.org/CorpusID:259370686}
}

@article{Meyer2022AFL,
title={A Fair Loss Function for Network Pruning},
author={Robbie Meyer and Alexander Wong},
journal={ArXiv},
year={2022},
volume={abs/2211.10285},
url={https://api.semanticscholar.org/CorpusID:253708377}
}

@misc{kindermans2017unreliability,
  title={The (Un)reliability of saliency methods},
  author={Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
  year={2017},
  eprint={1711.00867},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@inproceedings{NEURIPS2019_fe4b8556,
author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf},
volume = {32},
year = {2019}
}


@article{gale2019,
author       = {Trevor Gale and
                Erich Elsen and
                Sara Hooker},
title        = {The State of Sparsity in Deep Neural Networks},
journal      = {CoRR},
volume       = {abs/1902.09574},
year         = {2019},
url          = {http://arxiv.org/abs/1902.09574},
eprinttype    = {arXiv},
eprint       = {1902.09574},
timestamp    = {Tue, 21 May 2019 18:03:40 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-1902-09574.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{siddiqui2022metadata,
    title={Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics}, 
    author={Shoaib Ahmed Siddiqui and Nitarshan Rajkumar and Tegan Maharaj and David Krueger and Sara Hooker},
    year={2022},
    eprint={2209.10015},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{Agarwal_2022_CVPR,
  author    = {Agarwal, Chirag and D'souza, Daniel and Hooker, Sara},
  title     = {Estimating Example Difficulty Using Variance of Gradients},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {10368-10378}
}

@inproceedings{ogueji-etal-2022-intriguing,
  title = "Intriguing Properties of Compression on Multilingual Models",
  author = "Ogueji, Kelechi  and
    Ahia, Orevaoghene  and
    Onilude, Gbemileke  and
    Gehrmann, Sebastian  and
    Hooker, Sara  and
    Kreutzer, Julia",
  editor = "Goldberg, Yoav  and
    Kozareva, Zornitsa  and
    Zhang, Yue",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2022",
  address = "Abu Dhabi, United Arab Emirates",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.emnlp-main.619",
  doi = "10.18653/v1/2022.emnlp-main.619",
  pages = "9092--9110",
  abstract = "Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of sparsifying multilingual pre-trained language models during fine-tuning. Applying this framework to mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact the performance of low-resource languages.",
}


@article{Mohammadshahi2022WhatDC,
title={What Do Compressed Multilingual Machine Translation Models Forget?},
author={Alireza Mohammadshahi and Vassilina Nikoulina and Alexandre Berard and Caroline De Brun and James Henderson and Laurent Besacier},
journal={ArXiv},
year={2022},
volume={abs/2205.10828},
url={https://api.semanticscholar.org/CorpusID:248987296}
}

@inproceedings{Azeemi2022DatasetPF,
title={Dataset Pruning for Resource-constrained Spoofed Audio Detection},
author={Abdul Hameed Azeemi and Ihsan Ayyub Qazi and Agha Ali Raza},
booktitle={Interspeech},
year={2022},
url={https://api.semanticscholar.org/CorpusID:252353808}
}

@article{Leong2023AdaptingTT,
title={Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages},
author={Colin Leong and Herumb Shandilya and Bonaventure F. P. Dossou and Atnafu Lambebo Tonja and Joel Mathew and Abdul-Hakeem Omotayo and Oreen Yousuf and Zainab Akinjobi and Chris C. Emezue and Shamsudeen Muhammad and Steven Kolawole and Younwoo Choi and Tosin P. Adewumi},
journal={ArXiv},
year={2023},
volume={abs/2303.16985},
url={https://api.semanticscholar.org/CorpusID:257833893}
}

@article{Dery2023TransferLF,
title={Transfer Learning for Structured Pruning under Limited Task Data},
author={Lucio Dery and David Grangier and Awni Y. Hannun},
journal={ArXiv},
year={2023},
volume={abs/2311.06382},
url={https://api.semanticscholar.org/CorpusID:265150540}
}

@article{GarridoMuoz2021ASO,
title={A Survey on Bias in Deep NLP},
author={Ismael Garrido-Mu{\~n}oz and A. Montejo-R{\'a}ez and Fernando Mart{\'i}nez-Santiago and L. and Alfonso Ure{\~n}a-L{\'o}pez},
journal={Applied Sciences},
year={2021},
volume={11},
pages={3184},
url={https://api.semanticscholar.org/CorpusID:233832881}
}

@article{Jaiswal2022AttendWI,
title={Attend Who is Weak: Pruning-assisted Medical Image Localization under Sophisticated and Implicit Imbalances},
author={Ajay Jaiswal and Tianlong Chen and Justin F. Rousseau and Yifan Peng and Ying Ding and Zhangyang Wang},
journal={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
year={2022},
pages={4976-4985},
url={https://api.semanticscholar.org/CorpusID:254275463}
}

@inproceedings{Zhang2021CanSS,
title={Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?},
author={Dinghuai Zhang and Kartik Ahuja and Yilun Xu and Yisen Wang and Aaron C. Courville},
booktitle={International Conference on Machine Learning},
year={2021},
url={https://api.semanticscholar.org/CorpusID:235358638}
}

@article{Blanzeisky2021AlgorithmicFI,
title={Algorithmic Factors Influencing Bias in Machine Learning},
author={William Blanzeisky and Padraig Cunningham},
journal={ArXiv},
year={2021},
volume={abs/2104.14014},
url={https://api.semanticscholar.org/CorpusID:233443989}
}

@article{Suresh2019AFF,
title={A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle},
author={Harini Suresh and John V. Guttag},
journal={Equity and Access in Algorithms, Mechanisms, and Optimization},
year={2019},
url={https://api.semanticscholar.org/CorpusID:235436386}
}

@article{Iofinova2023BiasIP,
title={Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures},
author={Eugenia Iofinova and Alexandra Peste and Dan Alistarh},
journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2023},
pages={24364-24373},
url={https://api.semanticscholar.org/CorpusID:258309395}
}

@article{Li2023TowardsRP,
title={Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models},
author={Jianwei Li and Qi Lei and Wei Cheng and Dongkuan Xu},
journal={ArXiv},
year={2023},
volume={abs/2310.13191},
url={https://api.semanticscholar.org/CorpusID:264406220}
}

@article{Hutiri2022TinyAA,
title={Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows},
author={Wiebke Toussaint Hutiri and Aaron Yi Ding and Fahim Kawsar and Akhil Mathur},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={32},
pages={1 - 37},
url={https://api.semanticscholar.org/CorpusID:246035210}
}

@inproceedings{Stoychev2022TheEO,
title={The Effect of Model Compression on Fairness in Facial Expression Recognition},
author={Samuil Stoychev and Hatice Gunes},
booktitle={ICPR Workshops},
year={2022},
url={https://api.semanticscholar.org/CorpusID:245704555}
}

@inproceedings{Du2021RobustnessCI,
title={Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding},
author={Mengnan Du and Subhabrata Mukherjee and Yu Cheng and Milad Shokouhi and Xia Hu and Ahmed Hassan Awadallah},
booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
year={2021},
url={https://api.semanticscholar.org/CorpusID:257219883}
}

@article{Blakeney2021MeasureTC,
title={Measure Twice, Cut Once: Quantifying Bias and Fairness in Deep Neural Networks},
author={Cody Blakeney and Gentry Atkinson and Nathaniel Huish and Yan Yan and Vangelis Metsis and Ziliang Zong},
journal={ArXiv},
year={2021},
volume={abs/2110.04397},
url={https://api.semanticscholar.org/CorpusID:238582691}
}

@article{Lin2022FairGRAPEFG,
title={FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification},
author={Xiao-Ze Lin and Seungbae Kim and Jungseock Joo},
journal={ArXiv},
year={2022},
volume={abs/2207.10888},
url={https://api.semanticscholar.org/CorpusID:251018716}
}

@article{Tang2023FairST,
title={Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training},
author={Pengwei Tang and Wei Yao and Zhicong Li and Yong Liu},
journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2023},
pages={24406-24416},
url={https://api.semanticscholar.org/CorpusID:260068477}
}

@article{Chen2023ACS,
title={A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness},
author={Zongxiong Chen and Jiahui Geng and Herbert Woisetschlaeger and Sonja Schimmler and Ruben Mayer and Chunming Rong},
journal={ArXiv},
year={2023},
volume={abs/2305.03355},
url={https://api.semanticscholar.org/CorpusID:258547123}
}

@article{Neto2023CompressedMD,
title={Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition},
author={Pedro C. Neto and Eduarda Caldeira and Jaime S. Cardoso and Ana F. Sequeira},
journal={ArXiv},
year={2023},
volume={abs/2308.11840},
url={https://api.semanticscholar.org/CorpusID:261076232}
}



@article{Xu2022CanMC,
title={Can Model Compression Improve NLP Fairness},
author={Guangxuan Xu and Qingyuan Hu},
journal={ArXiv},
year={2022},
volume={abs/2201.08542},
url={https://api.semanticscholar.org/CorpusID:246210322}
}

<dt-cite key="@article{Toussaint2022BiasIA,
title={Bias in Automated Speaker Recognition},
author={Wiebke Toussaint and Aaron Yi Ding},
journal={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
year={2022},
url={https://api.semanticscholar.org/CorpusID:246240235}
}}

@article{Xu2021BeyondPA,
title={Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression},
author={Canwen Xu and Wangchunshu Zhou and Tao Ge and Kelvin J. Xu and Julian McAuley and Furu Wei},
journal={ArXiv},
year={2021},
volume={abs/2109.03228},
url={https://api.semanticscholar.org/CorpusID:237433629}
}


@article{Joseph2020ReliableMC,
title={Reliable Model Compression via Label-Preservation-Aware Loss Functions},
author={Vinu Joseph and Shoaib Ahmed Siddiqui and Aditya Bhaskara and Ganesh Gopalakrishnan and Saurav Muralidharan and Michael Garland and Sheraz Ahmed and Andreas R. Dengel},
journal={ArXiv},
year={2020},
volume={abs/2012.01604},
url={https://api.semanticscholar.org/CorpusID:227253734}
}

@article{Timpl2022UnderstandingTE,
title={Understanding the effect of sparsity on neural networks robustness},
author={Lukas Timpl and Rahim Entezari and Hanie Sedghi and Behnam Neyshabur and Olga Saukh},
journal={ArXiv},
year={2022},
volume={abs/2206.10915},
url={https://api.semanticscholar.org/CorpusID:249926776}
}

@inproceedings{ramesh-etal-2023-comparative,
  title = "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
  author = "Ramesh, Krithika  and
    Chavan, Arnav  and
    Pandit, Shrey  and
    Sitaram, Sunayana",
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.878",
  doi = "10.18653/v1/2023.acl-long.878",
  pages = "15762--15782",
  abstract = "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.",
}

@article{beyer2020,
author       = {Lucas Beyer and
                Olivier J. H{\'{e}}naff and
                Alexander Kolesnikov and
                Xiaohua Zhai and
                A{\"{a}}ron van den Oord},
title        = {Are we done with ImageNet?},
journal      = {CoRR},
volume       = {abs/2006.07159},
year         = {2020},
url          = {https://arxiv.org/abs/2006.07159},
eprinttype    = {arXiv},
eprint       = {2006.07159},
timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2006-07159.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Vasudevan2022WhenDD,
title={When does dough become a bagel? Analyzing the remaining mistakes on ImageNet},
author={Vijay Vasudevan and Benjamin Caine and Raphael Gontijo Lopes and Sara Fridovich-Keil and Rebecca Roelofs},
journal={ArXiv},
year={2022},
volume={abs/2205.04596},
url={https://api.semanticscholar.org/CorpusID:248665760}
}

@article{Iofinova2021HowWD,
title={How Well Do Sparse ImageNet Models Transfer?},
author={Eugenia Iofinova and Alexandra Peste and Mark Kurtz and Dan Alistarh},
journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2021},
pages={12256-12266},
url={https://api.semanticscholar.org/CorpusID:244709731}
}

@article{Good2022RecallDI,
title={Recall Distortion in Neural Network Pruning and the Undecayed Pruning Algorithm},
author={Aidan Good and Jia-Huei Lin and Hannah Sieg and Mikey Ferguson and Xin Yu and Shandian Zhe and Jerzy Wieczorek and Thiago Serra},
journal={ArXiv},
year={2022},
volume={abs/2206.02976},
url={https://api.semanticscholar.org/CorpusID:249431628}
}

@inproceedings{Misra2023UncoveringTH,
title={Uncovering the Hidden Cost of Model Compression},
author={Diganta Misra and Agam Goyal and Bharat Runwal and Pin-Yu Chen},
year={2023},
url={https://api.semanticscholar.org/CorpusID:261276429}
}

@inproceedings{Schwaiger2022BeyondTA,
title={Beyond Test Accuracy: The Effects of Model Compression on CNNs},
author={Adrian Schwaiger and Kristian Schwienbacher and Karsten Roscher},
booktitle={SafeAI@AAAI},
year={2022},
url={https://api.semanticscholar.org/CorpusID:247321084}
}

@article{Mohammadshahi2022WhatDC,
title={What Do Compressed Multilingual Machine Translation Models Forget?},
author={Alireza Mohammadshahi and Vassilina Nikoulina and Alexandre Berard and Caroline De Brun and James Henderson and Laurent Besacier},
journal={ArXiv},
year={2022},
volume={abs/2205.10828},
url={https://api.semanticscholar.org/CorpusID:248987296}
}

@unknown{Holste2023,
author = {Holste, Gregory and Jiang, Ziyu and Jaiswal, Ajay and Hanna, Maria and Minkowitz, Shlomo and Legasto, Alan and Escalon, Joanna and Steinberger, Sharon and Bittman, Mark and Shen, Thomas and Ding, Ying and Summers, Ronald and Shih, George and Peng, Yifan and Wang, Zhangyang},
year = {2023},
month = {08},
pages = {},
title = {How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?}
}

@misc{dam2023understanding,
    title={Understanding the Effect of the Long Tail on Neural Network Compression}, 
    author={Harvey Dam and Vinu Joseph and Aditya Bhaskara and Ganesh Gopalakrishnan and Saurav Muralidharan and Michael Garland},
    year={2023},
    eprint={2306.06238},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@InProceedings{pmlr-v216-wang23e,
title = 	 {Robust distillation for worst-class performance: on the interplay between teacher and student objectives},
author =       {Wang, Serena and Narasimhan, Harikrishna and Zhou, Yichen and Hooker, Sara and Lukasik, Michal and Menon, Aditya Krishna},
booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
pages = 	 {2237--2247},
year = 	 {2023},
editor = 	 {Evans, Robin J. and Shpitser, Ilya},
volume = 	 {216},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {31 Jul--04 Aug},
publisher =    {PMLR},
pdf = 	 {https://proceedings.mlr.press/v216/wang23e/wang23e.pdf},
url = 	 {https://proceedings.mlr.press/v216/wang23e.html},
abstract = 	 {Knowledge distillation is a popular technique that has been shown to produce remarkable gains in average accuracy. However, recent work has shown that these gains are not uniform across subgroups in the data, and can often come at the cost of accuracy on rare subgroups and classes. Robust optimization is a common remedy to improve worst-class accuracy in standard learning settings, but in distillation it is unknown whether it is best to apply robust objectives when training the teacher, the student, or both. This work studies the interplay between robust objectives for the teacher and student. Empirically, we show that that jointly modifying the teacher and student objectives can lead to better worst-class student performance and even Pareto improvement in the trade-off between worst-class and overall performance. Theoretically, we show that the <em>per-class calibration</em> of teacher scores is key when training a robust student. Both the theory and experiments support the surprising finding that applying a robust teacher training objective does not always yield a more robust student.}
}


@misc{corti2022studying,
    title={Studying the impact of magnitude pruning on contrastive learning methods}, 
    author={Francesco Corti and Rahim Entezari and Sara Hooker and Davide Bacciu and Olga Saukh},
    year={2022},
    eprint={2207.00200},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{tran2022pruning,
    title={Pruning has a disparate impact on model accuracy}, 
    author={Cuong Tran and Ferdinando Fioretto and Jung-Eun Kim and Rakshit Naidu},
    year={2022},
    eprint={2205.13574},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Du2021WhatDC,
title={What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression},
author={Mengnan Du and Subhabrata Mukherjee and Yu Cheng and Milad Shokouhi and Xia Hu and Ahmed Hassan Awadallah},
journal={ArXiv},
year={2021},
volume={abs/2110.08419},
url={https://api.semanticscholar.org/CorpusID:239016685}
}

@article{Blakeney2021SimonSE,
title={Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation},
author={Cody Blakeney and Nathaniel Huish and Yan Yan and Ziliang Zong},
journal={ArXiv},
year={2021},
volume={abs/2106.07849},
url={https://api.semanticscholar.org/CorpusID:235436182}
}

@article{Holste2023HowDP,
title={How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?},
author={Greg Holste and Ziyu Jiang and Ajay Jaiswal and Maria Hanna and Shlomo Minkowitz and Alan C. Legasto and Joanna G. Escalon and Sharon Steinberger and Mark E. Bittman and Thomas C. Shen and Ying Ding and Ronald M. Summers and George L. Shih and Yifan Peng and Zhangyang Wang},
journal={ArXiv},
year={2023},
url={https://api.semanticscholar.org/CorpusID:261030169}
}
@misc{wei2022emergent,
    title={Emergent Abilities of Large Language Models}, 
    author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
    year={2022},
    eprint={2206.07682},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{zheng-etal-2022-robust,
  title = "Robust Lottery Tickets for Pre-trained Language Models",
  author = "Zheng, Rui  and
    Rong, Bao  and
    Zhou, Yuhao  and
    Liang, Di  and
    Wang, Sirui  and
    Wu, Wei  and
    Gui, Tao  and
    Zhang, Qi  and
    Huang, Xuanjing",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.acl-long.157",
  doi = "10.18653/v1/2022.acl-long.157",
  pages = "2211--2224",
  abstract = "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.",
}
@article{Sajjad2020PoorMB,
title={Poor Man's BERT: Smaller and Faster Transformer Models},
author={Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
journal={ArXiv},
year={2020},
volume={abs/2004.03844}
}
@inproceedings{NEURIPS2020_eae15aab,
author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
pages = {20378--20389},
publisher = {Curran Associates, Inc.},
title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
url = {https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf},
volume = {33},
year = {2020}
}

@inproceedings{Iofinova2021HowWD,
title={How Well Do Sparse Imagenet Models Transfer?},
author={Eugenia Iofinova and Alexandra Peste and Mark Kurtz and Dan Alistarh},
year={2021}
}

@article{hu2020xtreme,
    author    = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
    title     = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},
    journal   = {CoRR},
    volume    = {abs/2003.11080},
    year      = {2020},
    archivePrefix = {arXiv},
    eprint    = {2003.11080}
}

@article{Du2021WhatDC,
author    = {Mengnan Du and
             Subhabrata Mukherjee and
             Yu Cheng and
             Milad Shokouhi and
             Xia Hu and
             Ahmed Hassan Awadallah},
title     = {What do Compressed Large Language Models Forget? Robustness Challenges
             in Model Compression},
journal   = {CoRR},
volume    = {abs/2110.08419},
year      = {2021},
url       = {https://arxiv.org/abs/2110.08419},
eprinttype = {arXiv},
eprint    = {2110.08419},
timestamp = {Wed, 06 Apr 2022 15:36:39 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2110-08419.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Goyal2020PoWERBERTAB,
title={PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination},
author={Saurabh Goyal and Anamitra R. Choudhury and Saurabh Raje and Venkatesan T. Chakaravarthy and Yogish Sabharwal and Ashish Verma},
booktitle={ICML},
year={2020}
}


@article{bapna2022building,
title={Building Machine Translation Systems for the Next Thousand Languages},
author={Bapna, Ankur and Caswell, Isaac and Kreutzer, Julia and Firat, Orhan and van Esch, Daan and Siddhant, Aditya and Niu, Mengmeng and Baljekar, Pallavi and Garcia, Xavier and Macherey, Wolfgang and others},
journal={arXiv preprint arXiv:2205.03983},
year={2022}
}
@article{Budhraja2021OnTP,
title={On the Prunability of Attention Heads in Multilingual BERT},
author={Aakriti Budhraja and Madhura Pande and Pratyush Kumar and Mitesh M. Khapra},
journal={ArXiv},
year={2021},
volume={abs/2109.12683}
}


% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@ARTICLE{2020arXiv201114826O,
     author = {{Obando-Ceron}, Johan S. and {Castro}, Pablo Samuel},
      title = "{Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
       year = 2020,
      month = nov,
        eid = {arXiv:2011.14826},
      pages = {arXiv:2011.14826},
archivePrefix = {arXiv},
     eprint = {2011.14826},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201114826O},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{rn50,
author    = {Kaiming He and
             Xiangyu Zhang and
             Shaoqing Ren and
             Jian Sun},
title     = {{D}eep {R}esidual {L}earning for {I}mage {R}ecognition},
booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
             {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
pages     = {770--778},
year      = {2016},
}

@article{10.1001/archopht.116.4.502,
author   = {Daw, Nigel W.},
title    = {{Critical Periods and Amblyopia}},
journal  = {Archives of Ophthalmology},
volume   = {116},
number   = {4},
pages    = {502-505},
year     = {1998},
month    = {04},
abstract = {{During the past 20 years, basic science has shown that there are different critical periods for different visual functions during the development of the visual system. Visual functions processed at higher anatomical levels within the system have a later critical period than functions processed at lower levels. This general principle suggests that treatments for amblyopia should be followed in a logical sequence, with treatment for each visual function to be started before its critical period is over. However, critical periods for some visual functions, such as stereopsis, are not yet fully determined, and the optimal treatment is, therefore, unknown. This article summarizes the current extent of our knowledge and points to the gaps that need to be filled.Arch Ophthalmol. 1998;116:502-505-->}},
issn     = {0003-9950},
doi      = {10.1001/archopht.116.4.502},
url      = {https://doi.org/10.1001/archopht.116.4.502},
eprint   = {https://jamanetwork.com/journals/jamaophthalmology/articlepdf/262202/emo7751.pdf}
}


@inproceedings{10.1145/2555243.2557966,
author    = {Olukotun, Kunle},
title     = {Beyond Parallel Programming with Domain Specific Languages},
year      = {2014},
isbn      = {9781450326568},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/2555243.2557966},
doi       = {10.1145/2555243.2557966},
abstract  = {Today, almost all computer architectures are parallel and heterogeneous; a combination of multiple CPUs, GPUs and specialized processors. This creates a challenging problem for application developers who want to develop high performance programs without the effort required to use low-level, architecture specific parallel programming models (e.g. OpenMP for CMPs, CUDA for GPUs, MPI for clusters). Domain-specific languages (DSLs) are a promising solution to this problem because they can provide an avenue for high-level application-specific abstractions with implicit parallelism to be mapped directly to low level architecture-specific programming models; providing both high programmer productivity and high execution performance.In this talk I will describe an approach to building high performance DSLs, which is based on DSL embedding in a general purpose programming language, metaprogramming and a DSL infrastructure called Delite. I will describe how we transform DSL programs into efficient first-order low-level code using domain specific optimization, parallelism and locality optimization with parallel patterns, and architecture-specific code generation. All optimizations and transformations are implemented in Delite: an extensible DSL compiler infrastucture that significantly reduces the effort required to develop new DSLs. Delite DSLs for machine learning, data querying, graph analysis, and scientific computing all achieve performance competitive with manually parallelized C++ code.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages     = {179–180},
numpages  = {2},
keywords  = {domain specific languages},
location  = {Orlando, Florida, USA},
series    = {PPoPP '14}
}

@inproceedings{10.1145/3079856.3080246,
author    = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title     = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year      = {2017},
isbn      = {9781450348928},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3079856.3080246},
doi       = {10.1145/3079856.3080246},
abstract  = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages     = {1–12},
numpages  = {12},
keywords  = {accelerator, CNN, RNN, neural network, TPU, deep learning, domain-specific architecture, LSTM, GPU, MLP, DNN, TensorFlow},
location  = {Toronto, ON, Canada},
series    = {ISCA '17}
}

@article{1050511,
author  = {R. H. {Dennard} and F. H. {Gaensslen} and H. {Yu} and V. L. {Rideout} and E. {Bassous} and A. R. {LeBlanc}},
journal = {IEEE Journal of Solid-State Circuits},
title   = {Design of ion-implanted MOSFET's with very small physical dimensions},
year    = {1974},
volume  = {9},
number  = {5},
pages   = {256-268}
}

@article{129422,
author  = {E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal = {IEEE Transactions on Neural Networks},
title   = {Application of the ANNA neural network chip to high-speed character recognition},
year    = {1992},
volume  = {3},
number  = {3},
pages   = {498-505}
}

@inproceedings{1575717,
author    = {D. {Steinkraus} and I. {Buck} and P. Y. {Simard}},
booktitle = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
title     = {Using GPUs for machine learning algorithms},
year      = {2005},
volume    = {},
number    = {},
pages     = {1115-1120 Vol. 2}
}

@article{1947welch,
author     = {Welch, B. L.},
title      = {The generalization of `{S}tudent's' problem when several
            different population variances are involved},
journal    = {Biometrika},
fjournal   = {Biometrika},
volume     = {34},
year       = {1947},
pages      = {28--35},
issn       = {0006-3444},
mrclass    = {62.0X},
mrnumber   = {0019277},
mrreviewer = {A. A. Bennett},
doi        = {10.2307/2332510},
url        = {https://doi.org/10.2307/2332510}
}


@article{1963steinbuch,
author  = {K, Steinbuch and U. Piske},
journal = {IEEE Transactions on Electronic Computers},
title   = {Learning matrices and their applications},
year    = {1963},
volume  = {EC-12},
number  = {6},
pages   = {846-862}
}


@article{1971Naturdistance_sets,
author  = {{Levandowsky}, M.},
title   = {{Distance between Sets}},
journal = {\nat},
year    = 1971,
month   = nov,
volume  = 234,
pages   = {34-35},
doi     = {10.1038/234034a0},
adsurl  = {https://ui.adsabs.harvard.edu/abs/1971Natur.234...34L},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{1985understandingcomputers,
author    = {Time},
title     = {Understanding computers: software},
publisher = {Time},
year      = {1985},
address   = {Virginia}
}



@misc{1986Rosenblatt,
author    = {Van Der Malsburg, C},
title     = {Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
year      = {1986},
publisher = {Springer Berlin Heidelberg},
pages     = {245--248}
}

@inbook{1988rumelhart,
author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title     = {Learning Representations by Back-Propagating Errors},
year      = {1988},
publisher = {MIT Press},
booktitle = {Neurocomputing: Foundations of Research},
pages     = {696–699},
numpages  = {4}
}

@article{1992_nowlan_hinton,
author   = {Nowlan, Steven J. and Hinton, Geoffrey E.},
title    = {Simplifying Neural Networks by Soft Weight-Sharing},
journal  = {Neural Computation},
volume   = {4},
number   = {4},
pages    = {473-493},
year     = {1992},
doi      = {10.1162/neco.1992.4.4.473},
url      = { 
      https://doi.org/10.1162/neco.1992.4.4.473
  
},
eprint   = { 
      https://doi.org/10.1162/neco.1992.4.4.473
  
},
abstract = { One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms. }
}




@article{1993PASJ,
author   = {{Ito}, Tomoyoshi and {Makino}, Junichiro and {Fukushige}, Toshiyuki and
       {Ebisuzaki}, Toshikazu and {Okumura}, Sachiko K. and
       {Sugimoto}, Daiichiro},
title    = {{A Special-Purpose Computer forN-Body Simulations: GRAPE-2A}},
journal  = {\pasj},
keywords = {MANY-BODY SYSTEM, SIMULATION, SPECIAL-PURPOSE COMPUTER, STELLAR DYNAMICS},
year     = 1993,
month    = jun,
volume   = {45},
pages    = {339-347},
adsurl   = {https://ui.adsabs.harvard.edu/abs/1993PASJ...45..339I},
adsnote  = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{1993pruningsurvey,
author   = {R. Reed},
journal  = {IEEE Transactions on Neural Networks},
title    = {Pruning algorithms-a survey},
year     = {1993},
volume   = {4},
number   = {5},
pages    = {740-747},
keywords = {learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods},
doi      = {10.1109/72.248452},
issn     = {1045-9227},
month    = {Sept}
}


@misc{1995thinkingmachines,
title  = {The Rise and Fall of Thinking Machines},
author = {Gary Taubes},
year   = {1995},
url    = {https://www.inc.com/magazine/19950915/2622.html}
}



@inbook{2003Gitelman,
title     = {How Users Define New Media: A History of the Amusement Phonograph},
author    = {Lisa Gitelman},
year      = {2003},
language  = {English (US)},
editor    = {Thorburn, {David } and Jenkins, {Henry }},
booktitle = {Rethinking Media Change},
publisher = {MIT Press}
}

@article{2007legg,
author        = {{Legg}, Shane and {Hutter}, Marcus},
title         = {{A Collection of Definitions of Intelligence}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Artificial Intelligence},
year          = 2007,
month         = jun,
eid           = {arXiv:0706.3639},
pages         = {arXiv:0706.3639},
archiveprefix = {arXiv},
eprint        = {0706.3639},
primaryclass  = {cs.AI},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2007arXiv0706.3639L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2012Prototype,
author        = {{Bien}, J. and {Tibshirani}, R.},
title         = {{Prototype selection for interpretable classification}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1202.5933},
primaryclass  = {stat.AP},
keywords      = {Statistics - Applications},
year          = 2012,
month         = feb,
adsurl        = {http://adsabs.harvard.edu/abs/2012arXiv1202.5933B},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2014certifying_removing_disparate_impact,
author        = {{Feldman}, Michael and {Friedler}, Sorelle and {Moeller}, John and
       {Scheidegger}, Carlos and {Venkatasubramanian}, Suresh},
title         = {{Certifying and removing disparate impact}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Computers and Society},
year          = {2014},
month         = {Dec},
eid           = {arXiv:1412.3756},
pages         = {arXiv:1412.3756},
archiveprefix = {arXiv},
eprint        = {1412.3756},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3756F},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2014Collins,
author        = {{Collins}, M.~D. and {Kohli}, P.},
title         = {{Memory Bounded Deep Convolutional Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1412.1442},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = dec,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2014Courbariaux_low_precision_multiplications,
author        = {{Courbariaux}, Matthieu and {Bengio}, Yoshua and {David}, Jean-Pierre},
title         = {{Training deep neural networks with low precision multiplications}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
year          = {2014},
month         = {Dec},
eid           = {arXiv:1412.7024},
pages         = {arXiv:1412.7024},
archiveprefix = {arXiv},
eprint        = {1412.7024},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.7024C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{2014Horowitz,
author    = {M. {Horowitz}},
booktitle = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
title     = {1.1 Computing's energy problem (and what we can do about it)},
year      = {2014},
volume    = {},
number    = {},
pages     = {10-14}
}

@article{2014Mark1,
title   = {Grace Hopper, computing pioneer},
journal = {The Harvard Gazette},
year    = {2014},
url     = {https://news.harvard.edu/gazette/story/2014/12/grace-hopper-computing-pioneer/},
author  = {Walter Isaacson}
}

@article{2014memorybounded,
author        = {{Collins}, M.~D. and {Kohli}, P.},
title         = {{Memory Bounded Deep Convolutional Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1412.1442},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = dec,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2014Nguyen,
author        = {{Nguyen}, Anh and {Yosinski}, Jason and {Clune}, Jeff},
title         = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
year          = {2014},
month         = {Dec},
eid           = {arXiv:1412.1897},
pages         = {arXiv:1412.1897},
archiveprefix = {arXiv},
eprint        = {1412.1897},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.1897N},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2014Simonyan,
author        = {{Simonyan}, K. and {Zisserman}, A.},
title         = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1409.1556},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = sep,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1409.1556S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2015Han,
author        = {{Han}, S. and {Mao}, H. and {Dally}, W.~J.},
title         = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1510.00149},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
year          = 2015,
month         = oct,
adsurl        = {http://adsabs.harvard.edu/abs/2015arXiv151000149H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}





@article{2015szegedy,
author        = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and
{Shlens}, Jonathon and {Wojna}, Zbigniew},
title         = {{Rethinking the Inception Architecture for Computer Vision}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2015,
month         = dec,
eid           = {arXiv:1512.00567},
pages         = {arXiv:1512.00567},
archiveprefix = {arXiv},
eprint        = {1512.00567},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2015Szegedy,
author        = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and
       {Shlens}, Jonathon and {Wojna}, Zbigniew},
title         = {{Rethinking the Inception Architecture for Computer Vision}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2015},
month         = {Dec},
eid           = {arXiv:1512.00567},
pages         = {arXiv:1512.00567},
archiveprefix = {arXiv},
eprint        = {1512.00567},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2016alexey,
author        = {{Kurakin}, Alexey and {Goodfellow}, Ian and {Bengio}, Samy},
title         = {{Adversarial examples in the physical world}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2016},
month         = {Jul},
eid           = {arXiv:1607.02533},
pages         = {arXiv:1607.02533},
archiveprefix = {arXiv},
eprint        = {1607.02533},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160702533K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016amodei,
author        = {{Amodei}, Dario and {Olah}, Chris and {Steinhardt}, Jacob and
       {Christiano}, Paul and {Schulman}, John and {Man{\'e}}, Dan},
title         = {{Concrete Problems in AI Safety}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
year          = {2016},
month         = {Jun},
eid           = {arXiv:1606.06565},
pages         = {arXiv:1606.06565},
archiveprefix = {arXiv},
eprint        = {1606.06565},
primaryclass  = {cs.AI},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160606565A},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2016DropNeuron,
author        = {{Pan}, W. and {Dong}, H. and {Guo}, Y.},
title         = {{DropNeuron: Simplifying the Structure of Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1606.07326},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2016,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160607326P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}





@article{2016fair_prediction,
author        = {{Chouldechova}, Alexandra},
title         = {{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Machine Learning},
year          = {2016},
month         = {Oct},
eid           = {arXiv:1610.07524},
pages         = {arXiv:1610.07524},
archiveprefix = {arXiv},
eprint        = {1610.07524},
primaryclass  = {stat.AP},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161007524C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016Lakshminarayanan,
author        = {{Lakshminarayanan}, Balaji and {Pritzel}, Alexander and
       {Blundell}, Charles},
title         = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2016},
month         = {Dec},
eid           = {arXiv:1612.01474},
pages         = {arXiv:1612.01474},
archiveprefix = {arXiv},
eprint        = {1612.01474},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161201474L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016Molchanov,
author        = {{Molchanov}, P. and {Tyree}, S. and {Karras}, T. and {Aila}, T. and 
{Kautz}, J.},
title         = {{Pruning Convolutional Neural Networks for Resource Efficient Inference}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1611.06440},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2016,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv161106440M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

}


@article{2016Szegedy,
author        = {{Szegedy}, Christian and {Ioffe}, Sergey and {Vanhoucke}, Vincent and
       {Alemi}, Alex},
title         = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2016},
month         = {Feb},
eid           = {arXiv:1602.07261},
pages         = {arXiv:1602.07261},
archiveprefix = {arXiv},
eprint        = {1602.07261},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207261S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016Zhang,
author        = {{Zhang}, Chiyuan and {Bengio}, Samy and {Hardt}, Moritz and
       {Recht}, Benjamin and {Vinyals}, Oriol},
title         = {{Understanding deep learning requires rethinking generalization}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning},
year          = {2016},
month         = {Nov},
eid           = {arXiv:1611.03530},
pages         = {arXiv:1611.03530},
archiveprefix = {arXiv},
eprint        = {1611.03530},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161103530Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2017arXiv170604599G,
author        = {{Guo}, C. and {Pleiss}, G. and {Sun}, Y. and {Weinberger}, K.~Q.
},
title         = {{On Calibration of Modern Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1706.04599},
keywords      = {Computer Science - Machine Learning},
year          = 2017,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170604599G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017arXivBenoit,
author        = {{Jacob}, Benoit and {Kligys}, Skirmantas and {Chen}, Bo and
       {Zhu}, Menglong and {Tang}, Matthew and {Howard}, Andrew and
       {Adam}, Hartwig and {Kalenichenko}, Dmitry},
title         = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2017,
month         = dec,
eid           = {arXiv:1712.05877},
pages         = {arXiv:1712.05877},
archiveprefix = {arXiv},
eprint        = {1712.05877},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017benoit,
author        = {{Jacob}, Benoit and {Kligys}, Skirmantas and {Chen}, Bo and
       {Zhu}, Menglong and {Tang}, Matthew and {Howard}, Andrew and
       {Adam}, Hartwig and {Kalenichenko}, Dmitry},
title         = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2017,
month         = dec,
eid           = {arXiv:1712.05877},
pages         = {arXiv:1712.05877},
archiveprefix = {arXiv},
eprint        = {1712.05877},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2017Cortes,
author        = {{Cortes}, Corinna and {DeSalvo}, Giulia and {Gentile}, Claudio and
       {Mohri}, Mehryar and {Yang}, Scott},
title         = {{Online Learning with Abstention}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning},
year          = {2017},
month         = {Mar},
eid           = {arXiv:1703.03478},
pages         = {arXiv:1703.03478},
archiveprefix = {arXiv},
eprint        = {1703.03478},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303478C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017Gurumoorthy,
author        = {{Gurumoorthy}, Karthik S. and {Dhurandhar}, Amit and
       {Cecchi}, Guillermo and {Aggarwal}, Charu},
title         = {{Efficient Data Representation by Selecting Prototypes with Importance Weights}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning, 65K05, 68W25},
year          = {2017},
month         = {Jul},
eid           = {arXiv:1707.01212},
pages         = {arXiv:1707.01212},
archiveprefix = {arXiv},
eprint        = {1707.01212},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170701212G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2017Hendrycks,
author        = {Dan Hendrycks and
             Kevin Gimpel},
title         = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples
             in Neural Networks},
journal       = {CoRR},
volume        = {abs/1610.02136},
year          = {2016},
url           = {http://arxiv.org/abs/1610.02136},
archiveprefix = {arXiv},
eprint        = {1610.02136},
timestamp     = {Mon, 13 Aug 2018 16:47:27 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/HendrycksG16c},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}



@article{2017hosseini,
author        = {{Hosseini}, Hossein and {Xiao}, Baicen and {Jaiswal}, Mayoore and
       {Poovendran}, Radha},
title         = {{On the Limitation of Convolutional Neural Networks in Recognizing Negative Images}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2017},
month         = {Mar},
eid           = {arXiv:1703.06857},
pages         = {arXiv:1703.06857},
archiveprefix = {arXiv},
eprint        = {1703.06857},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170306857H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2017Huang,
author        = {{Huang}, Gao and {Liu}, Shichen and {van der Maaten}, Laurens and
       {Weinberger}, Kilian Q.},
title         = {{CondenseNet: An Efficient DenseNet using Learned Group Convolutions}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.09224},
pages         = {arXiv:1711.09224},
archiveprefix = {arXiv},
eprint        = {1711.09224},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109224H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017jo,
author        = {{Jo}, Jason and {Bengio}, Yoshua},
title         = {{Measuring the tendency of CNNs to Learn Surface Statistical Regularities}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.11561},
pages         = {arXiv:1711.11561},
archiveprefix = {arXiv},
eprint        = {1711.11561},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111561J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017Kearns,
author   = {{Kearns}, Michael and {Neel}, Seth and {Roth}, Aaron and
       {Wu}, Zhiwei Steven},
title    = {{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}},
keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computer Science and Game Theory},
year     = 2017,
month    = nov
}



@article{2017l0_reg,
author        = {{Louizos}, C. and {Welling}, M. and {Kingma}, D.~P.},
title         = {{Learning Sparse Neural Networks through $L\_0$ Regularization}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1712.01312},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = 2017,
month         = dec,
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2017Lee,
author        = {{Lee}, Kimin and {Lee}, Honglak and {Lee}, Kibok and {Shin}, Jinwoo},
title         = {{Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.09325},
pages         = {arXiv:1711.09325},
archiveprefix = {arXiv},
eprint        = {1711.09325},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109325L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
} 



@article{2017Molchanov,
author        = {{Molchanov}, D. and {Ashukha}, A. and {Vetrov}, D.},
title         = {{Variational Dropout Sparsifies Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1701.05369},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = 2017,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
} 

@misc{fan2021training,
    title={Training with Quantization Noise for Extreme Model Compression}, 
    author={Angela Fan and Pierre Stock and Benjamin Graham and Edouard Grave and Remi Gribonval and Herve Jegou and Armand Joulin},
    year={2021},
    eprint={2004.07320},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@misc{hernandez2021scaling,
    title={Scaling Laws for Transfer}, 
    author={Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
    year={2021},
    eprint={2102.01293},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models}, 
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{2017neurons,
title   = {On the frontiers of biomedicine with professor Rahul Sarpeshkar},
journal = {Dartmouth Magazine},
year    = {2017},
url     = {https://dartmouthalumnimagazine.com/articles/cell-power},
author  = {Kristin Sainani}
}


@article{2017Telsa,
author  = {NHTSA},
title   = {{Technical report, U.S. Department of Transportation, National Highway Traffic, Tesla Crash Preliminary Evaluation Report
Safety Administration}},
journal = {PE 16-007},
year    = {2017},
month   = {Jan}
}


@article{2018_sparse_evolutionary_training,
author  = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
journal = {Nature Communications},
title   = {Scalable {T}raining of {A}rtificial {N}eural {N}etworks with {A}daptive {S}parse {C}onnectivity {I}nspired by {N}etwork {S}cience},
year    = {2018}
}



@misc{2018acceleratingai,
title  = {Accelerating AI: Past, Present, and Future},
year   = {2018},
url    = {https://www.youtube.com/watch?v=8n2HLp2gtYs&t=2116s},
author = {Krste Asanovic}
}



@misc{2018Amodei,
author = {Amodei,Dario and Hernandez, Danny and Sastry,Girish and Clark, Jack and Brockman, Greg and Sutskever, Ilya},
title  = {AI and Compute},
url    = {https://openai.com/blog/ai-and-compute/},
year   = {2018}
}


@article{2018Frankle,
author        = {{Frankle}, J. and {Carbin}, M.},
title         = {{The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1803.03635},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
year          = 2018,
month         = mar,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180303635F},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2018Hendrycks,
author        = {{Hendrycks}, Dan and {Dietterich}, Thomas G.},
title         = {{Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = {2018},
month         = {Jul},
eid           = {arXiv:1807.01697},
pages         = {arXiv:1807.01697},
archiveprefix = {arXiv},
eprint        = {1807.01697},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180701697H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2018Liu,
author        = {{Liu}, Y. and {Chen}, J. and {Chen}, H.},
title         = {{Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.02850},
primaryclass  = {cs.CR},
keywords      = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180102850L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}





@article{2018Mittal,
author        = {Deepak Mittal and
             Shweta Bhardwaj and
             Mitesh M. Khapra and
             Balaraman Ravindran},
title         = {Recovering from Random Pruning: On the Plasticity of Deep Convolutional
             Neural Networks},
journal       = {CoRR},
volume        = {abs/1801.10447},
year          = {2018},
url           = {http://arxiv.org/abs/1801.10447},
archiveprefix = {arXiv},
eprint        = {1801.10447},
timestamp     = {Mon, 13 Aug 2018 16:46:51 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1801-10447},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{2018Nalisnick,
author        = {{Nalisnick}, Eric and {Matsukawa}, Akihiro and {Whye Teh}, Yee and
       {Gorur}, Dilan and {Lakshminarayanan}, Balaji},
title         = {{Do Deep Generative Models Know What They Don't Know?}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2018},
month         = {Oct},
eid           = {arXiv:1810.09136},
pages         = {arXiv:1810.09136},
archiveprefix = {arXiv},
eprint        = {1810.09136},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009136N},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2018Parisi,
author        = {{Parisi}, German I. and {Kemker}, Ronald and {Part}, Jose L. and
       {Kanan}, Christopher and {Wermter}, Stefan},
title         = {{Continual Lifelong Learning with Neural Networks: A Review}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
year          = 2018,
month         = feb,
eid           = {arXiv:1802.07569},
pages         = {arXiv:1802.07569},
archiveprefix = {arXiv},
eprint        = {1802.07569},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180207569P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2018random_pruning,
author        = {{Mittal}, D. and {Bhardwaj}, S. and {Khapra}, M.~M. and {Ravindran}, B.
},
title         = {{Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.10447},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180110447M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{2018Sato,
author = {Kaz Sato},
title  = {What makes TPUs fine-tuned for deep learning?},
url    = {https://bit.ly/2ER3bIu},
year   = {2018}
}

@article{2018Theis,
author        = {{Theis}, L. and {Korshunova}, I. and {Tejani}, A. and {Husz{\'a}r}, F.
},
title         = {{Faster gaze prediction with dense networks and Fisher pruning}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.05787},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180105787T},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{2018vacuum,
author = {Computer History Archives Project},
title  = {Computer History 1949 - 1960 Early Vacuum Tube Computers Overview},
url    = {https://www.youtube.com/watch?v=WnNm_uJYWhA},
year   = {2018}
}

@article{2018Vodrahalli,
author        = {{Vodrahalli}, Kailas and {Li}, Ke and {Malik}, Jitendra},
title         = {{Are All Training Examples Created Equal? An Empirical Study}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = {2018},
month         = {Nov},
eid           = {arXiv:1811.12569},
pages         = {arXiv:1811.12569},
archiveprefix = {arXiv},
eprint        = {1811.12569},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181112569V},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2018Wang,
author        = {{Wang}, Tianyang and {Huan}, Jun and {Li}, Bo},
title         = {{Data Dropout: Optimizing Training Data for Convolutional Neural Networks}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2018},
month         = {Sep},
eid           = {arXiv:1809.00193},
pages         = {arXiv:1809.00193},
archiveprefix = {arXiv},
eprint        = {1809.00193},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180900193W},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv190110566Z,
author        = {{Zink}, Anna and {Rose}, Sherri},
title         = {{Fair Regression for Health Care Spending}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Methodology, Statistics - Machine Learning},
year          = {2019},
month         = {Jan},
eid           = {arXiv:1901.10566},
pages         = {arXiv:1901.10566},
archiveprefix = {arXiv},
eprint        = {1901.10566},
primaryclass  = {stat.AP},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190110566Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv190602243S,
author        = {{Strubell}, Emma and {Ganesh}, Ananya and {McCallum}, Andrew},
title         = {{Energy and Policy Considerations for Deep Learning in NLP}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computation and Language},
year          = 2019,
month         = jun,
eid           = {arXiv:1906.02243},
pages         = {arXiv:1906.02243},
archiveprefix = {arXiv},
eprint        = {1906.02243},
primaryclass  = {cs.CL},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602243S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2019arXiv190608158K,
author        = {{Kirsch}, Andreas and {van Amersfoort}, Joost and {Gal}, Yarin},
title         = {{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2019},
month         = {Jun},
eid           = {arXiv:1906.08158},
pages         = {arXiv:1906.08158},
archiveprefix = {arXiv},
eprint        = {1906.08158},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608158K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2019arXiv190802900M,
author        = {{Mwebaze}, Ernest and {Gebru}, Timnit and {Frome}, Andrea and
       {Nsumba}, Solomon and {Tusubira}, Jeremy},
title         = {{iCassava 2019Fine-Grained Visual Categorization Challenge}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2019},
month         = {Aug},
eid           = {arXiv:1908.02900},
pages         = {arXiv:1908.02900},
archiveprefix = {arXiv},
eprint        = {1908.02900},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190802900M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv191111134E,
author   = {{Evci}, Utku and {Gale}, Trevor and {Menick}, Jacob and
{Castro}, Pablo Samuel and {Elsen}, Erich},
title    = {{Rigging the Lottery: Making All Tickets Winners}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year     = 2019,
month    = nov
}



@inproceedings{2019barham,
author    = {Barham, Paul and Isard, Michael},
title     = {Machine Learning Systems Are Stuck in a Rut},
year      = {2019},
isbn      = {9781450367271},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3317550.3321441},
doi       = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages     = {177–183},
numpages  = {7},
location  = {Bertinoro, Italy},
series    = {HotOS ’19}
}

@misc{2019cross,
author   = {{Cross}, Andrew W. and {Bishop}, Lev S. and {Sheldon}, Sarah and
{Nation}, Paul D. and {Gambetta}, Jay M.},
title    = {{Validating quantum computers using randomized model circuits}},
keywords = {Quantum Physics},
year     = 2019,
month    = sep,
volume   = {100},
number   = {3}
}

@misc{2019EdgeTpu,
author = {Gupta, Suyog and Tan, Mingxing},
title  = {EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML},
url    = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html},
year   = {2019}
}


@misc{2019Feldman,
title  = {The era of general purpose computers is ending},
year   = {2019},
url    = {https://bit.ly/3hP8XJh},
author = {Michael Feldman}
}



@article{2019Hendrycks_Dietterich,
author        = {{Hendrycks}, Dan and {Dietterich}, Thomas},
title         = {{Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = {2019},
month         = {Mar},
eid           = {arXiv:1903.12261},
pages         = {arXiv:1903.12261},
archiveprefix = {arXiv},
eprint        = {1903.12261},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312261H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2019shooker,
author        = {{Hooker}, Sara and {Courville}, Aaron and {Clark}, Gregory and
       {Dauphin}, Yann and {Frome}, Andrea},
title         = {{What Do Compressed Deep Neural Networks Forget?}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2019,
month         = nov,
eid           = {arXiv:1911.05248},
pages         = {arXiv:1911.05248},
archiveprefix = {arXiv},
eprint        = {1911.05248},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv200610901G,
author   = {{Gale}, Trevor and {Zaharia}, Matei and {Young}, Cliff and
{Elsen}, Erich},
title    = {{Sparse GPU Kernels for Deep Learning}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
year     = 2020,
month    = jun
}

@article{2020arXiv200705558T,
author        = {{Thompson}, Neil C. and {Greenewald}, Kristjan and {Lee}, Keeheon and
{Manso}, Gabriel F.},
title         = {{The Computational Limits of Deep Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2020,
month         = jul,
eid           = {arXiv:2007.05558},
pages         = {arXiv:2007.05558},
archiveprefix = {arXiv},
eprint        = {2007.05558},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200705558T},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv200906489H,
author        = {{Hooker}, Sara},
title         = {{The Hardware Lottery}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
year          = 2020,
month         = sep,
eid           = {arXiv:2009.06489},
pages         = {arXiv:2009.06489},
archiveprefix = {arXiv},
eprint        = {2009.06489},
primaryclass  = {cs.CY},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200906489H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv201003058H,
author        = {{Hooker}, Sara and {Moorosi}, Nyalleng and {Clark}, Gregory and {Bengio}, Samy and {Denton}, Emily},
title         = {{Characterising Bias in Compressed Models}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
year          = 2020,
month         = oct,
eid           = {arXiv:2010.03058},
pages         = {arXiv:2010.03058},
archiveprefix = {arXiv},
eprint        = {2010.03058},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201003058H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2020blalock,
author        = {{Blalock}, Davis and {Gonzalez Ortiz}, Jose Javier and
       {Frankle}, Jonathan and {Guttag}, John},
title         = {{What is the State of Neural Network Pruning?}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2020,
month         = mar,
eid           = {arXiv:2003.03033},
pages         = {arXiv:2003.03033},
archiveprefix = {arXiv},
eprint        = {2003.03033},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200303033B},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2020brown,
author   = {{Brown}, Tom B. and {Mann}, Benjamin and {Ryder}, Nick and
{Subbiah}, Melanie and {Kaplan}, Jared and {Dhariwal}, Prafulla and
{Neelakantan}, Arvind and {Shyam}, Pranav and {Sastry}, Girish and
{Askell}, Amanda and {Agarwal}, Sandhini and {Herbert-Voss}, Ariel and
{Krueger}, Gretchen and {Henighan}, Tom and {Child}, Rewon and
{Ramesh}, Aditya and {Ziegler}, Daniel M. and {Wu}, Jeffrey and
{Winter}, Clemens and {Hesse}, Christopher and {Chen}, Mark and
{Sigler}, Eric and {Litwin}, Mateusz and {Gray}, Scott and
{Chess}, Benjamin and {Clark}, Jack and {Berner}, Christopher and {McCand
lish}, Sam and {Radford}, Alec and {Sutskever}, Ilya and {Amodei}, Dario},
title    = {{Language Models are Few-Shot Learners}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Computation and Language},
year     = 2020,
month    = may
}

@misc{2020cortexm,
author = {ARM},
title  = {Enhancing AI Performance for IoT Endpoint Devices},
url    = {https://www.arm.com/company/news/2020/02/new-ai-technology-from-arm},
year   = {2020}
}

@misc{2020matrix,
title    = {Understanding Matrix Capsules with EM
Routing.},
url      = {https://jhui.github.io/2017/11/14/
Matrix-Capsules-with-EM-routing-Capsule-Network},
accessed = {2020-04-20}
}



@article{2020Mirhoseini,
author        = {{Mirhoseini}, Azalia and {Goldie}, Anna and {Yazgan}, Mustafa and
       {Jiang}, Joe and {Songhori}, Ebrahim and {Wang}, Shen and
       {Lee}, Young-Joon and {Johnson}, Eric and {Pathak}, Omkar and
       {Bae}, Sungmin and {Nazi}, Azade and {Pak}, Jiwoo and {Tong}, Andy and
       {Srinivasa}, Kavya and {Hang}, William and {Tuncer}, Emre and
       {Babu}, Anand and {Le}, Quoc V. and {Laudon}, James and {Ho}, Richard and
       {Carpenter}, Roger and {Dean}, Jeff},
title         = {{Chip Placement with Deep Reinforcement Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
year          = 2020,
month         = apr,
eid           = {arXiv:2004.10746},
pages         = {arXiv:2004.10746},
archiveprefix = {arXiv},
eprint        = {2004.10746},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200410746M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2020sun,
author        = {{Sun}, Fei and {Qin}, Minghai and {Zhang}, Tianyun and {Liu}, Liu and
{Chen}, Yen-Kuang and {Xie}, Yuan},
title         = {{Computation on Sparse Neural Networks: an Inspiration for Future Hardware}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = 2020,
month         = apr,
eid           = {arXiv:2004.11946},
pages         = {arXiv:2004.11946},
archiveprefix = {arXiv},
eprint        = {2004.11946},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200411946S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020Thompson,
author        = {{Thompson}, Neil C. and {Greenewald}, Kristjan and {Lee}, Keeheon and
{Manso}, Gabriel F.},
title         = {{The Computational Limits of Deep Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2020,
month         = jul,
eid           = {arXiv:2007.05558},
pages         = {arXiv:2007.05558},
archiveprefix = {arXiv},
eprint        = {2007.05558},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200705558T},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{6527325,
author  = {D. E. {Nikonov} and I. A. {Young}},
journal = {Proceedings of the IEEE},
title   = {Overview of Beyond-CMOS Devices and a Uniform Methodology for Their Benchmarking},
year    = {2013},
volume  = {101},
number  = {12},
pages   = {2498-2533}
}


@inproceedings{6909517,
author    = {X. {Zhu} and D. {Anguelov} and D. {Ramanan}},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
title     = {Capturing Long-Tail Distributions of Object Subcategories},
year      = {2014},
volume    = {},
number    = {},
pages     = {915-922}
}

@inproceedings{7298594,
author    = {C. {Szegedy} and  {Wei Liu} and  {Yangqing Jia} and P. {Sermanet} and S. {Reed} and D. {Anguelov} and D. {Erhan} and V. {Vanhoucke} and A. {Rabinovich}},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title     = {Going deeper with convolutions},
year      = {2015},
volume    = {},
number    = {},
pages     = {1-9}
}

@inproceedings{7459430,
author    = {G. {Fursin} and A. {Lokhmotov} and E. {Plowman}},
booktitle = {2016 Design, Automation   Test in Europe Conference   Exhibition (DATE)},
title     = {Collective Knowledge: Towards R D sustainability},
year      = {2016},
volume    = {},
number    = {},
pages     = {864-869}
}




@article{7866802,
author  = {B. {Falsafi} and B. {Dally} and D. {Singh} and D. {Chiou} and J. J. {Yi} and R. {Sendag}},
journal = {IEEE Micro},
title   = {FPGAs versus GPUs in Data centers},
year    = {2017},
volume  = {37},
number  = {1},
pages   = {60-72}
}

@inproceedings{8192487,
author    = {R. {Prabhakar} and Y. {Zhang} and D. {Koeplinger} and M. {Feldman} and T. {Zhao} and S. {Hadjis} and A. {Pedram} and C. {Kozyrakis} and K. {Olukotun}},
booktitle = {2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)},
title     = {Plasticine: A reconfigurable architecture for parallel patterns},
year      = {2017},
volume    = {},
number    = {},
pages     = {389-402}
}

@article{8364435,
author   = {N. D. {Lane} and P. {Warden}},
journal  = {Computer},
title    = {The Deep (Learning) Transformation of Mobile and Embedded Computing},
year     = {2018},
volume   = {51},
number   = {5},
pages    = {12-16},
keywords = {embedded deep learning;machine learning;machine learning systems;deep model compression;mobile deep neural networks;embedded systems;mobile;mobile computing;pervasive computing;intelligent systems},
doi      = {10.1109/MC.2018.2381129},
issn     = {1558-0814},
month    = {May}
}


@article{8476161,
author  = {J. {Dongarra} and M. {Gates} and J. {Kurzak} and P. {Luszczek} and Y. M. {Tsai}},
journal = {Proceedings of the IEEE},
title   = {Autotuning Numerical Dense Linear Algebra for Batched Computation With GPU Hardware Accelerators},
year    = {2018},
volume  = {106},
number  = {11},
pages   = {2040-2055}
}


@inproceedings{8741810,
author    = {M. {Davies}},
booktitle = {2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)},
title     = {Progress in Neuromorphic Computing : Drawing Inspiration from Nature for Gains in AI and Computing},
year      = {2019},
volume    = {},
number    = {},
pages     = {1-1}
}

@article{adam-optimizer,
author  = {Diederik P. Kingma and
             Jimmy Ba},
title   = {Adam: {A} {M}ethod for {S}tochastic {O}ptimization},
journal = {CoRR},
volume  = {abs/1412.6980},
year    = {2014},
url     = {http://arxiv.org/abs/1412.6980}
}



@inproceedings{Aha_1989,
author    = {Aha, David W. and Kibler, Dennis},
title     = {Noise-tolerant Instance-based Learning Algorithms},
booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
series    = {IJCAI'89},
year      = {1989},
location  = {Detroit, Michigan},
pages     = {794--799},
numpages  = {6},
url       = {http://dl.acm.org/citation.cfm?id=1623755.1623881},
acmid     = {1623881},
publisher = {Morgan Kaufmann Publishers Inc.},
address   = {San Francisco, CA, USA}
}




@article{Aha1991,
author  = {Aha, David W.
and Kibler, Dennis
and Albert, Marc K.},
title   = {Instance-based learning algorithms},
journal = {Machine Learning},
year    = {1991},
month   = {Jan},
day     = {01},
volume  = {6},
number  = {1},
pages   = {37--66},
url     = {https://doi.org/10.1007/BF00153759}
}


@article{Aha1992ToleratingNI,
title   = {Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms},
author  = {David W. Aha},
journal = {International Journal of Man-Machine Studies},
year    = {1992},
volume  = {36},
pages   = {267-287}
}

@article{ambrogio2018,
author  = {Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert and Boybat, Irem and Nolfo, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan and Killeen, Benjamin and Cheng, Christina and Jaoudi, Yassine and Burr, Geoffrey},
year    = {2018},
month   = {06},
pages   = {},
title   = {Equivalent-accuracy accelerated neural-network training using analogue memory},
volume  = {558},
journal = {Nature},
doi     = {10.1038/s41586-018-0180-5}
}

@article{amodei2016,
author        = {Dario Amodei and
             Chris Olah and
             Jacob Steinhardt and
             Paul F. Christiano and
             John Schulman and
             Dan Man{\'{e}}},
title         = {Concrete Problems in {AI} Safety},
journal       = {CoRR},
volume        = {abs/1606.06565},
year          = {2016},
url           = {http://arxiv.org/abs/1606.06565},
archiveprefix = {arXiv},
eprint        = {1606.06565},
timestamp     = {Mon, 13 Aug 2018 16:48:59 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/AmodeiOSCSM16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}




@misc{anonymous,
title  = {Suppressed for Anonymity},
author = {Author, N. N.},
year   = {2018}
}


@inproceedings{Ansel2014,
author    = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
title     = {OpenTuner: An Extensible Framework for Program Autotuning},
year      = {2014},
isbn      = {9781450328098},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/2628071.2628092},
doi       = {10.1145/2628071.2628092},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages     = {303–316},
numpages  = {14},
keywords  = {optimization, autotuner},
location  = {Edmonton, AB, Canada},
series    = {PACT '14}
}

@article{article,
author  = {Konishi, Masakazu},
year    = {1985},
month   = {02},
pages   = {125-70},
title   = {Birdsong: From Behavior to Neuron},
volume  = {8},
journal = {Annual review of neuroscience},
doi     = {10.1146/annurev.ne.08.030185.001013}
}



@techreport{Asanovic2006,
author      = {Asanović, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A.},
title       = {The Landscape of Parallel Computing Research: A View from Berkeley},
institution = {EECS Department, University of California, Berkeley},
year        = {2006},
month       = {Dec},
url         = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
number      = {UCB/EECS-2006-183}
}

@article{autoencoding-variational-bayes,
author  = {Diederik P. Kingma and
             Max Welling},
title   = {Auto-Encoding Variational Bayes},
journal = {CoRR},
volume  = {abs/1312.6114},
year    = {2013}
} 


@inproceedings{automatic-model-compression,
author    = {Yihui He and
             Ji Lin and
             Zhijian Liu and
             Hanrui Wang and
             Li{-}Jia Li and
             Song Han},
title     = {{AMC:} AutoML for Model Compression and Acceleration on Mobile Devices},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VII}},
pages     = {815--832},
year      = {2018}
}



@article{ba2016layer,
title   = {Layer normalization},
author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
journal = {arXiv preprint arXiv:1607.06450},
year    = {2016}
}

@article{Bach2015,
author        = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
date-added    = {2016-10-12 11:12:26 +0000},
date-modified = {2016-10-12 11:16:21 +0000},
journal       = {PloS one},
number        = {7},
pages         = {e0130140},
publisher     = {Public Library of Science},
title         = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
volume        = {10},
year          = 2015
}




@article{balduzzi2017shattered,
title   = {The shattered gradients problem: If resnets are the answer, then what is the question?},
author  = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
journal = {arXiv preprint arXiv:1702.08591},
year    = {2017}
}

@inproceedings{Barham2019,
author    = {Barham, Paul and Isard, Michael},
title     = {Machine Learning Systems Are Stuck in a Rut},
year      = {2019},
isbn      = {9781450367271},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3317550.3321441},
doi       = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages     = {177–183},
numpages  = {7},
location  = {Bertinoro, Italy},
series    = {HotOS ’19}
}


@article{Barnett2002WhenAW,
title   = {When and where do we apply what we learn? A taxonomy for far transfer.},
author  = {Susan M. Barnett and S. Ceci},
journal = {Psychological bulletin},
year    = {2002},
volume  = {128 4},
pages   = {
        612-37
      }
}

@inproceedings{bayesian-compression,
author    = {Christos Louizos and
             Karen Ullrich and
             Max Welling},
title     = {Bayesian {C}ompression for {D}eep {L}earning},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {3290--3300},
year      = {2017}
}



@article{bellec2017deep,
title   = {Deep rewiring: Training very sparse deep networks},
author  = {Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
journal = {arXiv preprint arXiv:1711.05136},
year    = {2017}
}

@incollection{Bengio+chapter2007,
author    = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title     = {Scaling Learning Algorithms Towards {AI}},
year      = {2007}
}

@article{bengio1994learning,
title     = {Learning long-term dependencies with gradient descent is difficult},
author    = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
journal   = {IEEE transactions on neural networks},
volume    = {5},
number    = {2},
pages     = {157--166},
year      = {1994},
publisher = {IEEE}
}

@inproceedings{Bengio2009,
author    = {Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title     = {Curriculum Learning},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
series    = {ICML '09},
year      = {2009},
isbn      = {978-1-60558-516-1},
location  = {Montreal, Quebec, Canada},
pages     = {41--48},
numpages  = {8},
url       = {http://doi.acm.org/10.1145/1553374.1553380},
doi       = {10.1145/1553374.1553380},
acmid     = {1553380},
publisher = {ACM},
address   = {New York, NY, USA}
}


@article{benna2016,
author  = {Benna, Marcus and Fusi, Stefano},
year    = {2016},
month   = {10},
pages   = {},
title   = {Computational principles of synaptic memory consolidation},
volume  = {19},
journal = {Nature Neuroscience},
doi     = {10.1038/nn.4401}
}


@article{Bi10464,
author    = {Bi, Guo-qiang and Poo, Mu-ming},
title     = {Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type},
volume    = {18},
number    = {24},
pages     = {10464--10472},
year      = {1998},
doi       = {10.1523/JNEUROSCI.18-24-10464.1998},
publisher = {Society for Neuroscience},
abstract  = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb{\textquoteright}s rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
issn      = {0270-6474},
url       = {https://www.jneurosci.org/content/18/24/10464},
eprint    = {https://www.jneurosci.org/content/18/24/10464.full.pdf},
journal   = {Journal of Neuroscience}
}

@article{Bien_2011,
title     = {Prototype selection for interpretable classification},
volume    = {5},
issn      = {1932-6157},
url       = {http://dx.doi.org/10.1214/11-AOAS495},
doi       = {10.1214/11-aoas495},
number    = {4},
journal   = {The Annals of Applied Statistics},
publisher = {Institute of Mathematical Statistics},
author    = {Bien, Jacob and Tibshirani, Robert},
year      = {2011},
month     = {Dec},
pages     = {2403–2424}
}


@ARTICLE{2014arXiv1409.1257P,
     author = {{Pouget-Abadie}, Jean and {Bahdanau}, Dzmitry and {van Merrienboer}, Bart and {Cho}, Kyunghyun and {Bengio}, Yoshua},
      title = "{Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
       year = 2014,
      month = sep,
        eid = {arXiv:1409.1257},
      pages = {arXiv:1409.1257},
archivePrefix = {arXiv},
     eprint = {1409.1257},
primaryClass = {cs.CL},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1409.1257P},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Birodkar2019SemanticRI,
title   = {Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need},
author  = {Vighnesh Birodkar and Hossein Mobahi and Samy Bengio},
journal = {ArXiv},
year    = {2019},
volume  = {abs/1901.11409}
}

@misc{blocksparse-gpu-kernels,
title        = {Block-Sparse GPU Kernels},
author       = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year         = {2017}
}

misc{blocksparse-gpu-kernels,
title        = {Block-Sparse GPU Kernels},
author       = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year         = {2017}
}

@misc{bremner2013,
author = {Bremner, Andrew and Lewkowicz, David and Spence, Charles},
year   = {2013},
month  = {11},
pages  = {},
title  = {Multisensory Development},
doi    = {10.1093/acprof:oso/9780199586059.003.0001}
}


@book{bruce1991,
author    = {Collier, Bruce},
title     = {Little Engines That Could’ve: The Calculating Machines of Charles Babbage},
year      = {1991},
isbn      = {0824000439},
publisher = {Garland Publishing, Inc.},
address   = {USA}
} 


@article{bubic2010,
author  = {Bubic, Andreja and Von Cramon, D. Yves and Schubotz, Ricarda},
title   = {Prediction, cognition and the brain},
journal = {Frontiers in Human Neuroscience},
volume  = {4},
pages   = {25},
year    = {2010},
url     = {https://www.frontiersin.org/article/10.3389/fnhum.2010.00025},
doi     = {10.3389/fnhum.2010.00025},
issn    = {1662-5161}
}

@misc{Cade2018,
title  = {Big Bets on A.I. Open a New Frontier for Chip Start-Ups, Too},
author = {Cade Metz},
year   = {2018},
url    = {https://www.nytimes.com/2018/01/14/technology/artificial-intelligence-chip-start-ups.html}
}

@inproceedings{CameronJones1995InstanceSB,
title  = {Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing},
author = {R. Mike Cameron-Jones},
year   = {1995}
}


@article{CASEY2000241,
title   = {Structural and functional brain development and its relation to cognitive development},
journal = {Biological Psychology},
volume  = {54},
number  = {1},
pages   = {241 - 257},
year    = {2000},
issn    = {0301-0511},
doi     = {https://doi.org/10.1016/S0301-0511(00)00058-2},
url     = {http://www.sciencedirect.com/science/article/pii/S0301051100000582},
author  = {B.J. Casey and Jay N. Giedd and Kathleen M. Thomas}
}


@article{Cha2007,
author  = {Cha, Sung-Hyuk},
year    = {2007},
month   = {01},
pages   = {},
title   = {Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions},
volume  = {1},
journal = {Int. J. Math. Model. Meth. Appl. Sci.}
}

@inproceedings{chen2018gradnorm,
title        = {Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
author       = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
booktitle    = {International Conference on Machine Learning},
pages        = {794--803},
year         = {2018},
organization = {PMLR}
}

@inproceedings{Chierichetti2010,
author  = {Chierichetti, Flavio and Kumar, Ravi and Pandey, Sandeep and Vassilvitskii, Sergei},
year    = {2010},
month   = {01},
pages   = {293-311},
title   = {Finding the Jaccard Median},
journal = {Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms},
doi     = {10.1137/1.9781611973075.25}
}

@article{chouldechova2017fair,
title     = {Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
author    = {Chouldechova, Alexandra},
journal   = {Big data},
volume    = {5},
number    = {2},
pages     = {153--163},
year      = {2017},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@article{ciresan2011,
author  = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year    = {2011},
month   = {07},
pages   = {1237-1242},
title   = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
doi     = {10.5591/978-1-57735-516-8/IJCAI11-210}
}

@article{Ciresan2011,
author        = {Dan C. Ciresan and
             Ueli Meier and
             Jonathan Masci and
             Luca Maria Gambardella and
             J{\"{u}}rgen Schmidhuber},
title         = {High-Performance Neural Networks for Visual Object Classification},
journal       = {CoRR},
volume        = {abs/1102.0183},
year          = {2011},
url           = {http://arxiv.org/abs/1102.0183},
archiveprefix = {arXiv},
eprint        = {1102.0183},
timestamp     = {Mon, 13 Aug 2018 16:47:27 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1102-0183},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{claudiu2010,
author        = {{Claudiu Ciresan}, Dan and {Meier}, Ueli and {Gambardella}, Luca Maria and
{Schmidhuber}, Juergen},
title         = {{Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
year          = 2010,
month         = mar,
eid           = {arXiv:1003.0358},
pages         = {arXiv:1003.0358},
archiveprefix = {arXiv},
eprint        = {1003.0358},
primaryclass  = {cs.NE},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2010arXiv1003.0358C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{clevert2015fast,
title   = {Fast and accurate deep network learning by exponential linear units (elus)},
author  = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
journal = {arXiv preprint arXiv:1511.07289},
year    = {2015}
}

@article{CLINTWHALEY20013,
title    = {Automated empirical optimizations of software and the ATLAS project},
journal  = {Parallel Computing},
volume   = {27},
number   = {1},
pages    = {3 - 35},
year     = {2001},
note     = {New Trends in High Performance Computing},
issn     = {0167-8191},
doi      = {https://doi.org/10.1016/S0167-8191(00)00087-9},
url      = {http://www.sciencedirect.com/science/article/pii/S0167819100000879},
author   = {R. {Clint Whaley} and Antoine Petitet and Jack J. Dongarra},
keywords = {ATLAS, BLAS, Portable performance, AEOS}
}

@article{CollinsK14,
author        = {Maxwell D. Collins and
             Pushmeet Kohli},
title         = {Memory Bounded Deep Convolutional Networks},
journal       = {CoRR},
volume        = {abs/1412.1442},
year          = {2014},
url           = {http://arxiv.org/abs/1412.1442},
archiveprefix = {arXiv},
eprint        = {1412.1442},
timestamp     = {Mon, 13 Aug 2018 16:47:16 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/CollinsK14},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Colwell2013,
author    = {R. {Colwell}},
booktitle = {2013 IEEE Hot Chips 25 Symposium (HCS)},
title     = {The chip design game at the end of Moore's law},
year      = {2013},
volume    = {},
number    = {},
pages     = {1-16}
}

@misc{computerhistorymuseum,
title  = {Moore's Law},
author = {CHM},
year   = {2020},
url    = {https://www.computerhistory.org/revolution/digital-logic/12/267}
}

@article{Cong2011,
author  = {J. {Cong} and V. {Sarkar} and G. {Reinman} and A. {Bui}},
journal = {IEEE Design   Test of Computers},
title   = {Customizable Domain-Specific Computing},
year    = {2011},
volume  = {28},
number  = {2},
pages   = {6-15}
}


@inproceedings{Corinna2016,
title     = {Learning with Rejection},
author    = {Corinna Cortes and Giulia DeSalvo and Mehryar Mohri},
booktitle = {ALT},
year      = {2016}
}



@misc{cotter2018twoplayer,
title         = {Two-Player Games for Efficient Non-Convex Constrained Optimization},
author        = {Andrew Cotter and Heinrich Jiang and Karthik Sridharan},
year          = {2018},
eprint        = {1804.06500},
archiveprefix = {arXiv},
primaryclass  = {cs.LG}
}

@misc{DARPA018,
title  = {DARPA Announces Next Phase of Electronics Resurgence Initiative},
author = {DARPA},
year   = {2018},
url    = {https://www.darpa.mil/news-events/2018-11-01a}
}

@article{DBLP_Sze,
author        = {Vivienne Sze and
             Yu{-}Hsin Chen and
             Tien{-}Ju Yang and
             Joel S. Emer},
title         = {Efficient Processing of Deep Neural Networks: {A} Tutorial and Survey},
journal       = {CoRR},
volume        = {abs/1703.09039},
year          = {2017},
url           = {http://arxiv.org/abs/1703.09039},
archiveprefix = {arXiv},
eprint        = {1703.09039}
}

@article{DBLPKornblith,
author        = {Simon Kornblith and
             Jonathon Shlens and
             Quoc V. Le},
title         = {Do Better ImageNet Models Transfer Better?},
journal       = {CoRR},
volume        = {abs/1805.08974},
year          = {2018},
url           = {http://arxiv.org/abs/1805.08974},
archiveprefix = {arXiv},
eprint        = {1805.08974},
timestamp     = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1805-08974},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{Dean202011TD,
title   = {1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
author  = {Jeffrey Dean},
journal = {2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
year    = {2020},
pages   = {8-14}
}

@article{deep-learning-scaling,
author  = {Joel Hestness and
             Sharan Narang and
             Newsha Ardalani and
             Gregory F. Diamos and
             Heewoo Jun and
             Hassan Kianinejad and
             Md. Mostofa Ali Patwary and
             Yang Yang and
             Yanqi Zhou},
title   = {Deep Learning Scaling is Predictable, Empirically},
journal = {CoRR},
volume  = {abs/1712.00409},
year    = {2017}
}


@article{deep-rewiring,
author  = {Guillaume Bellec and
             David Kappel and
             Wolfgang Maass and
             Robert A. Legenstein},
title   = {Deep {R}ewiring: {T}raining {V}ery {S}parse {D}eep {N}etworks},
journal = {CoRR},
volume  = {abs/1711.05136},
year    = {2017}
}


@misc{Demuth93neuralnetwork,
author = {Howard Demuth and Mark Beale},
title  = {Neural Network Toolbox For Use with Matlab - User Guide Verion 3.0},
year   = {1993}
}

@article{demvsar2006statistical,
title   = {Statistical comparisons of classifiers over multiple data sets},
author  = {Dem{\v{s}}ar, Janez},
journal = {Journal of Machine learning research},
volume  = {7},
number  = {Jan},
pages   = {1--30},
year    = {2006}
}

@article{dettmers2019sparse,
title   = {Sparse networks from scratch: Faster training without losing performance},
author  = {Dettmers, Tim and Zettlemoyer, Luke},
journal = {arXiv preprint arXiv:1907.04840},
year    = {2019}
}


@misc{Dettmers2020,
title  = {Which GPU for deep learning?},
author = {Tim Dettmers},
year   = {2020},
url    = {https://bit.ly/35qq8xe}
}

@article{DeVries2019,
author  = {Terrance DeVries and
            Ishan Misra and
            Changhan Wang and
            Laurens van der Maaten},
title   = {Does Object Recognition Work for Everyone?},
journal = {CoRR},
volume  = {abs/1906.02659},
year    = {2019}
}

@book{diamond98,
title     = {Guns, Germs, and Steel: The Fates of Human Societies},
author    = {Diamond, J.M. and Diamond, P.G.J. and Bernard Hames Collection},
isbn      = {9780393317558},
lccn      = {96037068},
series    = {National bestseller / W.W. Norton \& Company},
url       = {https://books.google.com/books?id=1lBu\_bqSsSMC},
year      = {1999},
publisher = {W.W. Norton}
}



@article{domingo_1995,
author = {Domingos, Pedro},
year   = {1995},
month  = {05},
pages  = {},
title  = {Rule Induction and Instance-Based Learning: A Unified Approach}
}

@misc{Dong2019,
author = {Zhen, Dong and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Keutzer, Kurt},
year   = {2019},
month  = {10},
pages  = {293-302},
title  = {HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision},
doi    = {10.1109/ICCV.2019.00038}
}
@article{dropout-journal,
author  = {Nitish Srivastava and
             Geoffrey E. Hinton and
             Alex Krizhevsky and
             Ilya Sutskever and
             Ruslan Salakhutdinov},
title   = {Dropout: a simple way to prevent neural networks from overfitting},
journal = {Journal of Machine Learning Research},
volume  = {15},
number  = {1},
pages   = {1929--1958},
year    = {2014}
}



@mastersthesis{dubowski2020activation,
title  = {Activation function impact on Sparse Neural Networks},
author = {Dubowski, Adam},
type   = {{B.S.} thesis},
year   = {2020},
school = {University of Twente}
}

@article{duchi2011adaptive,
title   = {Adaptive subgradient methods for online learning and stochastic optimization.},
author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
journal = {Journal of machine learning research},
volume  = {12},
number  = {7},
year    = {2011}
}

@book{DudaHart2nd,
author    = {R. O. Duda and P. E. Hart and D. G. Stork},
title     = {Pattern Classification},
publisher = {John Wiley and Sons},
edition   = {2nd},
year      = {2000}
}



@article{dwayne2001,
author  = {Dwayne Moore},
title   = {The Anna Karenina Principle Applied to Ecological Risk Assessments of Multiple Stressors},
journal = {Human and Ecological Risk Assessment: An International Journal},
volume  = {7},
number  = {2},
pages   = {231-237},
year    = {2001},
doi     = {10.1080/20018091094349}
}


@inproceedings{dynamic-network-surgery,
title     = {Dynamic {N}etwork {S}urgery for {E}fficient {DNN}s},
author    = {Yiwen Guo and Anbang Yao and Yurong Chen},
booktitle = {NeurIPS},
year      = {2016}
}


@article{Eagleman2036,
author    = {Eagleman, David M. and Sejnowski, Terrence J.},
title     = {Motion Integration and Postdiction in Visual Awareness},
volume    = {287},
number    = {5460},
pages     = {2036--2038},
year      = {2000},
doi       = {10.1126/science.287.5460.2036},
publisher = {American Association for the Advancement of Science},
issn      = {0036-8075},
url       = {https://science.sciencemag.org/content/287/5460/2036},
eprint    = {https://science.sciencemag.org/content/287/5460/2036.full.pdf},
journal   = {Science}
}

@inproceedings{eldan2016power,
title     = {The power of depth for feedforward neural networks},
author    = {Eldan, Ronen and Shamir, Ohad},
booktitle = {Conference on learning theory},
pages     = {907--940},
year      = {2016}
}

@inproceedings{Elsen_2020_CVPR,
author    = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
title     = {Fast Sparse ConvNets},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2020}
}

@article{evci2019difficulty,
title   = {The difficulty of training sparse neural networks},
author  = {Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
journal = {arXiv preprint arXiv:1906.10732},
year    = {2019}
}


@misc{evci2019rigging,
title         = {Rigging the Lottery: Making All Tickets Winners},
author        = {Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro and Erich Elsen},
year          = {2019},
eprint        = {1911.11134},
archiveprefix = {arXiv},
primaryclass  = {cs.LG}
}

@article{evci2019riggingarXiv191111134E,
author        = {{Evci}, Utku and {Gale}, Trevor and {Menick}, Jacob and
       {Castro}, Pablo Samuel and {Elsen}, Erich},
title         = {{Rigging the Lottery: Making All Tickets Winners}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2019,
month         = nov,
eid           = {arXiv:1911.11134},
pages         = {arXiv:1911.11134},
archiveprefix = {arXiv},
eprint        = {1911.11134},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191111134E},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{evci2020gradient,
title   = {Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win},
author  = {Evci, Utku and Ioannou, Yani A and Keskin, Cem and Dauphin, Yann},
journal = {arXiv preprint arXiv:2010.03533},
year    = {2020}
}


@article{false_discovery_rates,
author  = {Ridgeway, Greg and MacDonald, John},
year    = {2009},
month   = {06},
pages   = {661-668},
title   = {Doubly Robust Internal Benchmarking and False Discovery Rates for Detecting Racial Bias in Police Stops},
volume  = {104},
journal = {Journal of the American Statistical Association},
doi     = {10.1198/jasa.2009.0034}
}



@inproceedings{Fatahalian2004,
author    = {Fatahalian, K. and Sugerman, J. and Hanrahan, P.},
title     = {Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication},
year      = {2004},
isbn      = {3905673150},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/1058129.1058148},
doi       = {10.1145/1058129.1058148},
booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
pages     = {133–137},
numpages  = {5},
location  = {Grenoble, France},
series    = {HWWS ’04}
}

@article{fbnet,
author  = {Bichen Wu and
             Xiaoliang Dai and
             Peizhao Zhang and
             Yanghan Wang and
             Fei Sun and
             Yiming Wu and
             Yuandong Tian and
             Peter Vajda and
             Yangqing Jia and
             Kurt Keutzer},
title   = {FBNet: {H}ardware-{A}ware {E}fficient {C}onv{N}et {D}esign via {D}ifferentiable
             Neural Architecture Search},
journal = {CoRR},
volume  = {abs/1812.03443},
year    = {2018},
url     = {http://arxiv.org/abs/1812.03443}
}


@inproceedings{feldman2015certifying,
title     = {Certifying and removing disparate impact},
author    = {Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
booktitle = {proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
pages     = {259--268},
year      = {2015}
}

@article{feldman2019does,
title   = {Does learning require memorization? A short tale about a long tail},
author  = {Feldman, Vitaly},
journal = {arXiv preprint arXiv:1906.05271},
year    = {2019}
}



@article{Florenta2014,
author  = {Teodoridis, Florenta},
year    = {2014},
month   = {01},
pages   = {},
title   = {Generalists, Specialists, and the Direction of Inventive Activity},
journal = {SSRN Electronic Journal},
doi     = {10.2139/ssrn.2541383}
}


@inproceedings{Fong2017,
author    = {Ruth C. Fong and
             Andrea Vedaldi},
title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
booktitle = {{ICCV}},
pages     = {3449--3457},
publisher = {{IEEE} Computer Society},
year      = 2017
}





@article{frankle2018lottery,
title   = {The lottery ticket hypothesis: Finding sparse, trainable neural networks},
author  = {Frankle, Jonathan and Carbin, Michael},
journal = {arXiv preprint arXiv:1803.03635},
year    = {2018}
}



@article{frankle2019linear,
title   = {Linear mode connectivity and the lottery ticket hypothesis},
author  = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
journal = {arXiv preprint arXiv:1912.05671},
year    = {2019}
}


@article{frankle2019stabilizing,
title   = {Stabilizing the lottery ticket hypothesis},
author  = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
journal = {arXiv preprint arXiv:1903.01611},
year    = {2019}
}


@article{FUKUSHIMA1982455,
title   = {Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position},
journal = {Pattern Recognition},
volume  = {15},
number  = {6},
pages   = {455 - 469},
year    = {1982},
issn    = {0031-3203},
url     = {http://www.sciencedirect.com/science/article/pii/0031320382900243},
author  = {Kunihiko Fukushima and Sei Miyake}
}


@inproceedings{Fumera2002,
author    = {Fumera, Giorgio and Roli, Fabio},
title     = {Support Vector Machines with Embedded Reject Option},
booktitle = {Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines},
series    = {SVM '02},
year      = {2002},
isbn      = {3-540-44016-X},
pages     = {68--82},
numpages  = {15},
url       = {http://dl.acm.org/citation.cfm?id=647230.719259},
acmid     = {719259},
publisher = {Springer-Verlag},
address   = {London, UK, UK}
}



@article{funahashi1989approximate,
title     = {On the approximate realization of continuous mappings by neural networks},
author    = {Funahashi, Ken-Ichi},
journal   = {Neural networks},
volume    = {2},
number    = {3},
pages     = {183--192},
year      = {1989},
publisher = {Elsevier}
}


@misc{Gallistel2009,
author = {Gallistel, Charles and King, Adam},
year   = {2009},
month  = {04},
pages  = {288-298},
title  = {Memory and the Computational Brain: Why Cognitive Science Will Transform Neuroscience},
doi    = {10.1002/9781444310498.refs}
}


@article{Garg2017,
author  = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
year    = {2017},
month   = {11},
pages   = {},
title   = {Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes},
volume  = {115},
journal = {Proceedings of the National Academy of Sciences},
doi     = {10.1073/pnas.1720347115}
}

%% Citation entry on ArXiv
@book{gilder2000telecosm,
title     = {Telecosm: How Infinite Bandwidth Will Revolutionize Our World},
author    = {Gilder, G.},
isbn      = {9780743215947},
url       = {https://books.google.com/books?id=Kzo-KTxdwcEC},
year      = {2000},
publisher = {Free Press}
}

% Citation entry on ArXiv
@inproceedings{glorot2010understanding,
title     = {Understanding the difficulty of training deep feedforward neural networks},
author    = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
pages     = {249--256},
year      = {2010}
}

% Citation entry on Semantic Scholar
@inproceedings{glorot2011deep,
title     = {Deep sparse rectifier neural networks},
author    = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
pages     = {315--323},
year      = {2011}
}

%% Same as above, from arxiv. For hybrid AKA dynamic range quantization.
@book{goodfellow2016deep,
title     = {Deep learning},
author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume    = {1},
year      = {2016},
publisher = {MIT Press}
}

@article{Goodman2016,
author        = {{Goodman}, B. and {Flaxman}, S.},
title         = {{European Union regulations on algorithmic decision-making and a ``right to explanation''}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1606.08813},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Learning},
year          = 2016,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160608813G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Goyal2017,
author        = {{Goyal}, P. and {Doll{\'a}r}, P. and {Girshick}, R. and {Noordhuis}, P. and 
{Wesolowski}, L. and {Kyrola}, A. and {Tulloch}, A. and {Jia}, Y. and 
{He}, K.},
title         = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1706.02677},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
year          = 2017,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170602677G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{gradcam2017,
author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month     = {Oct},
year      = 2017
}


@misc{Gray2017GPUKF,
title  = {GPU Kernels for Block-Sparse Weights},
author = {Scott Gray and A. Radford and Diederik P. Kingma},
year   = {2017}
}


@article{Guo2016,
author        = {Yiwen Guo and
             Anbang Yao and
             Yurong Chen},
title         = {Dynamic Network Surgery for Efficient DNNs},
journal       = {CoRR},
volume        = {abs/1608.04493},
year          = {2016},
url           = {http://arxiv.org/abs/1608.04493},
archiveprefix = {arXiv},
eprint        = {1608.04493},
timestamp     = {Mon, 13 Aug 2018 16:48:43 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/GuoYC16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{gupta2018proxy,
title   = {Proxy fairness},
author  = {Gupta, Maya and Cotter, Andrew and Fard, Mahdi Milani and Wang, Serena},
journal = {arXiv preprint arXiv:1806.11212},
year    = {2018}
}
@article{tigrinya,
author    = {Alp {\"{O}}ktem and
             Mirko Plitt and
             Grace Tang},
title     = {Tigrinya Neural Machine Translation with Transfer Learning for Humanitarian
             Response},
journal   = {AfricaNLP Workshop},
year      = {2020},
url       = {https://arxiv.org/abs/2003.11523},
}

@article{tico,
author    = {Antonios Anastasopoulos and
             Alessandro Cattelan and
             Zi{-}Yi Dou and
             Marcello Federico and
             Christian Federmann and
             Dmitriy Genzel and
             Francisco Guzm{\'{a}}n and
             Junjie Hu and
             Macduff Hughes and
             Philipp Koehn and
             Rosie Lazar and
             William Lewis and
             Graham Neubig and
             Mengmeng Niu and
             Alp {\"{O}}ktem and
             Eric Paquin and
             Grace Tang and
             Sylwia Tur},
title     = {{TICO-19:} the Translation Initiative for Covid-19},
journal   = {CoRR},
volume    = {abs/2007.01788},
year      = {2020},
url       = {https://arxiv.org/abs/2007.01788},
archivePrefix = {arXiv},
eprint    = {2007.01788},
timestamp = {Thu, 08 Apr 2021 11:46:39 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2007-01788.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hanson1989comparing,
title     = {Comparing biases for minimal network construction with back-propagation},
author    = {Hanson, Stephen Jos{\'e} and Pratt, Lorien Y},
booktitle = {Advances in neural information processing systems},
pages     = {177--185},
year      = {1989}
}

@inproceedings{Hardt2016,
author    = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title     = {Equality of Opportunity in Supervised Learning},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
series    = {NeurIPS'16},
year      = {2016},
isbn      = {978-1-5108-3881-9},
location  = {Barcelona, Spain},
pages     = {3323--3331},
numpages  = {9},
url       = {http://dl.acm.org/citation.cfm?id=3157382.3157469},
acmid     = {3157469},
publisher = {Curran Associates Inc.},
address   = {USA}
}


@inproceedings{hardt2016equality,
title     = {Equality of opportunity in supervised learning},
author    = {Hardt, Moritz and Price, Eric and Srebro, Nati},
booktitle = {Advances in neural information processing systems},
pages     = {3315--3323},
year      = {2016}
}


@InProceedings{katharopoulos2019samples,
title = 	 {Processing Megapixel Images with Deep Attention-Sampling Models},
author =       {Katharopoulos, Angelos and Fleuret, Francois},
booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
pages = 	 {3282--3291},
year = 	 {2019},
editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
volume = 	 {97},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {09--15 Jun},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v97/katharopoulos19a/katharopoulos19a.pdf},
url = 	 {https://proceedings.mlr.press/v97/katharopoulos19a.html},
abstract = 	 {Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.}
}


@manual{Harwell:2019,
title  = {A face-scanning algorithm increasingly decides whether you deserve the job},
author = {Drew Harwell},
url    = {https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/},
year   = {2019 (accessed May 19, 2020)}
}


@inproceedings{Hassibi93secondorder,
author    = {Babak Hassibi and David G. Stork and Stork Crc. Ricoh. Com},
title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
booktitle = {Advances in Neural Information Processing Systems 5},
year      = {1993},
pages     = {164--171},
publisher = {Morgan Kaufmann}
}


@book{Hauck2007,
author    = {Hauck, Scott and DeHon, Andre},
title     = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation},
year      = {2007},
isbn      = {9780080556017},
publisher = {Morgan Kaufmann Publishers Inc.},
address   = {San Francisco, CA, USA}
}





@book{Haugeland,
author    = {Haugeland, John},
title     = {Artificial Intelligence: The Very Idea},
year      = {1985},
isbn      = {0262081539},
publisher = {Massachusetts Institute of Technology},
address   = {USA}
}




@inproceedings{he2015delving,
title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages     = {1026--1034},
year      = {2015}
}


@inproceedings{he2016deep,
title     = {Deep residual learning for image recognition},
author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages     = {770--778},
year      = {2016}
}

@article{Heeger1773,
author    = {Heeger, David J.},
title     = {Theory of cortical function},
volume    = {114},
number    = {8},
pages     = {1773--1782},
year      = {2017},
doi       = {10.1073/pnas.1619788114},
publisher = {National Academy of Sciences},
issn      = {0027-8424},
url       = {https://www.pnas.org/content/114/8/1773},
eprint    = {https://www.pnas.org/content/114/8/1773.full.pdf},
journal   = {Proceedings of the National Academy of Sciences}
}


@article{Hendrycksimageneta,
author        = {{Hendrycks}, Dan and {Zhao}, Kevin and {Basart}, Steven and
       {Steinhardt}, Jacob and {Song}, Dawn},
title         = {{Natural Adversarial Examples}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2019,
month         = jul,
eid           = {arXiv:1907.07174},
pages         = {arXiv:1907.07174},
archiveprefix = {arXiv},
eprint        = {1907.07174},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190707174H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@misc{Hennessy2019,
title  = {The End of Moore's Law, CPUs (as we
know them), and the Rise of Domain
Specific Architectures},
author = {John Hennessy},
url    = {https://www.kisacoresearch.com/sites/default/files/presentations/09.00_-_alphabet_-_john_hennessy.pdf},
year   = {2019}
}



@article{HerculanoHouzel2014TheEB,
title   = {The elephant brain in numbers},
author  = {Suzana Herculano-Houzel and Kamilla Avelino-de-Souza and Kleber Neves and Jairo Porfirio and D{\'e}bora J. Messeder and Larissa Mattos Feij{\'o} and Jose Maldonado and Paul R Manger},
journal = {Frontiers in Neuroanatomy},
year    = {2014},
volume  = {8}
}


@article{Hinton06,
author  = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages   = {1527--1554},
title   = {A Fast Learning Algorithm for Deep Belief Nets},
volume  = {18},
year    = {2006}
}

@book{Hinton1989,
author    = {Hinton, Geoffrey E. and Anderson, J. A.},
title     = {Parallel Models of Associative Memory},
year      = {1989},
isbn      = {080580269X},
publisher = {L. Erlbaum Associates Inc.},
address   = {USA}
}

@article{hinton2012neural,
title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
journal = {Cited on},
volume  = {14},
number  = {8},
year    = {2012}
}

@article{hochreiter1991untersuchungen,
title   = {Untersuchungen zu dynamischen neuronalen Netzen},
author  = {Hochreiter, Sepp},
journal = {Diploma, Technische Universit{\"a}t M{\"u}nchen},
volume  = {91},
number  = {1},
year    = {1991}
}




@misc{hochreiter2001gradient,
title     = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
author    = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
year      = {2001},
publisher = {A field guide to dynamical recurrent neural networks. IEEE Press}
}

@article{holi210171,
author  = {J. L. {Holi} and J. -. {Hwang}},
journal = {IEEE Transactions on Computers},
title   = {Finite precision error analysis of neural network hardware implementations},
year    = {1993},
volume  = {42},
number  = {3},
pages   = {281-290}
}    


inproceedings{Hooker2019ABF,
title     = {A Benchmark for Interpretability Methods in Deep Neural Networks},
author    = {Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
booktitle = {NeurIPS 2019},
year      = {2019},
url       = {https://papers.NeurIPS.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf}
}


@misc{hooker2020hardware,
title         = {The Hardware Lottery},
author        = {Sara Hooker},
year          = {2020},
eprint        = {2009.06489},
archiveprefix = {arXiv},
primaryclass  = {cs.CY}
}

@article{hornik1989multilayer,
title   = {Multilayer feedforward networks are universal approximators.},
author  = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
journal = {Neural networks},
volume  = {2},
number  = {5},
pages   = {359--366},
year    = {1989}
}



@misc{HotelSoftwarePF,
title  = {Software Productivity for Extreme-Scale Science},
author = {H. Hotel and H. Johansen and D. Bernholdt and M. H{\'e}roux and R. Hornung},
year   = {2014}
}

@inbook{Howe1994,
author    = {Howe, Denis B.
and Asanovi{\'{c}}, Krste},
title     = {SPACE: Symbolic Processing in Associative Computing Elements},
booktitle = {VLSI for Neural Networks and Artificial Intelligence},
year      = {1994},
publisher = {Springer US},
address   = {Boston, MA},
pages     = {243--252},
isbn      = {978-1-4899-1331-9},
doi       = {10.1007/978-1-4899-1331-9_24},
url       = {https://doi.org/10.1007/978-1-4899-1331-9_24}
}

@article{Hubara2016_training_neural_networks_low_precision,
author        = {Itay Hubara and
             Matthieu Courbariaux and
             Daniel Soudry and
             Ran El{-}Yaniv and
             Yoshua Bengio},
title         = {Quantized Neural Networks: Training Neural Networks with Low Precision
             Weights and Activations},
journal       = {CoRR},
volume        = {abs/1609.07061},
year          = {2016},
url           = {http://arxiv.org/abs/1609.07061},
archiveprefix = {arXiv},
eprint        = {1609.07061},
timestamp     = {Mon, 13 Aug 2018 16:49:12 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/HubaraCSEB16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{Hughes2017,
author        = {{Wu}, M. and {Hughes}, M.~C. and {Parbhoo}, S. and {Zazzi}, M. and 
{Roth}, V. and {Doshi-Velez}, F.},
title         = {{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.06178},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171106178W},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Hutchinson2020,
title     = {Social Biases in NLP Models as Barriers for Persons with Disabilities},
author    = {Ben Hutchinson and Vinodkumar Prabhakaran and Emily Denton and Kellie Webster and Yu Zhong and Stephen Craig Denuyl},
year      = {2020},
booktitle = {Proceedings of ACL 2020}
}


@article{inductive_biases,
author        = {Peter W. Battaglia and
Jessica B. Hamrick and
Victor Bapst and
Alvaro Sanchez{-}Gonzalez and
Vin{\'{\i}}cius Flores Zambaldi and
Mateusz Malinowski and
Andrea Tacchetti and
David Raposo and
Adam Santoro and
Ryan Faulkner and
{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
H. Francis Song and
Andrew J. Ballard and
Justin Gilmer and
George E. Dahl and
Ashish Vaswani and
Kelsey R. Allen and
Charles Nash and
Victoria Langston and
Chris Dyer and
Nicolas Heess and
Daan Wierstra and
Pushmeet Kohli and
Matthew Botvinick and
Oriol Vinyals and
Yujia Li and
Razvan Pascanu},
title         = {Relational inductive biases, deep learning, and graph networks},
journal       = {CoRR},
volume        = {abs/1806.01261},
year          = {2018},
url           = {http://arxiv.org/abs/1806.01261},
archiveprefix = {arXiv},
eprint        = {1806.01261},
timestamp     = {Wed, 24 Jul 2019 18:56:21 +0200},
biburl        = {https://dblp.org/rec/journals/corr/abs-1806-01261.bib},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Jacob_2018,
title     = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
isbn      = {9781538664209},
url       = {http://dx.doi.org/10.1109/CVPR.2018.00286},
doi       = {10.1109/cvpr.2018.00286},
journal   = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
publisher = {IEEE},
author    = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
year      = {2018},
month     = {Jun}
}


@misc{jeff1990,
title  = {Parallel Implementations of neural network training: two back-propagation approaches.},
author = {Jeff Dean},
url    = {https://drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view},
year   = {1990}
}

@article{jiang2019fantastic,
title   = {Fantastic generalization measures and where to find them},
author  = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
journal = {arXiv preprint arXiv:1912.02178},
year    = {2019}
}

@article{jin2015deep,
title   = {Deep learning with s-shaped rectified linear activation units},
author  = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
journal = {arXiv preprint arXiv:1512.07030},
year    = {2015}
}

@article{Jouppi2017,
author     = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title      = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year       = {2017},
issue_date = {May 2017},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {45},
number     = {2},
issn       = {0163-5964},
url        = {https://doi.org/10.1145/3140659.3080246},
doi        = {10.1145/3140659.3080246},
journal    = {SIGARCH Comput. Archit. News},
month      = jun,
pages      = {1–12},
numpages   = {12},
keywords   = {LSTM, CNN, RNN, TPU, DNN, accelerator, neural network, deep learning, MLP, domain-specific architecture, TensorFlow, GPU}
}

@article{kallus2019assessing,
title   = {Assessing algorithmic fairness with unobserved protected class using data combination},
author  = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
journal = {arXiv preprint arXiv:1906.00285},
year    = {2019}
}


@article{karnin1990simple,
title     = {A simple procedure for pruning back-propagation trained neural networks},
author    = {Karnin, Ehud D},
journal   = {IEEE transactions on neural networks},
volume    = {1},
number    = {2},
pages     = {239--242},
year      = {1990},
publisher = {IEEE}
}




@misc{karpathy2014,
author       = {Karpathy, Andrej},
year         = {2014},
title        = {What I learned from competing against a ConvNet on ImageNet},
howpublished = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}},
note         = {Accessed: 2019-07-07}
}

misc{karpathy2014,
author       = {Karpathy, Andrej},
year         = {2014},
title        = {What I learned from competing against a ConvNet on ImageNet},
howpublished = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}},
note         = {Accessed: 2019-07-07}
}

@inproceedings{kay2015unequal,
title     = {Unequal representation and gender stereotypes in image search results for occupations},
author    = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages     = {3819--3828},
year      = {2015}
}

@phdthesis{kearns89,
author = {M. J. Kearns},
title  = {Computational Complexity of Machine Learning},
school = {Department of Computer Science, Harvard University},
year   = {1989}
}

phdthesis{kearns89,
author = {M. J. Kearns},
title  = {Computational Complexity of Machine Learning},
school = {Department of Computer Science, Harvard University},
year   = {1989}
}

@article{kendall1938new,
title     = {A new measure of rank correlation},
author    = {Kendall, Maurice G},
journal   = {Biometrika},
volume    = {30},
number    = {1/2},
pages     = {81--93},
year      = {1938},
publisher = {JSTOR}
}

@article{Kennedy2000SignalprocessingMA,
title   = {Signal-processing machines at the postsynaptic density.},
author  = {Mary B. Kennedy},
journal = {Science},
year    = {2000},
volume  = {290 5492},
pages   = {
750-4
}
}

@misc{keras_pruning,
author       = {{Keras TensorFlow}},
title        = {Keras Tensorflow Magnitude Pruning Open Source Code},
howpublished = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}},
note         = {Accessed: 2019-07-10}
}

misc{keras_pruning,
author       = {{Keras TensorFlow}},
title        = {Keras Tensorflow Magnitude Pruning Open Source Code},
howpublished = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}},
note         = {Accessed: 2019-07-10}
}


@article{kim2017,
author        = {{Kim}, B. and {Wattenberg}, M. and {Gilmer}, J. and {Cai}, C. and 
{Wexler}, J. and {Viegas}, F. and {Sayres}, R.},
title         = {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.11279},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171111279K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{kingma2014adam,
title   = {Adam: A method for stochastic optimization},
author  = {Kingma, Diederik P and Ba, Jimmy},
journal = {arXiv preprint arXiv:1412.6980},
year    = {2014}
}

@misc{kingsburyhipnet,
author = {Kingsbury, Brian and Morgan, Nelson and Wawrzynek, John},
year   = {1998},
month  = {03},
pages  = {},
title  = {HiPNeT-1: A Highly Pipelined Architecture for Neural Network Training}
}

@book{kipling1899,
author        = { Kipling, Rudyard},
title         = { American notes / by Rudyard Kipling },
publisher     = { Brown and Company Boston },
pages         = { 1 online resource (137 pages, 1 unnumbered leaf of plates) : },
year          = { 1899 },
type          = { Book, Online },
language      = { English },
subjects      = { United States -- Description and travel.; United States -- Social life and customs -- 1865-1918. },
life-dates    = { 1899 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn6847688 }
}




@article{kitty_paper2017,
author        = {{Kindermans}, P.-J. and {Hooker}, S. and {Adebayo}, J. and {Alber}, M. and 
{Sch{\"u}tt}, K.~T. and {D{\"a}hne}, S. and {Erhan}, D. and 
{Kim}, B.},
title         = {{The (Un)reliability of saliency methods}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.00867},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171100867K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{kleinberg2016inherent,
title   = {Inherent trade-offs in the fair determination of risk scores},
author  = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
journal = {arXiv preprint arXiv:1609.05807},
year    = {2016}
}


@inproceedings{koh2017,
title     = {Understanding Black-box Predictions via Influence Functions},
author    = {Pang Wei Koh and Percy Liang},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
pages     = {1885--1894},
year      = {2017},
editor    = {Doina Precup and Yee Whye Teh},
volume    = {70},
series    = {Proceedings of Machine Learning Research},
address   = {International Convention Centre, Sydney, Australia},
month     = {06--11 Aug},
publisher = {PMLR},
pdf       = {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf}
}





@article{kornblith2018,
author        = {Simon Kornblith and
Jonathon Shlens and
Quoc V. Le},
title         = {Do Better ImageNet Models Transfer Better?},
journal       = {CoRR},
volume        = {abs/1805.08974},
year          = {2018},
url           = {http://arxiv.org/abs/1805.08974},
archiveprefix = {arXiv},
eprint        = {1805.08974},
timestamp     = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl        = {https://dblp.org/rec/journals/corr/abs-1805-08974.bib},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Kornblith2018DoBI,
title   = {Do Better ImageNet Models Transfer Better?},
author  = {Simon Kornblith and Jonathon Shlens and Quoc V. Le},
journal = {ArXiv},
year    = {2018},
volume  = {abs/1805.08974}
}


@article{Kriegman1853,
author    = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
title     = {A scalable pipeline for designing reconfigurable organisms},
volume    = {117},
number    = {4},
pages     = {1853--1859},
year      = {2020},
doi       = {10.1073/pnas.1910837117},
publisher = {National Academy of Sciences},
issn      = {0027-8424},
url       = {https://www.pnas.org/content/117/4/1853},
eprint    = {https://www.pnas.org/content/117/4/1853.full.pdf},
journal   = {Proceedings of the National Academy of Sciences}
}

@article{krizhevsky2009cifar,
title   = {Cifar-10 and cifar-100 datasets},
author  = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
journal = {URl: https://www. cs. toronto. edu/kriz/cifar. html},
volume  = {6},
pages   = {1},
year    = {2009}
}

@inproceedings{krogh1992simple,
title     = {A simple weight decay can improve generalization},
author    = {Krogh, Anders and Hertz, John A},
booktitle = {Advances in neural information processing systems},
pages     = {950--957},
year      = {1992}
}

@inproceedings{Kubat97addressingthe,
author    = {Miroslav Kubat and Stan Matwin},
title     = {Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
booktitle = {In Proceedings of the Fourteenth International Conference on Machine Learning},
year      = {1997},
pages     = {179--186},
publisher = {Morgan Kaufmann}
}


@misc{Kubota2018,
title  = {China Plans $47$ Billion Fund to Boost Its Semiconductor Industry},
author = {Yoko Kubota},
year   = {2018},
url    = {https://on.wsj.com/32L7Kwn}
}

@book{Kuhn1962,
added-at  = {2011-09-20T15:32:20.000+0200},
address   = {Chicago},
author    = {Kuhn, Thomas S.},
biburl    = {https://www.bibsonomy.org/bibtex/231d37d5bf096e1cf5e893934336464d3/voj},
interhash = {329d7457ec19e3c93e0737215924fdfc},
intrahash = {31d37d5bf096e1cf5e893934336464d3},
keywords  = {paradigm},
publisher = {University of Chicago Press},
timestamp = {2013-01-07T08:31:59.000+0100},
title     = {The Structure of Scientific Revolutions},
year      = 1962
}

@inproceedings{kumar17,
title     = {Resource-efficient Machine Learning in 2 {KB} {RAM} for the Internet of Things},
author    = {Ashish Kumar and Saurabh Goyal and Manik Varma},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
pages     = {1935--1944},
year      = {2017},
editor    = {Doina Precup and Yee Whye Teh},
volume    = {70},
series    = {Proceedings of Machine Learning Research},
address   = {International Convention Centre, Sydney, Australia},
month     = {06--11 Aug},
publisher = {PMLR},
pdf       = {http://proceedings.mlr.press/v70/kumar17a/kumar17a.pdf},
url       = {http://proceedings.mlr.press/v70/kumar17a.html},
abstract  = {This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing’s L3 ranker when the model size is restricted to 300 bytes. Bonsai’s code can be downloaded from (http://www.manikvarma.org/code/Bonsai/download.html).}
}

@misc{dsouza2021tale,
    title={A Tale Of Two Long Tails}, 
    author={Daniel D'souza and Zach Nussbaum and Chirag Agarwal and Sara Hooker},
    year={2021},
    eprint={2107.13098},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{NIPS2017_3f5ee243,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Attention is All you Need},
url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
volume = {30},
year = {2017}
}


@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition}, 
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{liu2023outofdistribution,
    title={Towards Out-Of-Distribution Generalization: A Survey}, 
    author={Jiashuo Liu and Zheyan Shen and Yue He and Xingxuan Zhang and Renzhe Xu and Han Yu and Peng Cui},
    year={2023},
    eprint={2108.13624},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{Quionero2009,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press},
abstract = {Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series}
}

@article{Shen2021TowardsOG,
title={Towards Out-Of-Distribution Generalization: A Survey},
author={Zheyan Shen and Jiashuo Liu and Yue He and Xingxuan Zhang and Renzhe Xu and Han Yu and Peng Cui},
journal={ArXiv},
year={2021},
volume={abs/2108.13624},
url={https://api.semanticscholar.org/CorpusID:237364121}
}

@article{Patil2022,
author = {Patil, Pranita and Purcell, Kevin},
year = {2022},
month = {03},
pages = {110},
title = {Decorrelation-Based Deep Learning for Bias Mitigation},
volume = {14},
journal = {Future Internet},
doi = {10.3390/fi14040110}
}

@article{santurkar2020,
author       = {Shibani Santurkar and
                Dimitris Tsipras and
                Aleksander Madry},
title        = {{BREEDS:} Benchmarks for Subpopulation Shift},
journal      = {CoRR},
volume       = {abs/2008.04859},
year         = {2020},
url          = {https://arxiv.org/abs/2008.04859},
eprinttype    = {arXiv},
eprint       = {2008.04859},
timestamp    = {Sun, 16 Aug 2020 17:19:29 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2008-04859.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{hendrycks2021natural,
    title={Natural Adversarial Examples}, 
    author={Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Song},
    year={2021},
    eprint={1907.07174},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{
djurisic2023extremely,
title={Extremely Simple Activation Shaping for Out-of-Distribution Detection},
author={Andrija Djurisic and Nebojsa Bozanic and Arjun Ashok and Rosanne Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ndYXTEL6cZz}
}

@article{MORENOTORRES2012521,
title = {A unifying view on dataset shift in classification},
journal = {Pattern Recognition},
volume = {45},
number = {1},
pages = {521-530},
year = {2012},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2011.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0031320311002901},
author = {Jose G. Moreno-Torres and Troy Raeder and Rocío Alaiz-Rodríguez and Nitesh V. Chawla and Francisco Herrera},
keywords = {Dataset shift, Data fracture, Changing environments, Differing training and test populations, Covariate shift, Sample selection bias, Non-stationary distributions},
abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.}
}

@misc{yang2024imagenetood,
    title={ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms}, 
    author={William Yang and Byron Zhang and Olga Russakovsky},
    year={2024},
    eprint={2310.01755},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{koh2021wilds,
    title={WILDS: A Benchmark of in-the-Wild Distribution Shifts}, 
    author={Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
    year={2021},
    eprint={2012.07421},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{
dngelo2022uncertaintybased,
title={Uncertainty-based out-of-distribution detection requires suitable function space priors},
author={Francesco D'Angelo and Christian Henning},
year={2022},
url={https://openreview.net/forum?id=u7UxOTefG2}
}

@inproceedings{
farquhar2022what,
title={What 'Out-of-distribution' Is and Is Not},
author={Sebastian Farquhar and Yarin Gal},
booktitle={NeurIPS ML Safety Workshop},
year={2022},
url={https://openreview.net/forum?id=XCS_zBHQA2i}
}

@INPROCEEDINGS{9878988,
author={Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={How Well Do Sparse ImageNet Models Transfer?}, 
year={2022},
volume={},
number={},
pages={12256-12266},
keywords={Training;Adaptation models;Computer vision;Image coding;Transfer learning;Pattern recognition;Convolutional neural networks;Efficient learning and inferences; Transfer/low-shot/long-tail learning},
doi={10.1109/CVPR52688.2022.01195}}

@inproceedings{krogh1991,
author = {Krogh, Anders and Hertz, John A.},
title = {A simple weight decay can improve generalization},
year = {1991},
isbn = {1558602224},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
pages = {950–957},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'91}
}

@misc{mostafa2019parameter,
    title={Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization}, 
    author={Hesham Mostafa and Xin Wang},
    year={2019},
    eprint={1902.05967},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{evci2021rigging,
    title={Rigging the Lottery: Making All Tickets Winners}, 
    author={Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro and Erich Elsen},
    year={2021},
    eprint={1911.11134},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{9065523,
author={Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},
booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
title={SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training}, 
year={2020},
volume={},
number={},
pages={58-70},
keywords={Training;Sparse matrices;Arrays;Graphics processing units;Engines;Kernel;Tensile stress},
doi={10.1109/HPCA47549.2020.00015}}

@INPROCEEDINGS{8980296,
author={Zhang, Jiaqi and Chen, Xiangru and Song, Mingcong and Li, Tao},
booktitle={2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)}, 
title={Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks}, 
year={2019},
volume={},
number={},
pages={292-303},
keywords={Neural Network Training;Neural Network Pruning;Software-Hardware Co-Design},
doi={}}

@misc{see2016compression,
    title={Compression of Neural Machine Translation Models via Pruning}, 
    author={Abigail See and Minh-Thang Luong and Christopher D. Manning},
    year={2016},
    eprint={1606.09274},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{gale2019,
author    = {Trevor Gale and
             Erich Elsen and
             Sara Hooker},
title     = {The State of Sparsity in Deep Neural Networks},
journal   = {CoRR},
volume    = {abs/1902.09574},
year      = {2019},
url       = {http://arxiv.org/abs/1902.09574},
eprinttype = {arXiv},
eprint    = {1902.09574},
timestamp = {Tue, 21 May 2019 18:03:40 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-1902-09574.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gordon-etal-2020-compressing,
  title = "Compressing {BERT}: Studying the Effects of Weight Pruning on Transfer Learning",
  author = "Gordon, Mitchell  and
    Duh, Kevin  and
    Andrews, Nicholas",
  booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.repl4nlp-1.18",
  doi = "10.18653/v1/2020.repl4nlp-1.18",
  pages = "143--155",
  abstract = "Pre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40{%}) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.",
}

@article{l0-regularization,
author  = {Christos Louizos and
             Max Welling and
             Diederik P. Kingma},
title   = {Learning {S}parse {N}eural {N}etworks {t}hrough {L}\({}_{\mbox{0}}\) {R}egularization},
journal = {CoRR},
volume  = {abs/1712.01312},
year    = {2017}
}

@misc{2020hooker,
doi = {10.48550/ARXIV.2010.03058},

url = {https://arxiv.org/abs/2010.03058},

author = {Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Characterising Bias in Compressed Models},

publisher = {arXiv},

year = {2020},

copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{labatie2019characterizing,
title        = {Characterizing well-behaved vs. pathological deep neural networks},
author       = {Labatie, Antoine},
booktitle    = {International Conference on Machine Learning},
pages        = {3611--3621},
year         = {2019},
organization = {PMLR}
}



@inproceedings{Lakshminarayanan2017,
author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title     = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
series    = {NeurIPS'17},
year      = {2017},
isbn      = {978-1-5108-6096-4},
location  = {Long Beach, California, USA},
pages     = {6405--6416},
numpages  = {12},
url       = {http://dl.acm.org/citation.cfm?id=3295222.3295387},
acmid     = {3295387},
publisher = {Curran Associates Inc.},
address   = {USA}
}



@inproceedings{langley00,
author    = {P. Langley},
title     = {Crafting Papers on Machine Learning},
year      = {2000},
pages     = {1207--1216},
editor    = {Pat Langley},
booktitle = {Proceedings of the 17th International Conference
            on Machine Learning (ICML 2000)},
address   = {Stanford, CA},
publisher = {Morgan Kaufmann}
}


@misc{Lapedus2017,
title  = {Foundry Challenges In 2018},
author = {Mark Lapedus},
year   = {2017},
url    = {https://semiengineering.com/foundry-challenges-in-2018/}
}

@article{larus2008spending,
author     = {Larus, James},
title      = {Spending Moore's Dividend},
year       = {2009},
issue_date = {May 2009},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {52},
number     = {5},
issn       = {0001-0782},
url        = {https://doi.org/10.1145/1506409.1506425},
doi        = {10.1145/1506409.1506425},
abstract   = {Multicore computers shift the burden of software performance from chip designers and processor architects to software developers.},
journal    = {Commun. ACM},
month      = may,
pages      = {62–69},
numpages   = {8}
}

@article{lasso,
author  = {Robert Tibshirani},
title   = {Regression {S}hrinkage and {S}election {V}ia the {L}asso},
journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
year    = {1994},
volume  = {58},
pages   = {267--288}
}


@article{Lattner2020MLIRAC,
title   = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
author  = {Chris Lattner and Jacques A. Pienaar and Mehdi Amini and Uday Bondhugula and River Riddle and Albert Cohen and Tatiana Shpeisman and Andy Davis and Nicolas Vasilache and Oleksandr Zinenko},
journal = {ArXiv},
year    = {2020},
volume  = {abs/2002.11054}
}}

@misc{LeCun1989,
author    = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
year      = {1989},
publisher = {MIT Press},
volume    = {1},
number    = {4},
url       = {https://doi.org/10.1162/neco.1989.1.4.541},
pages     = {541–551}
}

@incollection{LeCun1990,
title     = {Optimal Brain Damage},
author    = {{LeCun}, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor    = {D. S. Touretzky},
pages     = {598--605},
year      = {1990},
publisher = {Morgan-Kaufmann},
url       = {http://papers.NeurIPS.cc/paper/250-optimal-brain-damage.pdf}
}


@inproceedings{lecun1990optimal,
title     = {Optimal brain damage},
author    = {LeCun, Yann and Denker, John S and Solla, Sara A},
booktitle = {Advances in neural information processing systems},
pages     = {598--605},
year      = {1990}
}

@article{Lee2011,
author  = {H. {Lee} and K. {Brown} and A. {Sujeeth} and H. {Chafi} and T. {Rompf} and M. {Odersky} and K. {Olukotun}},
journal = {IEEE Micro},
title   = {Implementing Domain-Specific Languages for Heterogeneous Parallel Computing},
year    = {2011},
volume  = {31},
number  = {5},
pages   = {42-53}
}


@misc{Lee2018,
title  = {The next step in Facebook AI hardware infrastructure},
author = {Lee, Kevin  and Wang,Xiaodong},
year   = {2018},
url    = {https://bit.ly/3bgZFDn}
}


@article{lee2018snip,
title   = {Snip: Single-shot network pruning based on connection sensitivity},
author  = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
journal = {arXiv preprint arXiv:1810.02340},
year    = {2018}
}

@inproceedings{lee2018training,
title     = {Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},
author    = {Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
booktitle = {International Conference on Learning Representations},
year      = {2018},
url       = {https://openreview.net/forum?id=ryiAv2xAZ}
}



@article{lee2019signal,
title   = {A signal propagation perspective for pruning neural networks at initialization},
author  = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
journal = {arXiv preprint arXiv:1906.06307},
year    = {2019}
}


@article{Leisersoneaam9744,
author       = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
title        = {There{\textquoteright}s plenty of room at the Top: What will drive computer performance after Moore{\textquoteright}s law?},
volume       = {368},
number       = {6495},
elocation-id = {eaam9744},
year         = {2020},
doi          = {10.1126/science.aam9744},
publisher    = {American Association for the Advancement of Science},
issn         = {0036-8075},
url          = {https://science.sciencemag.org/content/368/6495/eaam9744},
eprint       = {https://science.sciencemag.org/content/368/6495/eaam9744.full.pdf},
journal      = {Science}
}


@article{lenet,
author   = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
journal  = {Proceedings of the IEEE},
title    = {Gradient-based learning applied to document recognition},
year     = {1998},
volume   = {86},
number   = {11},
pages    = {2278-2324},
keywords = {optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
doi      = {10.1109/5.726791},
issn     = {0018-9219},
month    = {Nov}
}


@inproceedings{liang2018enhancing,
title     = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
author    = {Shiyu Liang and Yixuan Li and R. Srikant},
booktitle = {International Conference on Learning Representations},
year      = {2018},
url       = {https://openreview.net/forum?id=H1VGkIxRZ}
}




@article{LILLICRAP201982,
title   = {Backpropagation through time and the brain},
journal = {Current Opinion in Neurobiology},
volume  = {55},
pages   = {82 - 89},
year    = {2019},
note    = {Machine Learning, Big Data, and Neuroscience},
issn    = {0959-4388},
doi     = {https://doi.org/10.1016/j.conb.2019.01.011},
url     = {http://www.sciencedirect.com/science/article/pii/S0959438818302009},
author  = {Timothy P Lillicrap and Adam Santoro}
}


@article{Lin1004,
author    = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
title     = {All-optical machine learning using diffractive deep neural networks},
volume    = {361},
number    = {6406},
pages     = {1004--1008},
year      = {2018},
doi       = {10.1126/science.aat8084},
publisher = {American Association for the Advancement of Science},
issn      = {0036-8075},
url       = {https://science.sciencemag.org/content/361/6406/1004},
eprint    = {https://science.sciencemag.org/content/361/6406/1004.full.pdf},
journal   = {Science}
}

@inproceedings{Lindsey1994,
author       = {Lindsey, Clark S. and Lindblad, Thomas},
title        = {{Review of hardware neural networks: A User's perspective}},
booktitle    = {{3rd Workshop on Neural Networks: From Biology to High-energy Physics}},
reportnumber = {TRITA-FYS-9012},
pages        = {0215--224},
month        = {9},
year         = {1994}
}


@article{Linnainmaa1976TaylorEO,
title   = {Taylor expansion of the accumulated rounding error},
author  = {Seppo Linnainmaa},
journal = {BIT Numerical Mathematics},
year    = {1976},
volume  = {16},
pages   = {146-160}
}

@misc{lispcode,
title  = {Course: 15-880(A) -- Introduction to Neural Networks},
author = {Dave Touretzky and Alex Waibel},
year   = {1995},
url    = {shorturl.at/evKX9}
}

@article{liu2018rethinking,
title   = {Rethinking the value of network pruning},
author  = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
journal = {arXiv preprint arXiv:1810.05270},
year    = {2018}
}

@article{liu2020finding,
title   = {Finding trainable sparse networks through Neural Tangent Transfer},
author  = {Liu, Tianlin and Zenke, Friedemann},
journal = {arXiv preprint arXiv:2006.08228},
year    = {2020}
}

@inproceedings{LiuLWT15,
added-at  = {2018-10-09T00:00:00.000+0200},
author    = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
biburl    = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
booktitle = {ICCV},
crossref  = {conf/iccv/2015},
ee        = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
interhash = {3f735aaa11957e73914bbe2ca9d5e702},
intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},
isbn      = {978-1-4673-8391-2},
keywords  = {dblp},
pages     = {3730-3738},
publisher = {IEEE Computer Society},
timestamp = {2018-10-11T11:43:28.000+0200},
title     = {Deep Learning Face Attributes in the Wild.},
url       = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
year      = 2015
}



@article{loshchilov2017decoupled,
title   = {Decoupled weight decay regularization},
author  = {Loshchilov, Ilya and Hutter, Frank},
journal = {arXiv preprint arXiv:1711.05101},
year    = {2017}
}

@article{lottery-ticket-hypothesis,
author  = {Jonathan Frankle and
             Michael Carbin},
title   = {The {L}ottery {T}icket {H}ypothesis: {T}raining {P}runed {N}eural {N}etworks},
journal = {CoRR},
volume  = {abs/1803.03635},
year    = {2018},
url     = {http://arxiv.org/abs/1803.03635}
}


@article{LRP2016,
author    = {Avanti Shrikumar and
             Peyton Greenside and
             Anna Shcherbina and
             Anshul Kundaje},
title     = {Not Just a Black Box: Learning Important Features Through Propagating
             Activation Differences},
journal   = {CoRR},
volume    = {abs/1605.01713},
year      = 2016,
url       = {http://arxiv.org/abs/1605.01713},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShrikumarGSK16},
bibsource = {dblp computer science bibliography, http://dblp.org}
}


@inproceedings{lu2017expressive,
title     = {The expressive power of neural networks: A view from the width},
author    = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
booktitle = {Advances in neural information processing systems},
pages     = {6231--6239},
year      = {2017}
}

@misc{lucas1991,
author  = {Lucas, Peter and van der Gaag, Linda},
title   = {Principles of Expert Systems},
year    = {1991},
address = {USA}
}


@misc{lush2002,
title    = {Technical report: Lush reference manual, code available at http://lush.sourceforge.net},
author   = {Yann Lecun and Leon Bottou},
year     = {2002},
language = {English (US)},
type     = {Other}
}




@book{MachineLearningI,
editor    = {R. S. Michalski and J. G. Carbonell and T.
    M. Mitchell},
title     = {Machine Learning: An Artificial Intelligence
    Approach, Vol. I},
publisher = {Tioga},
year      = {1983},
address   = {Palo Alto, CA}
}


@article{macia2014,
author  = {Macía, Javier and Sole, Ricard},
year    = {2014},
month   = {02},
pages   = {e81248},
title   = {How to Make a Synthetic Multicellular Computer},
volume  = {9},
journal = {PloS one},
doi     = {10.1371/journal.pone.0081248}
}

@inproceedings{madaan2018analyze,
title     = {Analyze, detect and remove gender stereotyping from bollywood movies},
author    = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
booktitle = {Conference on Fairness, Accountability and Transparency},
pages     = {92--105},
year      = {2018}
}

@inproceedings{malsburg_1986,
author    = {Van Der Malsburg, C.},
editor    = {Palm, G{\"u}nther
and Aertsen, Ad},
title     = {Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
booktitle = {Brain Theory},
year      = {1986},
publisher = {Springer Berlin Heidelberg},
address   = {Berlin, Heidelberg},
pages     = {245--248},
abstract  = {Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.},
isbn      = {978-3-642-70911-1}
}

@article{Marblestone2016,
author  = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
title   = {Toward an Integration of Deep Learning and Neuroscience},
journal = {Frontiers in Computational Neuroscience},
volume  = {10},
pages   = {94},
year    = {2016},
url     = {https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
doi     = {10.3389/fncom.2016.00094},
issn    = {1662-5188}
}

@article{marcus2014,
title   = {The atoms of neural computation},
author  = {Gary Marcus and Adam Marblestone and Tom Dean},
year    = {2014},
note    = {Computational Neuroscience},
journal = {Science},
pages   = {551-552},
volume  = {346}
}

@article{Mcclelland1995,
author  = {Mcclelland, James and Mcnaughton, Bruce and O’Reilly, Randall},
year    = {1995},
month   = {08},
pages   = {419-57},
title   = {Why There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
volume  = {102},
journal = {Psychological review},
doi     = {10.1037/0033-295X.102.3.419}
}

@misc{MCCLOSKEY1989109,
title     = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor    = {Gordon H. Bower},
series    = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume    = {24},
pages     = {109 - 165},
year      = {1989},
issn      = {0079-7421},
doi       = {https://doi.org/10.1016/S0079-7421(08)60536-8},
author    = {Michael McCloskey and Neal J. Cohen}
}

@inproceedings{mccoy_etal_2019_right,
title     = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
author    = {McCoy, Tom  and
    Pavlick, Ellie  and
    Linzen, Tal},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
month     = jul,
year      = {2019},
address   = {Florence, Italy},
publisher = {Association for Computational Linguistics},
url       = {https://www.aclweb.org/anthology/P19-1334},
doi       = {10.18653/v1/P19-1334},
pages     = {3428--3448},
abstract  = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.}
}



@book{mcdonald2009handbook,
title     = {Handbook of biological statistics},
author    = {McDonald, John H},
volume    = {2},
year      = {2009},
publisher = {sparky house publishing Baltimore, MD}
}


@article{memory-bounded-convnet,
author  = {Maxwell D. Collins and
             Pushmeet Kohli},
title   = {Memory {B}ounded {D}eep {C}onvolutional {N}etworks},
journal = {CoRR},
volume  = {abs/1412.1442},
year    = {2014},
url     = {http://arxiv.org/abs/1412.1442}
}


@article{Mernik2005,
author     = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
title      = {When and How to Develop Domain-Specific Languages},
year       = {2005},
issue_date = {December 2005},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {37},
number     = {4},
issn       = {0360-0300},
url        = {https://doi.org/10.1145/1118890.1118892},
doi        = {10.1145/1118890.1118892},
journal    = {ACM Comput. Surv.},
month      = dec,
pages      = {316–344},
numpages   = {29},
keywords   = {language development system, domain analysis, application language, Domain-specific language}
}


@inproceedings{merrill2016merge,
title        = {Merge-based parallel sparse matrix-vector multiplication},
author       = {Merrill, Duane and Garland, Michael},
booktitle    = {SC'16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages        = {678--689},
year         = {2016},
organization = {IEEE}
} 
[download]

@misc{micikevicius2017mixed,
title         = {Mixed Precision Training},
author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
year          = {2017},
eprint        = {1710.03740},
archiveprefix = {arXiv},
primaryclass  = {cs.AI}
}


@misc{mirhoseini2020chip,
title         = {Chip Placement with Deep Reinforcement Learning},
author        = {Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Jiang and Ebrahim Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Sungmin Bae and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and William Hang and Emre Tuncer and Anand Babu and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
year          = {2020},
eprint        = {2004.10746},
archiveprefix = {arXiv},
primaryclass  = {cs.LG}
}

@article{MISRA2010239,
title   = {Artificial neural networks in hardware: A survey of two decades of progress},
journal = {Neurocomputing},
volume  = {74},
number  = {1},
pages   = {239 - 255},
year    = {2010},
note    = {Artificial Brains},
issn    = {0925-2312},
doi     = {https://doi.org/10.1016/j.neucom.2010.03.021},
url     = {http://www.sciencedirect.com/science/article/pii/S092523121000216X},
author  = {Janardan Misra and Indranil Saha}
}


@inproceedings{mitchell2019model,
title     = {Model cards for model reporting},
author    = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
booktitle = {Proceedings of the conference on fairness, accountability, and transparency},
pages     = {220--229},
year      = {2019}
} 
[download]

@techreport{mitchell80,
author      = {T. M. Mitchell},
title       = {The Need for Biases in Learning Generalizations},
institution = {Computer Science Department, Rutgers University},
year        = {1980},
address     = {New Brunswick, MA}
}


@article{mobilenetv2,
author  = {Mark Sandler and
             Andrew G. Howard and
             Menglong Zhu and
             Andrey Zhmoginov and
             Liang{-}Chieh Chen},
title   = {{I}nverted {R}esiduals and {L}inear {B}ottlenecks: {M}obile {N}etworks for {C}lassification,
             Detection and Segmentation},
journal = {CoRR},
volume  = {abs/1801.04381},
year    = {2018}
}



@article{mocanu2018scalable,
title     = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
author    = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
journal   = {Nature communications},
volume    = {9},
number    = {1},
pages     = {1--12},
year      = {2018},
publisher = {Nature Publishing Group}
}



@article{Montavon2011,
author        = {Montavon, Gregoire and Braun, Mikio L and M{\"u}ller, Klaus-Robert},
date-added    = {2016-10-12 11:13:21 +0000},
date-modified = {2016-10-19 16:36:27 +0000},
journal       = {Journal of Machine Learning Research},
number        = {Sep},
pages         = {2563--2581},
title         = {Kernel analysis of deep networks},
volume        = {12},
year          = 2011
}


@article{Montavon2017,
author        = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
date-modified = {2017-02-10 22:17:33 +0000},
journal       = {Pattern Recognition},
pages         = {211--222},
publisher     = {Elsevier},
title         = {Explaining nonlinear classification decisions with deep taylor decomposition},
volume        = {65},
year          = 2017
}




@article{moore1965,
author  = {Moore, Gordon},
journal = {Electronics},
month   = {April},
number  = {8},
title   = {Cramming more components onto integrated circuits},
url     = {https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf},
volume  = {38},
year    = {1965}
} 

@article{Moravec98whenwill,
author  = {Hans Moravec},
title   = {When will computer hardware match the human brain},
journal = {Journal of Transhumanism},
year    = {1998},
volume  = {1}
}



@article{morgan1983,
author  = {Morgan, M. Granger},
title   = {The fifth generation: Artificial intelligence and Japan's computer challenge to the world, by Edward A. Feigenbaum and Pamela McCorduck. Reading, MA: Addison-Wesley, 1983, 275 pp. Price: \$15.35},
journal = {Journal of Policy Analysis and Management},
volume  = {3},
number  = {1},
pages   = {156-156},
doi     = {10.2307/3324061},
url     = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3324061},
eprint  = {https://onlinelibrary.wiley.com/doi/pdf/10.2307/3324061},
year    = {1983}
}


@article{MORGAN1992248,
title   = {The ring array processor: A multiprocessing peripheral for connectionist applications},
journal = {Journal of Parallel and Distributed Computing},
volume  = {14},
number  = {3},
pages   = {248 - 259},
year    = {1992},
issn    = {0743-7315},
doi     = {https://doi.org/10.1016/0743-7315(92)90067-W},
url     = {http://www.sciencedirect.com/science/article/pii/074373159290067W},
author  = {Nelson Morgan and James Beck and Phil Kohn and Jeff Bilmes and Eric Allman and Joachim Beer}
}

@article{mostafa2019parameter,
title   = {Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
author  = {Mostafa, Hesham and Wang, Xin},
journal = {arXiv preprint arXiv:1902.05967},
year    = {2019}
}

@inproceedings{nair2010rectified,
title     = {Rectified linear units improve restricted boltzmann machines},
author    = {Nair, Vinod and Hinton, Geoffrey E},
booktitle = {ICML},
year      = {2010}
}

@article{namhoon2018,
author        = {Namhoon Lee and
             Thalaiyasingam Ajanthan and
             Philip H. S. Torr},
title         = {{SNIP:} Single-shot Network Pruning based on Connection Sensitivity},
journal       = {CoRR},
volume        = {abs/1810.02340},
year          = {2018},
url           = {http://arxiv.org/abs/1810.02340},
archiveprefix = {arXiv},
eprint        = {1810.02340},
timestamp     = {Tue, 30 Oct 2018 10:49:09 +0100},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-02340},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}




@article{neal1992connectionist,
title     = {Connectionist learning of belief networks},
author    = {Neal, Radford M},
journal   = {Artificial intelligence},
volume    = {56},
number    = {1},
pages     = {71--113},
year      = {1992},
publisher = {Elsevier}
}


@inproceedings{network-slimming,
author    = {Zhuang Liu and
             Jianguo Li and
             Zhiqiang Shen and
             Gao Huang and
             Shoumeng Yan and
             Changshui Zhang},
title     = {Learning {E}fficient {C}onvolutional {N}etworks through {N}etwork {S}limming},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {2755--2763},
year      = {2017}
}


@incollection{Newell81,
author    = {A. Newell and P. S. Rosenbloom},
title     = {Mechanisms of Skill Acquisition and the Law of
                Practice},
booktitle = {Cognitive Skills and Their Acquisition},
pages     = {1--51},
publisher = {Lawrence Erlbaum Associates, Inc.},
year      = {1981},
editor    = {J. R. Anderson},
chapter   = {1},
address   = {Hillsdale, NJ}
}



@inproceedings{NeurIPS1988_119,
title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
author={Mozer, Michael C and Smolensky, Paul},
booktitle={Advances in neural information processing systems},
pages={107--115},
year={1989}
}


@misc{NeurIPS2012_4824,
title     = {ImageNet Classification with Deep Convolutional Neural Networks},
author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
pages     = {1097--1105},
year      = {2012},
url       = {https://bit.ly/2GneDwp}
}

@incollection{NeurIPS2016_6300,
title     = {Examples are not enough, learn to criticize! Criticism for Interpretability},
author    = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {2280--2288},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf}
}


@incollection{NeurIPS2016_6316,
title     = {Satisfying Real-world Goals with Dataset Constraints},
author    = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {2415--2423},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf}
}


@incollection{NeurIPS2016Cortes,
title     = {Boosting with Abstention},
author    = {Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {1660--1668},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6336-boosting-with-abstention.pdf}
}


@misc{NeurIPS2017_6975,
title     = {Dynamic Routing Between Capsules},
author    = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 30},
pages     = {3856--3866},
year      = {2017},
url       = {http://papers.NeurIPS.cc/paper/6975-dynamic-routing-between-capsules.pdf}
}

@incollection{NeurIPS2018_7308,
title     = {Sparse DNNs with Improved Adversarial Robustness},
author    = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
booktitle = {Advances in Neural Information Processing Systems 31},
editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages     = {242--251},
year      = {2018},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf}
}


@incollection{NeurIPS2019_8410,
title     = {Model Compression with Adversarial Robustness: A Unified Optimization Framework},
author    = {Gui, Shupeng and Wang, Haotao N and Yang, Haichuan and Yu, Chen and Wang, Zhangyang and Liu, Ji},
booktitle = {Advances in Neural Information Processing Systems 32},
editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages     = {1285--1296},
year      = {2019},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework.pdf}
}

@incollection{NeurIPSKendall2017,
title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
author    = {Kendall, Alex and Gal, Yarin},
booktitle = {Advances in Neural Information Processing Systems 30},
editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages     = {5574--5584},
year      = {2017},
publisher = {Curran Associates, Inc.}
}


@incollection{NeurIPSKingma,
title     = {Variational Dropout and the Local Reparameterization Trick},
author    = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 28},
editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages     = {2575--2583},
year      = {2015},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf}
}



@article{nocedal2002behavior,
title     = {On the behavior of the gradient norm in the steepest descent method},
author    = {Nocedal, Jorge and Sartenaer, Annick and Zhu, Ciyou},
journal   = {Computational Optimization and Applications},
volume    = {22},
number    = {1},
pages     = {5--35},
year      = {2002},
publisher = {Springer}
}

@article{Novel2009,
author   = {H. A. Fayed and A. F. Atiya},
journal  = {IEEE Transactions on Neural Networks},
title    = {A Novel Template Reduction Approach for the$K$-Nearest Neighbor Method},
year     = {2009},
volume   = {20},
number   = {5},
pages    = {890-896},
keywords = {data reduction;pattern classification;template reduction approach;K-nearest neighbor method;pattern classification;large data set;condensing approach;pattern removal;computational burden;data reduction;Nearest neighbor searches;Prototypes;Pattern classification;Cellular neural networks;Classification algorithms;Satellites;Layout;Medical diagnosis;Design engineering;Mathematics;Condensing;cross validation;editing;$K$-nearest neighbor (KNN);template reduction},
doi      = {10.1109/TNN.2009.2018547},
issn     = {1045-9227},
month    = {May}
}


@misc{nvidia2020,
title  = {NVIDIA Ampere Architecture In-Depth.},
author = {Krashinsky, Ronny and Giroux, Olivier and Jones, Stephen and Stam, Nick and Ramaswamy,Sridhar},
url    = {https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/},
year   = {2020}
}


@article{olah2017feature,
author  = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
title   = {Feature Visualization},
journal = {Distill},
year    = 2017,
note    = {https://distill.pub/2017/feature-visualization},
doi     = {10.23915/distill.00007}
}



@article{Olukotun2014,
author     = {Olukotun, Kunle},
title      = {Beyond Parallel Programming with Domain Specific Languages},
year       = {2014},
issue_date = {August 2014},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {49},
number     = {8},
issn       = {0362-1340},
url        = {https://doi.org/10.1145/2692916.2557966},
doi        = {10.1145/2692916.2557966},
journal    = {SIGPLAN Not.},
month      = feb,
pages      = {179–180},
numpages   = {2},
keywords   = {domain specific languages}
}

@inproceedings{optimal-brain-damage,
author    = {Yann LeCun and
             John S. Denker and
             Sara A. Solla},
title     = {Optimal {B}rain {D}amage},
booktitle = {{NeurIPS}},
pages     = {598--605},
publisher = {Morgan Kaufmann},
year      = {1989}
}


@inproceedings{optimal-brain-surgeon,
author    = {Babak Hassibi and
             David G. Stork},
title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
booktitle = {{NeurIPS}},
pages     = {164--171},
publisher = {Morgan Kaufmann},
year      = {1992}
}


@inproceedings{papadimitriou1980,
author    = {Papadimitriou, Christos H.
and Bentley, Jon Louis},
editor    = {de Bakker, Jaco
and van Leeuwen, Jan},
title     = {A worst-case analysis of nearest neighbor searching by projection},
booktitle = {Automata, Languages and Programming},
year      = {1980}
}




@article{papernot2015,
author        = {{Papernot}, N. and {McDaniel}, P. and {Jha}, S. and {Fredrikson}, M. and 
{Berkay Celik}, Z. and {Swami}, A.},
title         = {{The Limitations of Deep Learning in Adversarial Settings}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1511.07528},
primaryclass  = {cs.CR},
keywords      = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = 2015,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2015arXiv151107528P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{pascanu2013difficulty,
title     = {On the difficulty of training recurrent neural networks},
author    = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
booktitle = {International conference on machine learning},
pages     = {1310--1318},
year      = {2013}
}


@misc{piaget1954,
title  = {The construction of reality in the child},
author = {Jean Piaget},
year   = {1954}
}


@article{PMID15632230,
title   = {How the brain decides what we see},
author  = {Smythies, John},
doi     = {10.1177/014107680509800106},
number  = {1},
volume  = {98},
month   = {January},
year    = {2005},
journal = {Journal of the Royal Society of Medicine},
issn    = {0141-0768},
pages   = {18—20},
url     = {https://europepmc.org/articles/PMC1079232}
}

@inproceedings{pmlrbuolamwini18a,
title     = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
author    = {Joy Buolamwini and Timnit Gebru},
booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
pages     = {77--91},
year      = {2018},
editor    = {Sorelle A. Friedler and Christo Wilson},
volume    = {81},
series    = {Proceedings of Machine Learning Research},
address   = {New York, NY, USA},
month     = {23--24 Feb},
publisher = {PMLR},
url       = {http://proceedings.mlr.press/v81/buolamwini18a.html}
}



@article{polyak1964some,
title     = {Some methods of speeding up the convergence of iteration methods},
author    = {Polyak, Boris T},
journal   = {USSR Computational Mathematics and Mathematical Physics},
volume    = {4},
number    = {5},
pages     = {1--17},
year      = {1964},
publisher = {Elsevier}
}

@book{posselt1888jacquard,
title     = {The Jacquard Machine Analyzed and Explained: The Preparation of Jacquard Cards and Practical Hints to Learners of Jacquard Designing},
author    = {Posselt, E.A.},
series    = {Posselt's textile library},
url       = {https://books.google.com/books?id=-6FtmgEACAAJ},
year      = {1888},
publisher = {E.A. Posselt}
}

@article{pruning-convnet-nvidia,
author  = {Pavlo Molchanov and
             Stephen Tyree and
             Tero Karras and
             Timo Aila and
             Jan Kautz},
title   = {Pruning {C}onvolutional {N}eural {N}etworks for {R}esource {E}fficient {T}ransfer {L}earning},
journal = {CoRR},
volume  = {abs/1611.06440},
year    = {2016}
}




@inproceedings{quoc2012,
author    = {Le, Quoc V. and Ranzato, Marc’Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
title     = {Building High-Level Features Using Large Scale Unsupervised Learning},
year      = {2012},
isbn      = {9781450312851},
publisher = {Omnipress},
address   = {Madison, WI, USA},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages     = {507–514},
numpages  = {8},
location  = {Edinburgh, Scotland},
series    = {ICML’12}
}

@inproceedings{Raghu2017,
author    = {Maithra Raghu and
             Justin Gilmer and
             Jason Yosinski and
             Jascha Sohl{-}Dickstein},
title     = {{SVCCA:} Singular Vector Canonical Correlation Analysis for Deep Learning
             Dynamics and Interpretability},
booktitle = {{NeurIPS}},
pages     = {6078--6087},
year      = 2017
}




@incollection{RAKI1994,
title     = {Synaptic development of the cerebral cortex: implications for learning, memory, and mental illness},
editor    = {J. Van Pelt and M.A. Corner and H.B.M. Uylings and F.H. Lopes Da Silva},
series    = {Progress in Brain Research},
publisher = {Elsevier},
volume    = {102},
pages     = {227 - 243},
year      = {1994},
booktitle = {The Self-Organizing Brain: From Growth Cones to Functional Networks},
issn      = {0079-6123},
doi       = {https://doi.org/10.1016/S0079-6123(08)60543-9},
url       = {http://www.sciencedirect.com/science/article/pii/S0079612308605439},
author    = {Pasko Rakic and Jean-Pierre Bourgeois and Patricia S. Goldman-Rakic}
}



@incollection{RAKIC1994227,
title     = {Synaptic development of the cerebral cortex: implications for learning, memory, and mental illness},
editor    = {J. Van Pelt and M.A. Corner and H.B.M. Uylings and F.H. Lopes Da Silva},
series    = {Progress in Brain Research},
publisher = {Elsevier},
volume    = {102},
pages     = {227 - 243},
year      = {1994},
booktitle = {The Self-Organizing Brain: From Growth Cones to Functional Networks},
issn      = {0079-6123},
doi       = {https://doi.org/10.1016/S0079-6123(08)60543-9},
url       = {http://www.sciencedirect.com/science/article/pii/S0079612308605439},
author    = {Pasko Rakic and Jean-Pierre Bourgeois and Patricia S. Goldman-Rakic},
abstract  = {Publisher Summary
This chapter explores various questions: Are synapses added as we learn? Are there more synapses in some cortical areas than in others? Are there gender differences in synaptic density and do we lose synapses as we age? If we lose synapses with age, what is the timing and rate of this dissolution? To address these issue this chapter present the finding reported in the rhesus monkey. The study of major structural and functional subdivisions of the cortex over the primate lifespan offers a particularly comprehensive view of synapse formation. From study, it is eminently clear that knowledge of the normal course and mechanisms of synapse formation, the influence of various exogenous and endogenous events upon synapse stability and turnover, are essential prerequisites to determining the locus and timing of etiological factors in diseases that affect the cortex and alter cognitive function.}
}


@article{ramachandran2017swish,
title     = {Swish: a self-gated activation function},
author    = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
journal   = {arXiv preprint arXiv:1710.05941},
volume    = {7},
year      = {2017},
publisher = {Technical report}
}

@book{Raymond1990,
author    = {Kurzweil, Raymond},
title     = {The Age of Intelligent Machines},
year      = {1990},
publisher = {MIT Press},
address   = {Cambridge, MA, USA}
}



@article{Recht2019,
author        = {Benjamin Recht and
             Rebecca Roelofs and
             Ludwig Schmidt and
             Vaishaal Shankar},
title         = {Do ImageNet Classifiers Generalize to ImageNet?},
journal       = {CoRR},
volume        = {abs/1902.10811},
year          = {2019},
url           = {http://arxiv.org/abs/1902.10811},
archiveprefix = {arXiv},
eprint        = {1902.10811},
timestamp     = {Tue, 21 May 2019 18:03:38 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1902-10811},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Reddi2020,
author    = {V. J. {Reddi} and C. {Cheng} and D. {Kanter} and P. {Mattson} and G. {Schmuelling} and C. {Wu} and B. {Anderson} and M. {Breughe} and M. {Charlebois} and W. {Chou} and R. {Chukka} and C. {Coleman} and S. {Davis} and P. {Deng} and G. {Diamos} and J. {Duke} and D. {Fick} and J. S. {Gardner} and I. {Hubara} and S. {Idgunji} and T. B. {Jablin} and J. {Jiao} and T. S. {John} and P. {Kanwar} and D. {Lee} and J. {Liao} and A. {Lokhmotov} and F. {Massa} and P. {Meng} and P. {Micikevicius} and C. {Osborne} and G. {Pekhimenko} and A. T. R. {Rajan} and D. {Sequeira} and A. {Sirasao} and F. {Sun} and H. {Tang} and M. {Thomson} and F. {Wei} and E. {Wu} and L. {Xu} and K. {Yamada} and B. {Yu} and G. {Yuan} and A. {Zhong} and P. {Zhang} and Y. {Zhou}},
booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
title     = {MLPerf Inference Benchmark},
year      = {2020},
volume    = {},
number    = {},
pages     = {446-459}
}

@article{reed_1993_pruning_algorithms_survey,
author   = {R. {Reed}},
journal  = {IEEE Transactions on Neural Networks},
title    = {Pruning algorithms-a survey},
year     = {1993},
volume   = {4},
number   = {5},
pages    = {740-747},
keywords = {learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods},
doi      = {10.1109/72.248452},
issn     = {1045-9227},
month    = {Sep.}
}




@article{rethinking-pruning,
author  = {Zhuang Liu and
             Mingjie Sun and
             Tinghui Zhou and
             Gao Huang and
             Trevor Darrell},
title   = {Rethinking the {V}alue of {N}etwork {P}runing},
journal = {CoRR},
volume  = {abs/1810.05270},
year    = {2018}
}



@article{ridgeway2009,
title     = {Doubly robust internal benchmarking and false discovery rates for detecting racial bias in police stops},
author    = {Ridgeway, Greg and MacDonald, John M},
journal   = {Journal of the American Statistical Association},
volume    = {104},
number    = {486},
pages     = {661--668},
year      = {2009},
publisher = {Taylor \& Francis}
}


@inproceedings{rn50,
author    = {Kaiming He and
             Xiangyu Zhang and
             Shaoqing Ren and
             Jian Sun},
title     = {{D}eep {R}esidual {L}earning for {I}mage {R}ecognition},
booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
             {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
pages     = {770--778},
year      = {2016}
}


@article{robbins1951stochastic,
title     = {A stochastic approximation method},
author    = {Robbins, Herbert and Monro, Sutton},
journal   = {The annals of mathematical statistics},
pages     = {400--407},
year      = {1951},
publisher = {JSTOR}
}

@article{rosenblatt1958perceptron,
title     = {The perceptron: a probabilistic model for information storage and organization in the brain.},
author    = {Rosenblatt, Frank},
journal   = {Psychological review},
volume    = {65},
number    = {6},
pages     = {386},
year      = {1958},
publisher = {American Psychological Association}
}

@article{RossFinale2017,
author  = {Andrew Slavin Ross and
             Finale Doshi{-}Velez},
title   = {Improving the Adversarial Robustness and Interpretability of Deep
             Neural Networks by Regularizing their Input Gradients},
journal = {CoRR},
volume  = {abs/1711.09404},
year    = 2017
}


@article{ruder2016overview,
title   = {An overview of gradient descent optimization algorithms},
author  = {Ruder, Sebastian},
journal = {arXiv preprint arXiv:1609.04747},
year    = {2016}
}

@book{rumelhart1987,
editor    = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
title     = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
year      = {1986},
isbn      = {026268053X},
publisher = {MIT Press},
address   = {Cambridge, MA, USA}
}

@inproceedings{runtime-neural-pruning,
author    = {Ji Lin and
             Yongming Rao and
             Jiwen Lu and
             Jie Zhou},
title     = {Runtime Neural Pruning},
booktitle = {{NeurIPS}},
pages     = {2178--2188},
year      = {2017}
} 


@article{Russakovsky2015,
author   = {Russakovsky, Olga
and Deng, Jia
and Su, Hao
and Krause, Jonathan
and Satheesh, Sanjeev
and Ma, Sean
and Huang, Zhiheng
and Karpathy, Andrej
and Khosla, Aditya
and Bernstein, Michael
and Berg, Alexander C.
and Fei-Fei, Li},
title    = {ImageNet Large Scale Visual Recognition Challenge},
journal  = {International Journal of Computer Vision},
year     = {2015},
month    = {Dec},
day      = {01},
volume   = {115},
number   = {3},
pages    = {211--252},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
issn     = {1573-1405},
doi      = {10.1007/s11263-015-0816-y},
url      = {https://doi.org/10.1007/s11263-015-0816-y}
}



@article{Sackinger129422,
author  = {E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal = {IEEE Transactions on Neural Networks},
title   = {Application of the ANNA neural network chip to high-speed character recognition},
year    = {1992},
volume  = {3},
number  = {3},
pages   = {498-505}
}

@article{samek2017,
author   = {W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. Müller},
journal  = {IEEE Transactions on Neural Networks and Learning Systems},
title    = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
year     = 2017,
month    = {Nov},
volume   = {28},
number   = {11},
pages    = {2660-2673},
keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;DNN;ILSVRC2012;MIT Places data sets;SUN397;complex machine learning tasks;data visualization;deconvolution method;deep neural network;heatmap;multilayer nonlinear structure;relevance propagation algorithm;sensitivity-based approach;Algorithm design and analysis;Biological neural networks;Deconvolution;Heating;Learning systems;Neurons;Sensitivity;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
doi      = {10.1109/TNNLS.2016.2599820},
issn     = {2162-237X}
}



@article{Samuel59,
author  = {A. L. Samuel},
title   = {Some Studies in Machine Learning Using the Game of
    Checkers},
journal = {IBM Journal of Research and Development},
year    = {1959},
volume  = {3},
number  = {3},
pages   = {211--229}
}




@article{saxe2013exact,
title   = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
author  = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
journal = {arXiv preprint arXiv:1312.6120},
year    = {2013}
}

@inproceedings{scaling-nmt,
author    = {Myle Ott and
             Sergey Edunov and
             David Grangier and
             Michael Auli},
title     = {Scaling {N}eural {M}achine {T}ranslation},
booktitle = {Proceedings of the Third Conference on Machine Translation: Research
             Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1, 2018},
pages     = {1--9},
year      = {2018}
}


@article{Sehwag2019,
author        = {Vikash Sehwag and
             Shiqi Wang and
             Prateek Mittal and
             Suman Jana},
title         = {Towards Compact and Robust Deep Neural Networks},
journal       = {CoRR},
volume        = {abs/1906.06110},
year          = {2019},
url           = {http://arxiv.org/abs/1906.06110},
archiveprefix = {arXiv},
eprint        = {1906.06110},
timestamp     = {Mon, 24 Jun 2019 17:28:45 +0200},
biburl        = {https://dblp.org/rec/journals/corr/abs-1906-06110.bib},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@misc{sehwag2020pruning,
title         = {On Pruning Adversarially Robust Neural Networks},
author        = {Vikash Sehwag and Shiqi Wang and Prateek Mittal and Suman Jana},
year          = {2020},
eprint        = {2002.10509},
archiveprefix = {arXiv},
primaryclass  = {cs.CV}
}

@article{Shalf2020TheFO,
title   = {The future of computing beyond Moore’s Law},
author  = {John Shalf},
journal = {Philosophical Transactions of the Royal Society A},
year    = {2020},
volume  = {378}
}

%Todo differentiate shooker2019 refs 
@article{shooker2019,
author        = {Sara Hooker and
             Dumitru Erhan and
             Pieter{-}Jan Kindermans and
             Been Kim},
title         = {Evaluating Feature Importance Estimates},
journal       = {CoRR},
volume        = {abs/1806.10758},
year          = {2018},
url           = {http://arxiv.org/abs/1806.10758},
archiveprefix = {arXiv},
eprint        = {1806.10758}
}



@article{singh_article,
author  = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year    = {2015},
month   = {01},
pages   = {Pages 76-85},
title   = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume  = {Volume 58},
journal = {Communications of the ACM},
doi     = {10.1145/2688498.2688499}
}

@article{Singh2015,
author  = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year    = {2015},
month   = {01},
pages   = {Pages 76-85},
title   = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume  = {Volume 58},
journal = {Communications of the ACM},
doi     = {10.1145/2688498.2688499}
}

@inproceedings{lubana2021a,
title     = {A Gradient Flow Framework For Analyzing Network Pruning},
author    = {Ekdeep Singh Lubana and Robert Dick},
booktitle = {International Conference on Learning Representations},
year      = {2021},
url       = {https://openreview.net/forum?id=rumv7QmLUue}
}

@article{singledirection2018,
author        = {{Morcos}, A.~S. and {Barrett}, D.~G.~T. and {Rabinowitz}, N.~C. and 
{Botvinick}, M.},
title         = {{On the importance of single directions for generalization}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1803.06959},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
year          = 2018,
month         = mar,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180306959M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{sparse-connection-1997,
author    = {Nikko Str\"om},
title     = {Sparse {C}onnection and {P}runing in {L}arge {D}ynamic {A}rtificial {N}eural {N}etworks},
booktitle = {EUROSPEECH},
year      = {1997}
}


@article{spelke2007,
author   = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
title    = {Core knowledge},
journal  = {Developmental Science},
volume   = {10},
number   = {1},
pages    = {89-96},
doi      = {10.1111/j.1467-7687.2007.00569.x},
url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00569.x},
eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-7687.2007.00569.x},
abstract = {Abstract Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
year     = {2007}
}


@article{spike-and-slab,
author    = { T. J.   Mitchell  and  J. J.   Beauchamp },
title     = {Bayesian {V}ariable {S}election in {L}inear {R}egression},
journal   = {Journal of the American Statistical Association},
volume    = {83},
number    = {404},
pages     = {1023-1032},
year      = {1988},
publisher = {Taylor & Francis}
}



@article{squeezenet2018,
author        = {Forrest N. Iandola and
             Matthew W. Moskewicz and
             Khalid Ashraf and
             Song Han and
             William J. Dally and
             Kurt Keutzer},
title         = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB
             model size},
journal       = {CoRR},
volume        = {abs/1602.07360},
year          = {2016},
url           = {http://arxiv.org/abs/1602.07360},
archiveprefix = {arXiv},
eprint        = {1602.07360},
timestamp     = {Mon, 13 Aug 2018 16:46:12 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/IandolaMAHDK16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{srivastava2015highway,
title   = {Highway networks},
author  = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
journal = {arXiv preprint arXiv:1505.00387},
year    = {2015}
}



@book{stein2004handbook,
title     = {The Handbook of Multisensory Processes},
author    = {Stein, G.C.C.S.B.E. and Calvert, G. and Spence, C. and Spence, D.E.P.C. and Stein, B.E. and Stein, P.C.B.E.},
isbn      = {9780262033213},
lccn      = {2004042612},
series    = {A Bradford book},
url       = {https://books.google.com/books?id=CZS\_yDoFV7AC},
year      = {2004},
publisher = {MIT Press}
}

@incollection{Stevenson2018,
author  = {{Megan}, Stevenson},
title   = {{Assessing Risk Assessment in Action}},
journal = {Minnesota Law Review},
year    = {2018},
volume  = {58},
url     = {https://scholarship.law.umn.edu/mlr/58}
}


@inproceedings{stochastic-backpropagation,
author    = {Danilo Jimenez Rezende and
             Shakir Mohamed and
             Daan Wierstra},
title     = {Stochastic {B}ackpropagation and {A}pproximate {I}nference in {D}eep {G}enerative
             Models},
booktitle = {{ICML}},
series    = {{JMLR} Workshop and Conference Proceedings},
volume    = {32},
pages     = {1278--1286},
publisher = {JMLR.org},
year      = {2014}
}



@inproceedings{stock2018,
author    = {Pierre Stock and
             Moustapha Ciss{\'{e}}},
title     = {ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and
             Uncovering Biases},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VI}},
pages     = {504--519},
year      = {2018},
crossref  = {DBLP:conf/eccv/2018-6},
url       = {https://doi.org/10.1007/978-3-030-01231-1\_31},
doi       = {10.1007/978-3-030-01231-1\_31},
timestamp = {Tue, 14 May 2019 10:00:45 +0200},
biburl    = {https://dblp.org/rec/bib/conf/eccv/StockC18},
bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{Strom97sparseconnection,
author = {Nikko Ström},
title  = {Sparse Connection And Pruning In Large Dynamic Artificial Neural Networks},
year   = {1997}
}


@article{Stroop1935,
pages   = {643},
doi     = {10.1037/h0054651},
title   = {Studies of Interference in Serial Verbal Reactions},
number  = {6},
journal = {Journal of Experimental Psychology},
year    = {1935},
volume  = {18},
author  = {J. R. Stroop}
}

@misc{strubell2019energy,
title         = {Energy and Policy Considerations for Deep Learning in NLP},
author        = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
year          = {2019},
eprint        = {1906.02243},
archiveprefix = {arXiv},
primaryclass  = {cs.CL}
}

@inproceedings{sutskever2013importance,
title     = {On the importance of initialization and momentum in deep learning},
author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle = {International conference on machine learning},
pages     = {1139--1147},
year      = {2013}
}

@article{sws,
author  = {Karen Ullrich and
             Edward Meeds and
             Max Welling},
title   = {Soft {W}eight-{S}haring for {N}eural {N}etwork {C}ompression},
journal = {CoRR},
volume  = {abs/1702.04008},
year    = {2017}
}


@article{taiji_1998,
title     = {Grape-4: A Teraflops Machine for N-Body Simulations},
volume    = {11},
doi       = {10.1017/S1539299600018244},
number    = {2},
journal   = {Highlights of Astronomy},
publisher = {Cambridge University Press},
author    = {Taiji, Makoto},
year      = {1998},
pages     = {600–602}
}

@article{tan2007,
author  = {Tan, Cheemeng and Song, Hao and Niemi, Jarad and You, Lingchong},
year    = {2007},
month   = {06},
pages   = {343-53},
title   = {A synthetic biology challenge: Making cells compute},
volume  = {3},
journal = {Molecular bioSystems},
doi     = {10.1039/b618473c}
}

@article{tanaka2020pruning,
title   = {Pruning neural networks without any data by iteratively conserving synaptic flow},
author  = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
journal = {arXiv preprint arXiv:2006.05467},
year    = {2020}
}

@book{tani2016exploring,
title     = {Exploring Robotic Minds: Actions, Symbols, and Consciousness as Self-organizing Dynamic Phenomena},
author    = {Tani, J. and Oxford University Press},
isbn      = {9780190281083},
lccn      = {2016014889},
series    = {Oxford series on cognitive models and architectures},
url       = {https://books.google.com/books?id=QswnnQAACAAJ},
year      = {2016},
publisher = {Oxford University Press}
}

@inproceedings{tartaglione2018learning,
title     = {Learning sparse neural networks via sensitivity-driven regularization},
author    = {Tartaglione, Enzo and Leps{\o}y, Skjalg and Fiandrotti, Attilio and Francini, Gianluca},
booktitle = {Advances in neural information processing systems},
pages     = {3878--3888},
year      = {2018}
}




@misc{tensorflow2015,
title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url    = {https://www.tensorflow.org/},
note   = {Software available from tensorflow.org},
author = {
  Mart\'{\i}n~Abadi and
  Ashish~Agarwal and
  Paul~Barham and
  Eugene~Brevdo and
  Zhifeng~Chen and
  Craig~Citro and
  Greg~S.~Corrado and
  Andy~Davis and
  Jeffrey~Dean and
  Matthieu~Devin and
  Sanjay~Ghemawat and
  Ian~Goodfellow and
  Andrew~Harp and
  Geoffrey~Irving and
  Michael~Isard and
  Yangqing Jia and
  Rafal~Jozefowicz and
  Lukasz~Kaiser and
  Manjunath~Kudlur and
  Josh~Levenberg and
  Dandelion~Man\'{e} and
  Rajat~Monga and
  Sherry~Moore and
  Derek~Murray and
  Chris~Olah and
  Mike~Schuster and
  Jonathon~Shlens and
  Benoit~Steiner and
  Ilya~Sutskever and
  Kunal~Talwar and
  Paul~Tucker and
  Vincent~Vanhoucke and
  Vijay~Vasudevan and
  Fernanda~Vi\'{e}gas and
  Oriol~Vinyals and
  Pete~Warden and
  Martin~Wattenberg and
  Martin~Wicke and
  Yuan~Yu and
  Xiaoqiang~Zheng},
year   = {2015},
month  = jan
}


@article{tgale_shooker_2019,
author        = {Trevor Gale and
             Erich Elsen and
             Sara Hooker},
title         = {The State of Sparsity in Deep Neural Networks},
journal       = {CoRR},
volume        = {abs/1902.09574},
year          = {2019},
url           = {http://arxiv.org/abs/1902.09574},
archiveprefix = {arXiv},
eprint        = {1902.09574},
timestamp     = {Tue, 21 May 2019 18:03:40 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1902-09574},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}



@misc{thebitterlesson2019,
title  = {The Bitter Lesson},
author = {Rich Sutton},
year   = {2019},
url    = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@inproceedings{thinet,
author    = {Jian{-}Hao Luo and
             Jianxin Wu and
             Weiyao Lin},
title     = {ThiNet: {A} {F}ilter {L}evel {P}runing {M}ethod for {D}eep {N}eural {N}etwork {C}ompression},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {5068--5076},
year      = {2017}
}



@misc{Thompson2018TheDO,
title  = {The Decline of Computers As a General Purpose Technology: Why Deep Learning and the End of Moore’s Law are Fragmenting Computing},
author = {Neil Thompson and Svenja Spanuth},
year   = {2018}
}


@article{tibshirani1996regression,
title     = {Regression shrinkage and selection via the lasso},
author    = {Tibshirani, Robert},
journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume    = {58},
number    = {1},
pages     = {267--288},
year      = {1996},
publisher = {Wiley Online Library}
}

@book{tolstoy2016anna,
title     = {Anna Karenina},
author    = {Tolstoy, L. and Bartlett, R.},
isbn      = {9780198748847},
lccn      = {2015943753},
series    = {Oxford world's classics},
url       = {https://books.google.com/books?id=1DooDwAAQBAJ},
year      = {2016},
publisher = {Oxford University Press}
}

@misc{Torch2002,
author = {Collobert, Ronan and Bengio, Samy and Marithoz, Johnny},
year   = {2002},
month  = {11},
title  = {Torch: A Modular Machine Learning Software Library}
}

@inproceedings{transformer,
author    = {Ashish Vaswani and
             Noam Shazeer and
             Niki Parmar and
             Jakob Uszkoreit and
             Llion Jones and
             Aidan N. Gomez and
             Lukasz Kaiser and
             Illia Polosukhin},
title     = {{A}ttention is {A}ll you {N}eed},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {6000--6010},
year      = {2017}
}


@article{Tulving2002,
author  = {Tulving, Endel},
title   = {Episodic Memory: From Mind to Brain},
journal = {Annual Review of Psychology},
volume  = {53},
number  = {1},
pages   = {1-25},
year    = {2002},
doi     = {10.1146/annurev.psych.53.100901.135114},
note    = {PMID: 11752477},
url     = { 
      https://doi.org/10.1146/annurev.psych.53.100901.135114
  
},
eprint  = { 
      https://doi.org/10.1146/annurev.psych.53.100901.135114
  
}
}


@inproceedings{variational-dropout,
author    = {Dmitry Molchanov and
             Arsenii Ashukha and
             Dmitry P. Vetrov},
title     = {Variational {D}ropout {S}parsifies {D}eep {N}eural {N}etworks},
booktitle = {Proceedings of the 34th International Conference on Machine Learning,
             {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
pages     = {2498--2507},
year      = {2017}
}



@article{variational-dropout-local-reparameterization,
author  = {Diederik P. Kingma and
             Tim Salimans and
             Max Welling},
title   = {Variational Dropout and the Local Reparameterization Trick},
journal = {CoRR},
volume  = {abs/1506.02557},
year    = {2015}
}



@article{variational-information-bottleneck,
author  = {Bin Dai and
             Chen Zhu and
             David P. Wipf},
title   = {Compressing {N}eural {N}etworks using the {V}ariational {I}nformation {B}ottleneck},
journal = {CoRR},
volume  = {abs/1802.10399},
year    = {2018}
}

@article{Veale2017,
author  = {Michael Veale and Reuben Binns},
title   = {Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data},
journal = {Big Data \& Society},
volume  = {4},
number  = {2},
pages   = {2053951717743530},
year    = {2017},
doi     = {10.1177/2053951717743530},
url     = { 
      https://doi.org/10.1177/2053951717743530
  
},
eprint  = { 
      https://doi.org/10.1177/2053951717743530
  
}
}

@book{villani2008optimal,
title     = {Optimal Transport: Old and New},
author    = {Villani, C.},
isbn      = {9783540710509},
lccn      = {2008932183},
series    = {Grundlehren der mathematischen Wissenschaften},
url       = {https://books.google.com/books?id=hV8o5R7\_5tkC},
year      = {2008},
publisher = {Springer Berlin Heidelberg}
}

@book{von2000computer,
title     = {The Computer and the Brain},
author    = {Von Neumann, J. and Churchland, P.M. and Churchland, P.S.},
isbn      = {9780300084733},
lccn      = {00026937},
series    = {The Silliman Memorial Lectures Series},
url       = {https://books.google.com/books?id=Q30MqJjRv1gC},
year      = {2000},
publisher = {Yale University Press}
}

@inproceedings{wang2017learning,
title     = {Learning to model the tail},
author    = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
booktitle = {Advances in Neural Information Processing Systems},
pages     = {7029--7039},
year      = {2017}
}

@article{Wang2018HAQHA,
title   = {HAQ: Hardware-Aware Automated Quantization},
author  = {Kuan Wang and Zhijian Liu and Yujun Lin and Ji Lin and Song Han},
journal = {ArXiv},
year    = {2018},
volume  = {abs/1811.08886}
}


@article{wang2020picking,
title   = {Picking winning tickets before training by preserving gradient flow},
author  = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
journal = {arXiv preprint arXiv:2002.07376},
year    = {2020}
}



@article{wang2020robust,
title   = {Robust Optimization for Fairness with Noisy Protected Groups},
author  = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael I},
journal = {arXiv preprint arXiv:2002.09343},
year    = {2020}
}

@book{warden2019tinyml,
title     = {TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers},
author    = {Warden, P. and Situnayake, D.},
isbn      = {9781492052043},
url       = {https://books.google.com/books?id=sB3mxQEACAAJ},
year      = {2019},
publisher = {O'Reilly Media, Incorporated}
}

@inproceedings{wavenet,
author    = {A{\"{a}}ron van den Oord and
             Sander Dieleman and
             Heiga Zen and
             Karen Simonyan and
             Oriol Vinyals and
             Alex Graves and
             Nal Kalchbrenner and
             Andrew W. Senior and
             Koray Kavukcuoglu},
title     = {WaveNet: {A} {Generative} {Model} for {Raw} {Audio}},
booktitle = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
             September 2016},
pages     = {125},
year      = {2016}
}







@misc{welling2019,
title  = {Do we still need models or just more data and compute?},
author = {Max Welling},
year   = {2019},
url    = {shorturl.at/qABIY}
}


@incollection{werbos1982applications,
title     = {Applications of advances in nonlinear sensitivity analysis},
author    = {Werbos, Paul J},
booktitle = {System modeling and optimization},
pages     = {762--770},
year      = {1982},
publisher = {Springer}
}


@inproceedings{wide-resnet,
author    = {Sergey Zagoruyko and
             Nikos Komodakis},
title     = {Wide {R}esidual {N}etworks},
booktitle = {Proceedings of the British Machine Vision Conference 2016, {BMVC}
             2016, York, UK, September 19-22, 2016},
year      = {2016}
}



@misc{wigger2020,
title  = {OpenAI launches an API to commercialize its research},
author = {Kyle Wiggers},
year   = {2020},
url    = {https://bit.ly/31NAJQB}
}

@article{wilcoxon1945individual,
title     = {Individual Comparisons by Ranking Methods},
author    = {Wilcoxon, Frank},
journal   = {Biometrics Bulletin},
volume    = {1},
number    = {6},
pages     = {80--83},
year      = {1945},
publisher = {JSTOR}
}

@article{Wilson2000,
author     = {Wilson, D. Randall and Martinez, Tony R.},
title      = {Reduction Techniques for Instance-BasedLearning Algorithms},
journal    = {Mach. Learn.},
issue_date = {March 2000},
volume     = {38},
number     = {3},
month      = mar,
year       = {2000},
issn       = {0885-6125},
pages      = {257--286},
numpages   = {30},
url        = {https://doi.org/10.1023/A:1007626913721},
doi        = {10.1023/A:1007626913721},
acmid      = {343200},
publisher  = {Kluwer Academic Publishers},
address    = {Hingham, MA, USA},
keywords   = {classification, instance reduction, instance-based learning, nearest neighbor, pruning}
}


@article{xiao2017fashion,
title   = {Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
author  = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
journal = {arXiv preprint arXiv:1708.07747},
year    = {2017}
}




@article{xiao2018dynamical,
title   = {Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks},
author  = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
journal = {arXiv preprint arXiv:1806.05393},
year    = {2018}
}

@article{XIE2019109,
title   = {Automated pulmonary nodule detection in CT images using deep convolutional neural networks},
journal = {Pattern Recognition},
volume  = {85},
pages   = {109 - 119},
year    = {2019},
issn    = {0031-3203},
doi     = {https://doi.org/10.1016/j.patcog.2018.07.031},
url     = {http://www.sciencedirect.com/science/article/pii/S0031320318302711},
author  = {Hongtao Xie and Dongbao Yang and Nannan Sun and Zhineng Chen and Yongdong Zhang}
}

@inproceedings{xie2019exploring,
title     = {Exploring randomly wired neural networks for image recognition},
author    = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
pages     = {1284--1293},
year      = {2019}
}


@misc{Xu2010,
author  = {Xu, Harry and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Sevitsky, Gary},
year    = {2010},
month   = {01},
pages   = {421-426},
title   = {Software bloat analysis: Finding, removing, and preventing performance problems in modern large-scale object-oriented applications},
journal = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
doi     = {10.1145/1882362.1882448}
}

@article{yang2019mean,
title   = {A mean field theory of batch normalization},
author  = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
journal = {arXiv preprint arXiv:1902.08129},
year    = {2019}
}}

@article{Zador2019ACO,
title   = {A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains},
author  = {Anthony M. Zador},
journal = {bioRxiv},
year    = {2019}
}

@article{Zagoruyko2016,
author        = {Sergey Zagoruyko and
             Nikos Komodakis},
title         = {Wide Residual Networks},
journal       = {CoRR},
volume        = {abs/1605.07146},
year          = {2016},
url           = {http://arxiv.org/abs/1605.07146},
archiveprefix = {arXiv},
eprint        = {1605.07146},
timestamp     = {Mon, 13 Aug 2018 16:46:42 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/ZagoruykoK16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{zaheer2018adaptive,
title     = {Adaptive methods for nonconvex optimization},
author    = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Advances in neural information processing systems},
pages     = {9793--9803},
year      = {2018}
}

@article{zech2018,
author    = {Zech, John R. AND Badgeley, Marcus A. AND Liu, Manway AND Costa, Anthony B. AND Titano, Joseph J. AND Oermann, Eric Karl},
journal   = {PLOS Medicine},
publisher = {Public Library of Science},
title     = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study},
year      = {2018},
month     = {11},
volume    = {15},
url       = {https://doi.org/10.1371/journal.pmed.1002683},
pages     = {1-17},
abstract  = {Eric Oermann and colleagues ask whether a DL-based model for pneumonia detection performs well in external validation and consider the effects of hospital system–specific biases.},
number    = {11},
doi       = {10.1371/journal.pmed.1002683}
}

@article{zeiler2012adadelta,
title   = {Adadelta: an adaptive learning rate method},
author  = {Zeiler, Matthew D},
journal = {arXiv preprint arXiv:1212.5701},
year    = {2012}
}


@inproceedings{Zeiler2014,
author        = {Zeiler, Matthew D and Fergus, Rob},
booktitle     = {European Conference on Computer Vision},
date-added    = {2016-10-12 11:14:28 +0000},
date-modified = {2016-10-12 11:16:57 +0000},
organization  = {Springer},
pages         = {818--833},
title         = {Visualizing and understanding convolutional networks},
year          = 2014
}



@incollection{ZHANG1992,
title     = {Selecting Typical Instances in Instance-Based Learning},
editor    = {Derek Sleeman and Peter Edwards},
booktitle = {Machine Learning Proceedings 1992},
publisher = {Morgan Kaufmann},
address   = {San Francisco (CA)},
pages     = {470 - 479},
year      = {1992},
isbn      = {978-1-55860-247-2},
doi       = {https://doi.org/10.1016/B978-1-55860-247-2.50066-8},
url       = {http://www.sciencedirect.com/science/article/pii/B9781558602472500668},
author    = {Jianping Zhang},
abstract  = {Concepts involved in real world applications usually possess graded structures. Instead of being equivalent, instances of a concept may be characterized by a degree of typicality in representing the concept. Typical instances of a concept usually better characterize the concept than atypical instances do. This paper presents an instance-based learning approach in which typical instances are selected to store as concept descriptions. It first addresses the issue of measuring the typicality of an instance with respect to its concept. Then, it empirically shows that some concepts in standard datasets do have graded structures. Finally, it presents a simple instance-based learning and classification algorithm that successfully uses typicalities of instances. This approach has been tested on both artificial and practical domains, and compared with three different IBL approaches. The experimental results showed that the approach recorded lower storage requirements and higher classification accuracies than previous instance-based algorithms on several domains.}
}



@article{zhang2016,
author        = {{Zhang}, J. and {Lin}, Z. and {Brandt}, J. and {Shen}, X. and 
{Sclaroff}, S.},
title         = {{Top-down Neural Attention by Excitation Backprop}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1608.00507},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2016,
month         = aug,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160800507Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{zhang2016understanding,
title   = {Understanding deep learning requires rethinking generalization},
author  = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
journal = {arXiv preprint arXiv:1611.03530},
year    = {2016}
}

@article{zhang2019dive,
title   = {Dive into deep learning},
author  = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
journal = {Unpublished Draft. Retrieved},
volume  = {19},
pages   = {2019},
year    = {2019}
}

@inproceedings{zhao2018bridging,
title     = {Bridging the gap between deep learning and sparse matrix format selection},
author    = {Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng},
booktitle = {Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
pages     = {94--108},
year      = {2018}
}}

@article{Zhu2014,
author = {Zhu, Xiangxin and Anguelov, Dragomir and Ramanan, Deva},
year   = {2014},
month  = {09},
pages  = {915-922},
title  = {Capturing Long-Tail Distributions of Object Subcategories},
doi    = {10.1109/CVPR.2014.122}
}


@inproceedings{transformer,
author    = {Ashish Vaswani and
             Noam Shazeer and
             Niki Parmar and
             Jakob Uszkoreit and
             Llion Jones and
             Aidan N. Gomez and
             Lukasz Kaiser and
             Illia Polosukhin},
title     = {{A}ttention is {A}ll you {N}eed},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {6000--6010},
year      = {2017},
}


@inproceedings{wavenet,
author    = {A{\"{a}}ron van den Oord and
             Sander Dieleman and
             Heiga Zen and
             Karen Simonyan and
             Oriol Vinyals and
             Alex Graves and
             Nal Kalchbrenner and
             Andrew W. Senior and
             Koray Kavukcuoglu},
title     = {WaveNet: {A} {Generative} {Model} for {Raw} {Audio}},
booktitle = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
             September 2016},
pages     = {125},
year      = {2016},
}

@article{deep-learning-scaling,
author    = {Joel Hestness and
             Sharan Narang and
             Newsha Ardalani and
             Gregory F. Diamos and
             Heewoo Jun and
             Hassan Kianinejad and
             Md. Mostofa Ali Patwary and
             Yang Yang and
             Yanqi Zhou},
title     = {Deep Learning Scaling is Predictable, Empirically},
journal   = {CoRR},
volume    = {abs/1712.00409},
year      = {2017}
}



@article{lasso,
  author = {Robert Tibshirani},
  title = {Regression {S}hrinkage and {S}election {V}ia the {L}asso},
  journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
  year = {1994},
  volume = {58},
  pages = {267--288}
}

@article{dropout-journal,
author    = {Nitish Srivastava and
             Geoffrey E. Hinton and
             Alex Krizhevsky and
             Ilya Sutskever and
             Ruslan Salakhutdinov},
title     = {Dropout: a simple way to prevent neural networks from overfitting},
journal   = {Journal of Machine Learning Research},
volume    = {15},
number    = {1},
pages     = {1929--1958},
year      = {2014},
}

@article{l0-regularization,
author    = {Christos Louizos and
             Max Welling and
             Diederik P. Kingma},
title     = {Learning {S}parse {N}eural {N}etworks {t}hrough {L}\({}_{\mbox{0}}\) {R}egularization},
journal   = {CoRR},
volume    = {abs/1712.01312},
year      = {2017}
}

@misc{blocksparse-gpu-kernels,
title = {Block-Sparse GPU Kernels},
author = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year = {2017}
}


@article{sparse-evolutionary-training,
  author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio}, 
  journal = {Nature Communications}, 
  title = {Scalable {T}raining of {A}rtificial {N}eural {N}etworks with {A}daptive {S}parse {C}onnectivity {I}nspired by {N}etwork {S}cience}, 
  year = {2018}, 
}

@article{deep-rewiring,
author    = {Guillaume Bellec and
             David Kappel and
             Wolfgang Maass and
             Robert A. Legenstein},
title     = {Deep {R}ewiring: {T}raining {V}ery {S}parse {D}eep {N}etworks},
journal   = {CoRR},
volume    = {abs/1711.05136},
year      = {2017},
}

@article{lottery-ticket-hypothesis,
author    = {Jonathan Frankle and
             Michael Carbin},
title     = {The {L}ottery {T}icket {H}ypothesis: {T}raining {P}runed {N}eural {N}etworks},
journal   = {CoRR},
volume    = {abs/1803.03635},
year      = {2018},
url       = {http://arxiv.org/abs/1803.03635},
}

@article{rethinking-pruning,
author    = {Zhuang Liu and
             Mingjie Sun and
             Tinghui Zhou and
             Gao Huang and
             Trevor Darrell},
title     = {Rethinking the {V}alue of {N}etwork {P}runing},
journal   = {CoRR},
volume    = {abs/1810.05270},
year      = {2018}
}

@article{to-prune-or-not,
author    = {Michael Zhu and
             Suyog Gupta},
title     = {To prune, or not to prune: exploring the efficacy of pruning for model
             compression},
journal   = {CoRR},
volume    = {abs/1710.01878},
year      = {2017},
url       = {http://arxiv.org/abs/1710.01878},
}

@inproceedings{NEURIPS2020_1e14bfe2,
author = {Feldman, Vitaly and Zhang, Chiyuan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
pages = {2881--2891},
publisher = {Curran Associates, Inc.},
title = {What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation},
url = {https://proceedings.neurips.cc/paper/2020/file/1e14bfe2714193e7af5abc64ecbd6b46-Paper.pdf},
volume = {33},
year = {2020}
}



@misc{brown2020memorization,
    title={When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?}, 
    author={Gavin Brown and Mark Bun and Vitaly Feldman and Adam Smith and Kunal Talwar},
    year={2020},
    eprint={2012.06421},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{variational-information-bottleneck,
author    = {Bin Dai and
             Chen Zhu and
             David P. Wipf},
title     = {Compressing {N}eural {N}etworks using the {V}ariational {I}nformation {B}ottleneck},
journal   = {CoRR},
volume    = {abs/1802.10399},
year      = {2018}
}

@inproceedings{bayesian-compression,
author    = {Christos Louizos and
             Karen Ullrich and
             Max Welling},
title     = {Bayesian {C}ompression for {D}eep {L}earning},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {3290--3300},
year      = {2017},
}

@inproceedings{optimal-brain-damage,
author    = {Yann LeCun and
             John S. Denker and
             Sara A. Solla},
title     = {Optimal {B}rain {D}amage},
booktitle = {{NeurIPS}},
pages     = {598--605},
publisher = {Morgan Kaufmann},
year      = {1989}
}

@inproceedings{optimal-brain-surgeon,
author    = {Babak Hassibi and
             David G. Stork},
title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
booktitle = {{NeurIPS}},
pages     = {164--171},
publisher = {Morgan Kaufmann},
year      = {1992}
}

@article{pruning-convnet-nvidia,
author    = {Pavlo Molchanov and
             Stephen Tyree and
             Tero Karras and
             Timo Aila and
             Jan Kautz},
title     = {Pruning {C}onvolutional {N}eural {N}etworks for {R}esource {E}fficient {T}ransfer {L}earning},
journal   = {CoRR},
volume    = {abs/1611.06440},
year      = {2016}
}


@inproceedings{automatic-model-compression,
author    = {Yihui He and
             Ji Lin and
             Zhijian Liu and
             Hanrui Wang and
             Li{-}Jia Li and
             Song Han},
title     = {{AMC:} AutoML for Model Compression and Acceleration on Mobile Devices},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VII}},
pages     = {815--832},
year      = {2018},
}



@inproceedings{runtime-neural-pruning,
author    = {Ji Lin and
             Yongming Rao and
             Jiwen Lu and
             Jie Zhou},
title     = {Runtime Neural Pruning},
booktitle = {{NeurIPS}},
pages     = {2178--2188},
year      = {2017}
}

@article{ogueji2022,
doi = {10.48550/ARXIV.2211.02738},

url = {https://arxiv.org/abs/2211.02738},

author = {Ogueji, Kelechi and Ahia, Orevaoghene and Onilude, Gbemileke and Gehrmann, Sebastian and Hooker, Sara and Kreutzer, Julia},

keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Intriguing Properties of Compression on Multilingual Models},

publisher = {arXiv},

year = {2022},

copyright = {Creative Commons Attribution 4.0 International}
}



@misc{marion2023more,
    title={When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale}, 
    author={Max Marion and Ahmet Üstün and Luiza Pozzobon and Alex Wang and Marzieh Fadaee and Sara Hooker},
    year={2023},
    eprint={2309.04564},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{variational-dropout-local-reparameterization,
author    = {Diederik P. Kingma and
             Tim Salimans and
             Max Welling},
title     = {Variational Dropout and the Local Reparameterization Trick},
journal   = {CoRR},
volume    = {abs/1506.02557},
year      = {2015}
}

@article{autoencoding-variational-bayes,
author    = {Diederik P. Kingma and
             Max Welling},
title     = {Auto-Encoding Variational Bayes},
journal   = {CoRR},
volume    = {abs/1312.6114},
year      = {2013}
}

@inproceedings{stochastic-backpropagation,
author    = {Danilo Jimenez Rezende and
             Shakir Mohamed and
             Daan Wierstra},
title     = {Stochastic {B}ackpropagation and {A}pproximate {I}nference in {D}eep {G}enerative
             Models},
booktitle = {{ICML}},
series    = {{JMLR} Workshop and Conference Proceedings},
volume    = {32},
pages     = {1278--1286},
publisher = {JMLR.org},
year      = {2014}
}

@article{spike-and-slab,
author = { T. J.   Mitchell  and  J. J.   Beauchamp },
title = {Bayesian {V}ariable {S}election in {L}inear {R}egression},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {404},
pages = {1023-1032},
year  = {1988},
publisher = {Taylor & Francis},
}

@inproceedings{alyafeai-ahmad-2021-arabic,
  title = "{A}rabic Compact Language Modelling for Resource Limited Devices",
  author = "Alyafeai, Zaid  and
    Ahmad, Irfan",
  booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
  month = apr,
  year = "2021",
  address = "Kyiv, Ukraine (Virtual)",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2021.wanlp-1.6",
  pages = "53--59",
  abstract = "Natural language modelling has gained a lot of interest recently. The current state-of-the-art results are achieved by first training a very large language model and then fine-tuning it on multiple tasks. However, there is little work on smaller more compact language models for resource-limited devices or applications. Not to mention, how to efficiently train such models for a low-resource language like Arabic. In this paper, we investigate how such models can be trained in a compact way for Arabic. We also show how distillation and quantization can be applied to create even smaller models. Our experiments show that our largest model which is 2x smaller than the baseline can achieve better results on multiple tasks with 2x less data for pretraining.",
}


@misc{webster2021measuring,
    title={Measuring and Reducing Gendered Correlations in Pre-trained Models}, 
    author={Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Ed Chi and Slav Petrov},
    year={2021},
    eprint={2010.06032},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{alyafeai-ahmad-2021-arabic,
  title = "{A}rabic Compact Language Modelling for Resource Limited Devices",
  author = "Alyafeai, Zaid  and
    Ahmad, Irfan",
  booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
  month = apr,
  year = "2021",
  address = "Kyiv, Ukraine (Virtual)",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2021.wanlp-1.6",
  pages = "53--59",
  abstract = "Natural language modelling has gained a lot of interest recently. The current state-of-the-art results are achieved by first training a very large language model and then fine-tuning it on multiple tasks. However, there is little work on smaller more compact language models for resource-limited devices or applications. Not to mention, how to efficiently train such models for a low-resource language like Arabic. In this paper, we investigate how such models can be trained in a compact way for Arabic. We also show how distillation and quantization can be applied to create even smaller models. Our experiments show that our largest model which is 2x smaller than the baseline can achieve better results on multiple tasks with 2x less data for pretraining.",
}

@inproceedings{scaling-nmt,
author    = {Myle Ott and
             Sergey Edunov and
             David Grangier and
             Michael Auli},
title     = {Scaling {N}eural {M}achine {T}ranslation},
booktitle = {Proceedings of the Third Conference on Machine Translation: Research
             Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1, 2018},
pages     = {1--9},
year      = {2018},
}

@article{adam-optimizer,
author    = {Diederik P. Kingma and
             Jimmy Ba},
title     = {Adam: {A} {M}ethod for {S}tochastic {O}ptimization},
journal   = {CoRR},
volume    = {abs/1412.6980},
year      = {2014},
url       = {http://arxiv.org/abs/1412.6980},
}

@inproceedings{network-slimming,
author    = {Zhuang Liu and
             Jianguo Li and
             Zhiqiang Shen and
             Gao Huang and
             Shoumeng Yan and
             Changshui Zhang},
title     = {Learning {E}fficient {C}onvolutional {N}etworks through {N}etwork {S}limming},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {2755--2763},
year      = {2017},
}

@article{Xu2020DynamicCL,
title={Dynamic Curriculum Learning for Low-Resource Neural Machine Translation},
author={Chen Xu and Bojie Hu and Yu-fan Jiang and Kai Feng and Zeyang Wang and Shen Huang and Q. Ju and Tong Xiao and Jingbo Zhu},
journal={ArXiv},
year={2020},
volume={abs/2011.14608}
}

@inproceedings{Duh2020BenchmarkingNA,
title={Benchmarking Neural and Statistical Machine Translation on Low-Resource African Languages},
author={Kevin Duh and P. McNamee and Matt Post and Brian Thompson},
booktitle={LREC},
year={2020}
}

@inproceedings{murray-chiang-2015-auto,
  title = "Auto-Sizing Neural Networks: With Applications to n-gram Language Models",
  author = "Murray, Kenton  and
    Chiang, David",
  booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
  month = sep,
  year = "2015",
  address = "Lisbon, Portugal",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/D15-1107",
  doi = "10.18653/v1/D15-1107",
  pages = "908--916",
}





@inproceedings{araabi-monz-2020-optimizing,
  title = "Optimizing Transformer for Low-Resource Neural Machine Translation",
  author = "Araabi, Ali  and
    Monz, Christof",
  booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
  month = dec,
  year = "2020",
  address = "Barcelona, Spain (Online)",
  publisher = "International Committee on Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2020.coling-main.304",
  doi = "10.18653/v1/2020.coling-main.304",
  pages = "3429--3435",
  abstract = "Language pairs with limited amounts of parallel data, also known as low-resource languages, remain a challenge for neural machine translation. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of Transformer under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.",
}

@inproceedings{thinet,
author    = {Jian{-}Hao Luo and
             Jianxin Wu and
             Weiyao Lin},
title     = {ThiNet: {A} {F}ilter {L}evel {P}runing {M}ethod for {D}eep {N}eural {N}etwork {C}ompression},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {5068--5076},
year      = {2017},
}

@inproceedings{sennrich-zhang-2019-revisiting,
  title = "Revisiting Low-Resource Neural Machine Translation: A Case Study",
  author = "Sennrich, Rico  and
    Zhang, Biao",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/P19-1021",
  doi = "10.18653/v1/P19-1021",
  pages = "211--221",
  abstract = "It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German{--}English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean{--}English dataset, surpassing previously reported results by 4 BLEU.",
}

@misc{vanbiljon2020optimal,
    title={On Optimal Transformer Depth for Low-Resource Language Translation}, 
    author={Elan van Biljon and Arnu Pretorius and Julia Kreutzer},
    year={2020},
    eprint={2004.04418},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{murray2019autosizing,
    title={Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation}, 
    author={Kenton Murray and Jeffery Kinnison and Toan Q. Nguyen and Walter Scheirer and David Chiang},
    year={2019},
    eprint={1910.06717},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{fan2019reducing,
    title={Reducing Transformer Depth on Demand with Structured Dropout}, 
    author={Angela Fan and Edouard Grave and Armand Joulin},
    year={2019},
    eprint={1909.11556},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{wide-resnet,
author    = {Sergey Zagoruyko and
             Nikos Komodakis},
title     = {Wide {R}esidual {N}etworks},
booktitle = {Proceedings of the British Machine Vision Conference 2016, {BMVC}
             2016, York, UK, September 19-22, 2016},
year      = {2016},
}

@inproceedings{sparse-connection-1997,
author    = {Nikko Str\"om},
title     = {Sparse {C}onnection and {P}runing in {L}arge {D}ynamic {A}rtificial {N}eural {N}etworks},
booktitle = {EUROSPEECH},
year      = {1997},
}

@book{zipf1999psycho,
title={The Psycho-Biology of Language: An Introduction to Dynamic Philology},
author={Zipf, G.K.},
isbn={9780415209762},
series={Cognitive psychology]},
url={https://books.google.com/books?id=w1Z4Aq-5sWMC},
year={1999},
publisher={Routledge}
}



@misc{søgaard2021need,
    title={We Need to Talk About Random Splits}, 
    author={Anders Søgaard and Sebastian Ebert and Jasmijn Bastings and Katja Filippova},
    year={2021},
    eprint={2005.00636},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{fbnet,
author    = {Bichen Wu and
             Xiaoliang Dai and
             Peizhao Zhang and
             Yanghan Wang and
             Fei Sun and
             Yiming Wu and
             Yuandong Tian and
             Peter Vajda and
             Yangqing Jia and
             Kurt Keutzer},
title     = {FBNet: {H}ardware-{A}ware {E}fficient {C}onv{N}et {D}esign via {D}ifferentiable
             Neural Architecture Search},
journal   = {CoRR},
volume    = {abs/1812.03443},
year      = {2018},
url       = {http://arxiv.org/abs/1812.03443},
}

@article{mobilenetv2,
author    = {Mark Sandler and
             Andrew G. Howard and
             Menglong Zhu and
             Andrey Zhmoginov and
             Liang{-}Chieh Chen},
title     = {{I}nverted {R}esiduals and {L}inear {B}ottlenecks: {M}obile {N}etworks for {C}lassification,
             Detection and Segmentation},
journal   = {CoRR},
volume    = {abs/1801.04381},
year      = {2018}
}


@misc{kuwanto2021lowresource,
    title={Low-Resource Machine Translation for Low-Resource Languages: Leveraging Comparable Data, Code-Switching and Compute Resources}, 
    author={Garry Kuwanto and Afra Feyza Akyürek and Isidora Chara Tourni and Siyang Li and Derry Wijaya},
    year={2021},
    eprint={2103.13272},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{massiveWild,
author    = {Naveen Arivazhagan and
             Ankur Bapna and
             Orhan Firat and
             Dmitry Lepikhin and
             Melvin Johnson and
             Maxim Krikun and
             Mia Xu Chen and
             Yuan Cao and
             George F. Foster and
             Colin Cherry and
             Wolfgang Macherey and
             Zhifeng Chen and
             Yonghui Wu},
title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
             and Challenges},
journal   = {CoRR},
volume    = {abs/1907.05019},
year      = {2019},
url       = {http://arxiv.org/abs/1907.05019},
archivePrefix = {arXiv},
eprint    = {1907.05019},
timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1907-05019.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{curiousHallucinations,
author    = {Vikas Raunak and
             Arul Menezes and
             Marcin Junczys{-}Dowmunt},
title     = {The Curious Case of Hallucinations in Neural Machine Translation},
journal   = {CoRR},
volume    = {abs/2104.06683},
year      = {2021},
url       = {https://arxiv.org/abs/2104.06683},
archivePrefix = {arXiv},
eprint    = {2104.06683},
timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2104-06683.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{2016arXiv161002136H,
     author = {{Hendrycks}, Dan and {Gimpel}, Kevin},
      title = "{A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
       year = 2016,
      month = oct,
        eid = {arXiv:1610.02136},
      pages = {arXiv:1610.02136},
archivePrefix = {arXiv},
     eprint = {1610.02136},
primaryClass = {cs.NE},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161002136H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@incollection{NeurIPS2016_6300,
title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2280--2288},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.NeurIPS.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf}
}


@ARTICLE{2020blalock,
     author = {{Blalock}, Davis and {Gonzalez Ortiz}, Jose Javier and
       {Frankle}, Jonathan and {Guttag}, John},
      title = "{What is the State of Neural Network Pruning?}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = 2020,
      month = mar,
        eid = {arXiv:2003.03033},
      pages = {arXiv:2003.03033},
archivePrefix = {arXiv},
     eprint = {2003.03033},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200303033B},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Zhu2014,
author = {Zhu, Xiangxin and Anguelov, Dragomir and Ramanan, Deva},
year = {2014},
month = {09},
pages = {915-922},
title = {Capturing Long-Tail Distributions of Object Subcategories},
doi = {10.1109/CVPR.2014.122}
}

@article{zhang2016understanding,
title={Understanding deep learning requires rethinking generalization},
author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
journal={arXiv preprint arXiv:1611.03530},
year={2016}
}

@inproceedings{Papineni2002,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}



@ARTICLE{2021arXiv210201670T,
     author = {{Tessera}, Kale-ab and {Hooker}, Sara and {Rosman}, Benjamin},
      title = "{Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
       year = 2021,
      month = feb,
        eid = {arXiv:2102.01670},
      pages = {arXiv:2102.01670},
archivePrefix = {arXiv},
     eprint = {2102.01670},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210201670T},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2019arXiv190608158K,
     author = {{Kirsch}, Andreas and {van Amersfoort}, Joost and {Gal}, Yarin},
      title = "{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = "2019",
      month = "Jun",
        eid = {arXiv:1906.08158},
      pages = {arXiv:1906.08158},
archivePrefix = {arXiv},
     eprint = {1906.08158},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608158K},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{10.1001/archopht.116.4.502,
  author = {Daw, Nigel W.},
  title = "{Critical Periods and Amblyopia}",
  journal = {Archives of Ophthalmology},
  volume = {116},
  number = {4},
  pages = {502-505},
  year = {1998},
  month = {04},
  abstract = "{During the past 20 years, basic science has shown that there are different critical periods for different visual functions during the development of the visual system. Visual functions processed at higher anatomical levels within the system have a later critical period than functions processed at lower levels. This general principle suggests that treatments for amblyopia should be followed in a logical sequence, with treatment for each visual function to be started before its critical period is over. However, critical periods for some visual functions, such as stereopsis, are not yet fully determined, and the optimal treatment is, therefore, unknown. This article summarizes the current extent of our knowledge and points to the gaps that need to be filled.Arch Ophthalmol. 1998;116:502-505-->}",
  issn = {0003-9950},
  doi = {10.1001/archopht.116.4.502},
  url = {https://doi.org/10.1001/archopht.116.4.502},
  eprint = {https://jamanetwork.com/journals/jamaophthalmology/articlepdf/262202/emo7751.pdf},
}


@inproceedings{LiuLWT15,
added-at = {2018-10-09T00:00:00.000+0200},
author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
booktitle = {ICCV},
crossref = {conf/iccv/2015},
ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
interhash = {3f735aaa11957e73914bbe2ca9d5e702},
intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},
isbn = {978-1-4673-8391-2},
keywords = {dblp},
pages = {3730-3738},
publisher = {IEEE Computer Society},
timestamp = {2018-10-11T11:43:28.000+0200},
title = {Deep Learning Face Attributes in the Wild.},
url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
year = 2015
}

@article{Bien_2011,
 title={Prototype selection for interpretable classification},
 volume={5},
 ISSN={1932-6157},
 url={http://dx.doi.org/10.1214/11-AOAS495},
 DOI={10.1214/11-aoas495},
 number={4},
 journal={The Annals of Applied Statistics},
 publisher={Institute of Mathematical Statistics},
 author={Bien, Jacob and Tibshirani, Robert},
 year={2011},
 month={Dec},
 pages={2403–2424}
}

@misc{cotter2018twoplayer,
  title={Two-Player Games for Efficient Non-Convex Constrained Optimization},
  author={Andrew Cotter and Heinrich Jiang and Karthik Sridharan},
  year={2018},
  eprint={1804.06500},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


@InProceedings{pmlrbuolamwini18a,
title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
author = 	 {Joy Buolamwini and Timnit Gebru},
booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
pages = 	 {77--91},
year = 	 {2018},
editor = 	 {Sorelle A. Friedler and Christo Wilson},
volume = 	 {81},
series = 	 {Proceedings of Machine Learning Research},
address = 	 {New York, NY, USA},
month = 	 {23--24 Feb},
publisher = 	 {PMLR},
url = 	 {http://proceedings.mlr.press/v81/buolamwini18a.html},
}
@article{Recht2019,
author    = {Benjamin Recht and
             Rebecca Roelofs and
             Ludwig Schmidt and
             Vaishaal Shankar},
title     = {Do ImageNet Classifiers Generalize to ImageNet?},
journal   = {CoRR},
volume    = {abs/1902.10811},
year      = {2019},
url       = {http://arxiv.org/abs/1902.10811},
archivePrefix = {arXiv},
eprint    = {1902.10811},
timestamp = {Tue, 21 May 2019 18:03:38 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-10811},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kleinberg2016inherent,
title={Inherent trade-offs in the fair determination of risk scores},
author={Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
journal={arXiv preprint arXiv:1609.05807},
year={2016}
}

@inproceedings{feldman2015certifying,
title={Certifying and removing disparate impact},
author={Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
booktitle={proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
pages={259--268},
year={2015}
}

@article{wang2020robust,
title={Robust Optimization for Fairness with Noisy Protected Groups},
author={Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael I},
journal={arXiv preprint arXiv:2002.09343},
year={2020}
}
@article{gupta2018proxy,
title={Proxy fairness},
author={Gupta, Maya and Cotter, Andrew and Fard, Mahdi Milani and Wang, Serena},
journal={arXiv preprint arXiv:1806.11212},
year={2018}
}

@article{kallus2019assessing,
title={Assessing algorithmic fairness with unobserved protected class using data combination},
author={Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
journal={arXiv preprint arXiv:1906.00285},
year={2019}
}


@inproceedings{madaan2018analyze,
title={Analyze, detect and remove gender stereotyping from bollywood movies},
author={Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
booktitle={Conference on Fairness, Accountability and Transparency},
pages={92--105},
year={2018}
}


@inproceedings{kay2015unequal,
title={Unequal representation and gender stereotypes in image search results for occupations},
author={Kay, Matthew and Matuszek, Cynthia and Munson, Sean A},
booktitle={Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages={3819--3828},
year={2015}
}

@article{amodei2016,
author    = {Dario Amodei and
             Chris Olah and
             Jacob Steinhardt and
             Paul F. Christiano and
             John Schulman and
             Dan Man{\'{e}}},
title     = {Concrete Problems in {AI} Safety},
journal   = {CoRR},
volume    = {abs/1606.06565},
year      = {2016},
url       = {http://arxiv.org/abs/1606.06565},
archivePrefix = {arXiv},
eprint    = {1606.06565},
timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/AmodeiOSCSM16},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{2019arXiv190405160L,
     author = {{Liu}, Ziwei and {Miao}, Zhongqi and {Zhan}, Xiaohang and
       {Wang}, Jiayun and {Gong}, Boqing and {Yu}, Stella X.},
      title = "{Large-Scale Long-Tailed Recognition in an Open World}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
       year = 2019,
      month = apr,
        eid = {arXiv:1904.05160},
      pages = {arXiv:1904.05160},
archivePrefix = {arXiv},
     eprint = {1904.05160},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190405160L},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{false_discovery_rates,
author = {Ridgeway, Greg and MacDonald, John},
year = {2009},
month = {06},
pages = {661-668},
title = {Doubly Robust Internal Benchmarking and False Discovery Rates for Detecting Racial Bias in Police Stops},
volume = {104},
journal = {Journal of the American Statistical Association},
doi = {10.1198/jasa.2009.0034}
}

@article{feldman2019does,
title={Does learning require memorization? A short tale about a long tail},
author={Feldman, Vitaly},
journal={arXiv preprint arXiv:1906.05271},
year={2019}
}

@ARTICLE{2014certifying_removing_disparate_impact,
     author = {{Feldman}, Michael and {Friedler}, Sorelle and {Moeller}, John and
       {Scheidegger}, Carlos and {Venkatasubramanian}, Suresh},
      title = "{Certifying and removing disparate impact}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Machine Learning, Computer Science - Computers and Society},
       year = "2014",
      month = "Dec",
        eid = {arXiv:1412.3756},
      pages = {arXiv:1412.3756},
archivePrefix = {arXiv},
     eprint = {1412.3756},
primaryClass = {stat.ML},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3756F},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{ridgeway2009,
title={Doubly robust internal benchmarking and false discovery rates for detecting racial bias in police stops},
author={Ridgeway, Greg and MacDonald, John M},
journal={Journal of the American Statistical Association},
volume={104},
number={486},
pages={661--668},
year={2009},
publisher={Taylor \& Francis}
}

Your .bib file should contain, for example,

@manual{Harwell:2019,
title  = "A face-scanning algorithm increasingly decides whether you deserve the job",
author = "Drew Harwell",
url    = "https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/",
year   = "2019 (accessed May 19, 2020)"
}

@article{XIE2019109,
title = "Automated pulmonary nodule detection in CT images using deep convolutional neural networks",
journal = "Pattern Recognition",
volume = "85",
pages = "109 - 119",
year = "2019",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2018.07.031",
url = "http://www.sciencedirect.com/science/article/pii/S0031320318302711",
author = "Hongtao Xie and Dongbao Yang and Nannan Sun and Zhineng Chen and Yongdong Zhang"
}

@incollection{NeurIPS2016_6316,
title = {Satisfying Real-world Goals with Dataset Constraints},
author = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2415--2423},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.NeurIPS.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf}
}


@ARTICLE{2019arXiv190110566Z,
     author = {{Zink}, Anna and {Rose}, Sherri},
      title = "{Fair Regression for Health Care Spending}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Methodology, Statistics - Machine Learning},
       year = "2019",
      month = "Jan",
        eid = {arXiv:1901.10566},
      pages = {arXiv:1901.10566},
archivePrefix = {arXiv},
     eprint = {1901.10566},
primaryClass = {stat.AP},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190110566Z},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016fair_prediction,
     author = {{Chouldechova}, Alexandra},
      title = "{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Machine Learning},
       year = "2016",
      month = "Oct",
        eid = {arXiv:1610.07524},
      pages = {arXiv:1610.07524},
archivePrefix = {arXiv},
     eprint = {1610.07524},
primaryClass = {stat.AP},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161007524C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{du2022glam,
    title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}, 
    author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
    year={2022},
    eprint={2112.06905},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{fedus2022switch,
    title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
    author={William Fedus and Barret Zoph and Noam Shazeer},
    year={2022},
    eprint={2101.03961},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{riquelme2021scaling,
title={Scaling vision with sparse mixture of experts},
author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
journal={Advances in Neural Information Processing Systems},
volume={34},
pages={8583--8595},
year={2021}
}

@misc{shazeer2018meshtensorflow,
    title={Mesh-TensorFlow: Deep Learning for Supercomputers}, 
    author={Noam Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake Hechtman},
    year={2018},
    eprint={1811.02084},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{steiger2021psychological,
title        = {The psychological well-being of content moderators: the emotional labor of commercial moderation and avenues for improving support},
author       = {Steiger, Miriah and Bharucha, Timir J and Venkatagiri, Sukrit and Riedl, Martin J and Lease, Matthew},
year         = 2021,
booktitle    = {Proceedings of the 2021 CHI conference on human factors in computing systems},
pages        = {1--14}
}

@inproceedings{NIPS2016Cortes,
author = {Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems},
editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Boosting with Abstention},
url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
volume = {29},
year = {2016}
}


@misc{OPT-zhang2022,
    title={OPT: Open Pre-trained Transformer Language Models}, 
    author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
    year={2022},
    eprint={2205.01068},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{pu-etal-2021-learning,
  title = "Learning Compact Metrics for {MT}",
  author = "Pu, Amy  and
    Chung, Hyung Won  and
    Parikh, Ankur  and
    Gehrmann, Sebastian  and
    Sellam, Thibault",
  editor = "Moens, Marie-Francine  and
    Huang, Xuanjing  and
    Specia, Lucia  and
    Yih, Scott Wen-tau",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.58",
  doi = "10.18653/v1/2021.emnlp-main.58",
  pages = "751--762",
  abstract = "Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity with RemBERT, a state-of-the-art multilingual language model, using data from the WMT Metrics Shared Task. We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages. Our method yields up to 10.5{%} improvement over vanilla fine-tuning and reaches 92.6{%} of RemBERT{'}s performance using only a third of its parameters.",
}

@misc{pozzobon2023goodtriever,
    title={Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models}, 
    author={Luiza Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
    year={2023},
    eprint={2310.07589},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{duh-etal-2020-benchmarking,
  title = "Benchmarking Neural and Statistical Machine Translation on Low-Resource {A}frican Languages",
  author = "Duh, Kevin  and
    McNamee, Paul  and
    Post, Matt  and
    Thompson, Brian",
  editor = "Calzolari, Nicoletta  and
    B{\'e}chet, Fr{\'e}d{\'e}ric  and
    Blache, Philippe  and
    Choukri, Khalid  and
    Cieri, Christopher  and
    Declerck, Thierry  and
    Goggi, Sara  and
    Isahara, Hitoshi  and
    Maegaard, Bente  and
    Mariani, Joseph  and
    Mazo, H{\'e}l{\`e}ne  and
    Moreno, Asuncion  and
    Odijk, Jan  and
    Piperidis, Stelios",
  booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
  month = may,
  year = "2020",
  address = "Marseille, France",
  publisher = "European Language Resources Association",
  url = "https://aclanthology.org/2020.lrec-1.325",
  pages = "2667--2675",
  abstract = "Research in machine translation (MT) is developing at a rapid pace. However, most work in the community has focused on languages where large amounts of digital resources are available. In this study, we benchmark state of the art statistical and neural machine translation systems on two African languages which do not have large amounts of resources: Somali and Swahili. These languages are of social importance and serve as test-beds for developing technologies that perform reasonably well despite the low-resource constraint. Our findings suggest that statistical machine translation (SMT) and neural machine translation (NMT) can perform similarly in low-resource scenarios, but neural systems require more careful tuning to match performance. We also investigate how to exploit additional data, such as bilingual text harvested from the web, or user dictionaries; we find that NMT can significantly improve in performance with the use of these additional data. Finally, we survey the landscape of machine translation resources for the languages of Africa and provide some suggestions for promising future research directions.",
  language = "English",
  ISBN = "979-10-95546-34-4",
}


@inproceedings{chung-etal-2020-extremely,
  title = "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation",
  author = "Chung, Insoo  and
    Kim, Byeongwook  and
    Choi, Yoonjung  and
    Kwon, Se Jung  and
    Jeon, Yongkweon  and
    Park, Baeseong  and
    Kim, Sangha  and
    Lee, Dongsoo",
  editor = "Cohn, Trevor  and
    He, Yulan  and
    Liu, Yang",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.findings-emnlp.433",
  doi = "10.18653/v1/2020.findings-emnlp.433",
  pages = "4812--4826",
  abstract = "The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8{\mbox{$\times$}} smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3{\mbox{$\times$}} reduction in run-time memory footprints and 3.5{\mbox{$\times$}} speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.",
}

@article{bordley2014,
author = {Robert F. Bordley},
title = {Reference Class Forecasting: Resolving Its Challenge to Statistical Modeling},
journal = {The American Statistician},
volume = {68},
number = {4},
pages = {221--229},
year = {2014},
publisher = {Taylor \& Francis},
doi = {10.1080/00031305.2014.937544},


URL = { 
  
      https://doi.org/10.1080/00031305.2014.937544
  
  

},
eprint = { 
  
      https://doi.org/10.1080/00031305.2014.937544
}

}


@article{dehghani2021,
author       = {Mostafa Dehghani and
                Anurag Arnab and
                Lucas Beyer and
                Ashish Vaswani and
                Yi Tay},
title        = {The Efficiency Misnomer},
journal      = {CoRR},
volume       = {abs/2110.12894},
year         = {2021},
url          = {https://arxiv.org/abs/2110.12894},
eprinttype    = {arXiv},
eprint       = {2110.12894},
timestamp    = {Thu, 28 Oct 2021 15:25:31 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2110-12894.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Stevenson2018,
author = {{Megan}, Stevenson},
title = "{Assessing Risk Assessment in Action}",
journal = {Minnesota Law Review},
year = "2018",
  volume = {58},
  url = "https://scholarship.law.umn.edu/mlr/58"}

@ARTICLE{2017Gurumoorthy,
     author = {{Gurumoorthy}, Karthik S. and {Dhurandhar}, Amit and
       {Cecchi}, Guillermo and {Aggarwal}, Charu},
      title = "{Efficient Data Representation by Selecting Prototypes with Importance Weights}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, 65K05, 68W25},
       year = "2017",
      month = "Jul",
        eid = {arXiv:1707.01212},
      pages = {arXiv:1707.01212},
archivePrefix = {arXiv},
     eprint = {1707.01212},
primaryClass = {stat.ML},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170701212G},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Guo2016,
author    = {Yiwen Guo and
             Anbang Yao and
             Yurong Chen},
title     = {Dynamic Network Surgery for Efficient DNNs},
journal   = {CoRR},
volume    = {abs/1608.04493},
year      = {2016},
url       = {http://arxiv.org/abs/1608.04493},
archivePrefix = {arXiv},
eprint    = {1608.04493},
timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/GuoYC16},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{CollinsK14,
author    = {Maxwell D. Collins and
             Pushmeet Kohli},
title     = {Memory Bounded Deep Convolutional Networks},
journal   = {CoRR},
volume    = {abs/1412.1442},
year      = {2014},
url       = {http://arxiv.org/abs/1412.1442},
archivePrefix = {arXiv},
eprint    = {1412.1442},
timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/CollinsK14},
bibsource = {dblp computer science bibliography, https://dblp.org}
}




@ARTICLE{2017Huang,
     author = {{Huang}, Gao and {Liu}, Shichen and {van der Maaten}, Laurens and
       {Weinberger}, Kilian Q.},
      title = "{CondenseNet: An Efficient DenseNet using Learned Group Convolutions}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
       year = "2017",
      month = "Nov",
        eid = {arXiv:1711.09224},
      pages = {arXiv:1711.09224},
archivePrefix = {arXiv},
     eprint = {1711.09224},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109224H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@inproceedings{Lakshminarayanan2017,
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
series = {NeurIPS'17},
year = {2017},
isbn = {978-1-5108-6096-4},
location = {Long Beach, California, USA},
pages = {6405--6416},
numpages = {12},
url = {http://dl.acm.org/citation.cfm?id=3295222.3295387},
acmid = {3295387},
publisher = {Curran Associates Inc.},
address = {USA},
} 



@inproceedings{Fumera2002,
author = {Fumera, Giorgio and Roli, Fabio},
title = {Support Vector Machines with Embedded Reject Option},
booktitle = {Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines},
series = {SVM '02},
year = {2002},
isbn = {3-540-44016-X},
pages = {68--82},
numpages = {15},
url = {http://dl.acm.org/citation.cfm?id=647230.719259},
acmid = {719259},
publisher = {Springer-Verlag},
address = {London, UK, UK},
} 
[download]

@ARTICLE{2016alexey,
     author = {{Kurakin}, Alexey and {Goodfellow}, Ian and {Bengio}, Samy},
      title = "{Adversarial examples in the physical world}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
       year = "2016",
      month = "Jul",
        eid = {arXiv:1607.02533},
      pages = {arXiv:1607.02533},
archivePrefix = {arXiv},
     eprint = {1607.02533},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160702533K},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2018Wang,
     author = {{Wang}, Tianyang and {Huan}, Jun and {Li}, Bo},
      title = "{Data Dropout: Optimizing Training Data for Convolutional Neural Networks}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
       year = "2018",
      month = "Sep",
        eid = {arXiv:1809.00193},
      pages = {arXiv:1809.00193},
archivePrefix = {arXiv},
     eprint = {1809.00193},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180900193W},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{NeurIPS2016_6300,
title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2280--2288},
year = {2016},
publisher = {Curran Associates, Inc.},
}


@inproceedings{Bengio2009,
author = {Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum Learning},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
series = {ICML '09},
year = {2009},
isbn = {978-1-60558-516-1},
location = {Montreal, Quebec, Canada},
pages = {41--48},
numpages = {8},
url = {http://doi.acm.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
acmid = {1553380},
publisher = {ACM},
address = {New York, NY, USA},
} 
[download]

@inproceedings{
lee2018training,
title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},
author={Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryiAv2xAZ},
}

@inproceedings{
liang2018enhancing,
title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
author={Shiyu Liang and Yixuan Li and R. Srikant},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1VGkIxRZ},
}

@ARTICLE{2017Lee,
     author = {{Lee}, Kimin and {Lee}, Honglak and {Lee}, Kibok and {Shin}, Jinwoo},
      title = "{Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
       year = "2017",
      month = "Nov",
        eid = {arXiv:1711.09325},
      pages = {arXiv:1711.09325},
archivePrefix = {arXiv},
     eprint = {1711.09325},
primaryClass = {stat.ML},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109325L},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





@ARTICLE{2016amodei,
     author = {{Amodei}, Dario and {Olah}, Chris and {Steinhardt}, Jacob and
       {Christiano}, Paul and {Schulman}, John and {Man{\'e}}, Dan},
      title = "{Concrete Problems in AI Safety}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
       year = "2016",
      month = "Jun",
        eid = {arXiv:1606.06565},
      pages = {arXiv:1606.06565},
archivePrefix = {arXiv},
     eprint = {1606.06565},
primaryClass = {cs.AI},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160606565A},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018Vodrahalli,
     author = {{Vodrahalli}, Kailas and {Li}, Ke and {Malik}, Jitendra},
      title = "{Are All Training Examples Created Equal? An Empirical Study}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
       year = "2018",
      month = "Nov",
        eid = {arXiv:1811.12569},
      pages = {arXiv:1811.12569},
archivePrefix = {arXiv},
     eprint = {1811.12569},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181112569V},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2014Nguyen,
     author = {{Nguyen}, Anh and {Yosinski}, Jason and {Clune}, Jeff},
      title = "{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
       year = "2014",
      month = "Dec",
        eid = {arXiv:1412.1897},
      pages = {arXiv:1412.1897},
archivePrefix = {arXiv},
     eprint = {1412.1897},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.1897N},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018Hendrycks,
     author = {{Hendrycks}, Dan and {Dietterich}, Thomas G.},
      title = "{Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
       year = "2018",
      month = "Jul",
        eid = {arXiv:1807.01697},
      pages = {arXiv:1807.01697},
archivePrefix = {arXiv},
     eprint = {1807.01697},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180701697H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016Szegedy,
     author = {{Szegedy}, Christian and {Ioffe}, Sergey and {Vanhoucke}, Vincent and
       {Alemi}, Alex},
      title = "{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
       year = "2016",
      month = "Feb",
        eid = {arXiv:1602.07261},
      pages = {arXiv:1602.07261},
archivePrefix = {arXiv},
     eprint = {1602.07261},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207261S},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2015Szegedy,
     author = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and
       {Shlens}, Jonathon and {Wojna}, Zbigniew},
      title = "{Rethinking the Inception Architecture for Computer Vision}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
       year = "2015",
      month = "Dec",
        eid = {arXiv:1512.00567},
      pages = {arXiv:1512.00567},
archivePrefix = {arXiv},
     eprint = {1512.00567},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Kornblith2018DoBI,
title={Do Better ImageNet Models Transfer Better?},
author={Simon Kornblith and Jonathon Shlens and Quoc V. Le},
journal={ArXiv},
year={2018},
volume={abs/1805.08974}
}



@ARTICLE{2016Zhang,
     author = {{Zhang}, Chiyuan and {Bengio}, Samy and {Hardt}, Moritz and
       {Recht}, Benjamin and {Vinyals}, Oriol},
      title = "{Understanding deep learning requires rethinking generalization}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning},
       year = "2016",
      month = "Nov",
        eid = {arXiv:1611.03530},
      pages = {arXiv:1611.03530},
archivePrefix = {arXiv},
     eprint = {1611.03530},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161103530Z},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@Book{kipling1899,
author = { Kipling, Rudyard},
title = { American notes / by Rudyard Kipling },
publisher = { Brown and Company Boston },
pages = { 1 online resource (137 pages, 1 unnumbered leaf of plates) : },
year = { 1899 },
type = { Book, Online },
language = { English },
subjects = { United States -- Description and travel.; United States -- Social life and customs -- 1865-1918. },
life-dates = { 1899 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn6847688 },
}

@inproceedings{rn50,
author    = {Kaiming He and
             Xiangyu Zhang and
             Shaoqing Ren and
             Jian Sun},
title     = {{D}eep {R}esidual {L}earning for {I}mage {R}ecognition},
booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
             {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
pages     = {770--778},
year      = {2016},
}

@inproceedings{transformer,
author    = {Ashish Vaswani and
             Noam Shazeer and
             Niki Parmar and
             Jakob Uszkoreit and
             Llion Jones and
             Aidan N. Gomez and
             Lukasz Kaiser and
             Illia Polosukhin},
title     = {{A}ttention is {A}ll you {N}eed},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {6000--6010},
year      = {2017},
}



@inproceedings{wavenet,
author    = {A{\"{a}}ron van den Oord and
             Sander Dieleman and
             Heiga Zen and
             Karen Simonyan and
             Oriol Vinyals and
             Alex Graves and
             Nal Kalchbrenner and
             Andrew W. Senior and
             Koray Kavukcuoglu},
title     = {WaveNet: {A} {Generative} {Model} for {Raw} {Audio}},
booktitle = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
             September 2016},
pages     = {125},
year      = {2016},
}

@article{deep-learning-scaling,
author    = {Joel Hestness and
             Sharan Narang and
             Newsha Ardalani and
             Gregory F. Diamos and
             Heewoo Jun and
             Hassan Kianinejad and
             Md. Mostofa Ali Patwary and
             Yang Yang and
             Yanqi Zhou},
title     = {Deep Learning Scaling is Predictable, Empirically},
journal   = {CoRR},
volume    = {abs/1712.00409},
year      = {2017}
}


@article{lasso,
  author = {Robert Tibshirani},
  title = {Regression {S}hrinkage and {S}election {V}ia the {L}asso},
  journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
  year = {1994},
  volume = {58},
  pages = {267--288}
}


@article{dropout-journal,
author    = {Nitish Srivastava and
             Geoffrey E. Hinton and
             Alex Krizhevsky and
             Ilya Sutskever and
             Ruslan Salakhutdinov},
title     = {Dropout: a simple way to prevent neural networks from overfitting},
journal   = {Journal of Machine Learning Research},
volume    = {15},
number    = {1},
pages     = {1929--1958},
year      = {2014},
}

@article{l0-regularization,
author    = {Christos Louizos and
             Max Welling and
             Diederik P. Kingma},
title     = {Learning {S}parse {N}eural {N}etworks {t}hrough {L}\({}_{\mbox{0}}\) {R}egularization},
journal   = {CoRR},
volume    = {abs/1712.01312},
year      = {2017}
}

@ARTICLE{2016Lakshminarayanan,
     author = {{Lakshminarayanan}, Balaji and {Pritzel}, Alexander and
       {Blundell}, Charles},
      title = "{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
       year = "2016",
      month = "Dec",
        eid = {arXiv:1612.01474},
      pages = {arXiv:1612.01474},
archivePrefix = {arXiv},
     eprint = {1612.01474},
primaryClass = {stat.ML},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161201474L},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019arXiv190802900M,
     author = {{Mwebaze}, Ernest and {Gebru}, Timnit and {Frome}, Andrea and
       {Nsumba}, Solomon and {Tusubira}, Jeremy},
      title = "{iCassava 2019Fine-Grained Visual Categorization Challenge}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition},
       year = "2019",
      month = "Aug",
        eid = {arXiv:1908.02900},
      pages = {arXiv:1908.02900},
archivePrefix = {arXiv},
     eprint = {1908.02900},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190802900M},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





@inproceedings{Zeiler2014,
Author = {Zeiler, Matthew D and Fergus, Rob},
Booktitle = {European Conference on Computer Vision},
Date-Added = {2016-10-12 11:14:28 +0000},
Date-Modified = {2016-10-12 11:16:57 +0000},
Organization = {Springer},
Pages = {818--833},
Title = {Visualizing and understanding convolutional networks},
Year = 2014,
}

@article{Montavon2011,
Author = {Montavon, Gregoire and Braun, Mikio L and M{\"u}ller, Klaus-Robert},
Date-Added = {2016-10-12 11:13:21 +0000},
Date-Modified = {2016-10-19 16:36:27 +0000},
Journal = {Journal of Machine Learning Research},
Number = {Sep},
Pages = {2563--2581},
Title = {Kernel analysis of deep networks},
Volume = {12},
Year = 2011,
}

@article{Bach2015,
Author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
Date-Added = {2016-10-12 11:12:26 +0000},
Date-Modified = {2016-10-12 11:16:21 +0000},
Journal = {PloS one},
Number = {7},
Pages = {e0130140},
Publisher = {Public Library of Science},
Title = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
Volume = {10},
Year = 2015,
}

@article{Montavon2017,
Author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
Date-Modified = {2017-02-10 22:17:33 +0000},
Journal = {Pattern Recognition},
Pages = {211--222},
Publisher = {Elsevier},
Title = {Explaining nonlinear classification decisions with deep taylor decomposition},
Volume = {65},
year = 2017,
}

@article{RossFinale2017,
author    = {Andrew Slavin Ross and
             Finale Doshi{-}Velez},
title     = {Improving the Adversarial Robustness and Interpretability of Deep
             Neural Networks by Regularizing their Input Gradients},
journal   = {CoRR},
volume    = {abs/1711.09404},
year      = 2017,
}

@inproceedings{Fong2017,
author    = {Ruth C. Fong and
             Andrea Vedaldi},
title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
booktitle = {{ICCV}},
pages     = {3449--3457},
publisher = {{IEEE} Computer Society},
year      = 2017,
}

@InProceedings{gradcam2017,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = 2017,
}



@ARTICLE{singledirection2018,
 author = {{Morcos}, A.~S. and {Barrett}, D.~G.~T. and {Rabinowitz}, N.~C. and 
{Botvinick}, M.},
  title = "{On the importance of single directions for generalization}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1803.06959},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
   year = 2018,
  month = mar,
 adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180306959M},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@ARTICLE{kitty_paper2017,
 author = {{Kindermans}, P.-J. and {Hooker}, S. and {Adebayo}, J. and {Alber}, M. and 
{Sch{\"u}tt}, K.~T. and {D{\"a}hne}, S. and {Erhan}, D. and 
{Kim}, B.},
  title = "{The (Un)reliability of saliency methods}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1711.00867},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Learning},
   year = 2017,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100867K},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@ARTICLE{zhang2016,
 author = {{Zhang}, J. and {Lin}, Z. and {Brandt}, J. and {Shen}, X. and 
{Sclaroff}, S.},
  title = "{Top-down Neural Attention by Excitation Backprop}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1608.00507},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2016,
  month = aug,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160800507Z},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@inproceedings{Raghu2017,
author    = {Maithra Raghu and
             Justin Gilmer and
             Jason Yosinski and
             Jascha Sohl{-}Dickstein},
title     = {{SVCCA:} Singular Vector Canonical Correlation Analysis for Deep Learning
             Dynamics and Interpretability},
booktitle = {{NeurIPS}},
pages     = {6078--6087},
year      = 2017,
}

@ARTICLE{Hughes2017,
 author = {{Wu}, M. and {Hughes}, M.~C. and {Parbhoo}, S. and {Zazzi}, M. and 
{Roth}, V. and {Doshi-Velez}, F.},
  title = "{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1711.06178},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Learning},
   year = 2017,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171106178W},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}


@ARTICLE{papernot2015,
 author = {{Papernot}, N. and {McDaniel}, P. and {Jha}, S. and {Fredrikson}, M. and 
{Berkay Celik}, Z. and {Swami}, A.},
  title = "{The Limitations of Deep Learning in Adversarial Settings}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1511.07528},
primaryClass = "cs.CR",
keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
   year = 2015,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151107528P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}


@article{olah2017feature,
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
title = {Feature Visualization},
journal = {Distill},
year = 2017,
note = {https://distill.pub/2017/feature-visualization},
doi = {10.23915/distill.00007},
}

@ARTICLE{samek2017, 
author={W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. Müller},
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title="{Evaluating the Visualization of What a Deep Neural Network Has Learned}", 
year=2017, 
month={Nov},
volume={28}, 
number={11}, 
pages={2660-2673}, 
keywords={data visualisation;image classification;learning (artificial intelligence);neural nets;DNN;ILSVRC2012;MIT Places data sets;SUN397;complex machine learning tasks;data visualization;deconvolution method;deep neural network;heatmap;multilayer nonlinear structure;relevance propagation algorithm;sensitivity-based approach;Algorithm design and analysis;Biological neural networks;Deconvolution;Heating;Learning systems;Neurons;Sensitivity;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models}, 
doi={10.1109/TNNLS.2016.2599820}, 
ISSN={2162-237X},
}


@InProceedings{koh2017,
title = 	 {Understanding Black-box Predictions via Influence Functions},
author = 	 {Pang Wei Koh and Percy Liang},
booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
pages = 	 {1885--1894},
year = 	 {2017},
editor = 	 {Doina Precup and Yee Whye Teh},
volume = 	 {70},
series = 	 {Proceedings of Machine Learning Research},
address = 	 {International Convention Centre, Sydney, Australia},
month = 	 {06--11 Aug},
publisher = 	 {PMLR},
pdf = 	 {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf}
}

@ARTICLE{Goodman2016,
 author = {{Goodman}, B. and {Flaxman}, S.},
  title = "{European Union regulations on algorithmic decision-making and a ``right to explanation''}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1606.08813},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Learning},
   year = 2016,
  month = jun,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160608813G},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}}


@ARTICLE{kim2017,
 author = {{Kim}, B. and {Wattenberg}, M. and {Gilmer}, J. and {Cai}, C. and 
{Wexler}, J. and {Viegas}, F. and {Sayres}, R.},
  title = "{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1711.11279},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning},
   year = 2017,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111279K},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}}

@article{LRP2016,
author    = {Avanti Shrikumar and
             Peyton Greenside and
             Anna Shcherbina and
             Anshul Kundaje},
title     = {Not Just a Black Box: Learning Important Features Through Propagating
             Activation Differences},
journal   = {CoRR},
volume    = {abs/1605.01713},
year      = 2016,
url       = {http://arxiv.org/abs/1605.01713},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShrikumarGSK16},
bibsource = {dblp computer science bibliography, http://dblp.org}}


@inproceedings{Zeiler2014,
Author = {Zeiler, Matthew D and Fergus, Rob},
Booktitle = {European Conference on Computer Vision},
Date-Added = {2016-10-12 11:14:28 +0000},
Date-Modified = {2016-10-12 11:16:57 +0000},
Organization = {Springer},
Pages = {818--833},
Title = {Visualizing and understanding convolutional networks},
Year = 2014,
}

@article{Montavon2011,
Author = {Montavon, Gregoire and Braun, Mikio L and M{\"u}ller, Klaus-Robert},
Date-Added = {2016-10-12 11:13:21 +0000},
Date-Modified = {2016-10-19 16:36:27 +0000},
Journal = {Journal of Machine Learning Research},
Number = {Sep},
Pages = {2563--2581},
Title = {Kernel analysis of deep networks},
Volume = {12},
Year = 2011,
}

@article{Bach2015,
Author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
Date-Added = {2016-10-12 11:12:26 +0000},
Date-Modified = {2016-10-12 11:16:21 +0000},
Journal = {PloS one},
Number = {7},
Pages = {e0130140},
Publisher = {Public Library of Science},
Title = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
Volume = {10},
Year = 2015,
}

@article{Montavon2017,
Author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
Date-Modified = {2017-02-10 22:17:33 +0000},
Journal = {Pattern Recognition},
Pages = {211--222},
Publisher = {Elsevier},
Title = {Explaining nonlinear classification decisions with deep taylor decomposition},
Volume = {65},
year = 2017,
}

@article{RossFinale2017,
author    = {Andrew Slavin Ross and
             Finale Doshi{-}Velez},
title     = {Improving the Adversarial Robustness and Interpretability of Deep
             Neural Networks by Regularizing their Input Gradients},
journal   = {CoRR},
volume    = {abs/1711.09404},
year      = 2017,
}

@inproceedings{Fong2017,
author    = {Ruth C. Fong and
             Andrea Vedaldi},
title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
booktitle = {{ICCV}},
pages     = {3449--3457},
publisher = {{IEEE} Computer Society},
year      = 2017,
}

@InProceedings{gradcam2017,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = 2017,
}



@ARTICLE{kitty_paper2017,
 author = {{Kindermans}, P.-J. and {Hooker}, S. and {Adebayo}, J. and {Alber}, M. and 
{Sch{\"u}tt}, K.~T. and {D{\"a}hne}, S. and {Erhan}, D. and 
{Kim}, B.},
  title = "{The (Un)reliability of saliency methods}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1711.00867},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Learning},
   year = 2017,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100867K},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@ARTICLE{zhang2016,
 author = {{Zhang}, J. and {Lin}, Z. and {Brandt}, J. and {Shen}, X. and 
{Sclaroff}, S.},
  title = "{Top-down Neural Attention by Excitation Backprop}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1608.00507},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2016,
  month = aug,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160800507Z},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}


@ARTICLE{papernot2015,
 author = {{Papernot}, N. and {McDaniel}, P. and {Jha}, S. and {Fredrikson}, M. and 
{Berkay Celik}, Z. and {Swami}, A.},
  title = "{The Limitations of Deep Learning in Adversarial Settings}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1511.07528},
primaryClass = "cs.CR",
keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
   year = 2015,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151107528P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}


@ARTICLE{samek2017, 
author={W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. Müller},
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title="{Evaluating the Visualization of What a Deep Neural Network Has Learned}", 
year=2017, 
month={Nov},
volume={28}, 
number={11}, 
pages={2660-2673}, 
keywords={data visualisation;image classification;learning (artificial intelligence);neural nets;DNN;ILSVRC2012;MIT Places data sets;SUN397;complex machine learning tasks;data visualization;deconvolution method;deep neural network;heatmap;multilayer nonlinear structure;relevance propagation algorithm;sensitivity-based approach;Algorithm design and analysis;Biological neural networks;Deconvolution;Heating;Learning systems;Neurons;Sensitivity;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models}, 
doi={10.1109/TNNLS.2016.2599820}, 
ISSN={2162-237X},
}


@ARTICLE{Goodman2016,
 author = {{Goodman}, B. and {Flaxman}, S.},
  title = "{European Union regulations on algorithmic decision-making and a ``right to explanation''}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1606.08813},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Learning},
   year = 2016,
  month = jun,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160608813G},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}}

@article{LRP2016,
author    = {Avanti Shrikumar and
             Peyton Greenside and
             Anna Shcherbina and
             Anshul Kundaje},
title     = {Not Just a Black Box: Learning Important Features Through Propagating
             Activation Differences},
journal   = {CoRR},
volume    = {abs/1605.01713},
year      = 2016,
url       = {http://arxiv.org/abs/1605.01713},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShrikumarGSK16},
bibsource = {dblp computer science bibliography, http://dblp.org}}

@ARTICLE{1971Naturdistance_sets,
 author = {{Levandowsky}, M.},
  title = "{Distance between Sets}",
journal = {\nat},
   year = 1971,
  month = nov,
 volume = 234,
  pages = {34-35},
    doi = {10.1038/234034a0},
 adsurl = {https://ui.adsabs.harvard.edu/abs/1971Natur.234...34L},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Gruetzemacher20183DDL,
title={3D deep learning for detecting pulmonary nodules in CT scans},
author={Ross Gruetzemacher and Ashish Gupta and David B. Paradice},
journal={Journal of the American Medical Informatics Association : JAMIA},
year={2018},
volume={25 10},
pages={
        1301-1310
      }
}

@INPROCEEDINGS{Kubat97addressingthe,
  author = {Miroslav Kubat and Stan Matwin},
  title = {Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
  booktitle = {In Proceedings of the Fourteenth International Conference on Machine Learning},
  year = {1997},
  pages = {179--186},
  publisher = {Morgan Kaufmann}
}

@inproceedings{CameronJones1995InstanceSB,
title={Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing},
author={R. Mike Cameron-Jones},
year={1995}
}

@Article{Aha1991,
author="Aha, David W.
and Kibler, Dennis
and Albert, Marc K.",
title="Instance-based learning algorithms",
journal="Machine Learning",
year="1991",
month="Jan",
day="01",
volume="6",
number="1",
pages="37--66",
url="https://doi.org/10.1007/BF00153759"
}

@misc{karpathy2014,
author="Karpathy, Andrej",
year="2014",
title = {What I learned from competing against a ConvNet on ImageNet},
howpublished = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}},
note = {Accessed: 2019-07-07}
}

@inproceedings{hardt2016equality,
title={Equality of opportunity in supervised learning},
author={Hardt, Moritz and Price, Eric and Srebro, Nati},
booktitle={Advances in neural information processing systems},
pages={3315--3323},
year={2016}
}


@article{chouldechova2017fair,
title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
author={Chouldechova, Alexandra},
journal={Big data},
volume={5},
number={2},
pages={153--163},
year={2017},
publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@ARTICLE{2016Zhang,
     author = {{Zhang}, Chiyuan and {Bengio}, Samy and {Hardt}, Moritz and
       {Recht}, Benjamin and {Vinyals}, Oriol},
      title = "{Understanding deep learning requires rethinking generalization}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning},
       year = "2016",
      month = "Nov",
        eid = {arXiv:1611.03530},
      pages = {arXiv:1611.03530},
archivePrefix = {arXiv},
     eprint = {1611.03530},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161103530Z},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018Nalisnick,
     author = {{Nalisnick}, Eric and {Matsukawa}, Akihiro and {Whye Teh}, Yee and
       {Gorur}, Dilan and {Lakshminarayanan}, Balaji},
      title = "{Do Deep Generative Models Know What They Don't Know?}",
    journal = {arXiv e-prints},
   keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
       year = "2018",
      month = "Oct",
        eid = {arXiv:1810.09136},
      pages = {arXiv:1810.09136},
archivePrefix = {arXiv},
     eprint = {1810.09136},
primaryClass = {stat.ML},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009136N},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017jo,
     author = {{Jo}, Jason and {Bengio}, Yoshua},
      title = "{Measuring the tendency of CNNs to Learn Surface Statistical Regularities}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = "2017",
      month = "Nov",
        eid = {arXiv:1711.11561},
      pages = {arXiv:1711.11561},
archivePrefix = {arXiv},
     eprint = {1711.11561},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111561J},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2017hosseini,
     author = {{Hosseini}, Hossein and {Xiao}, Baicen and {Jaiswal}, Mayoore and
       {Poovendran}, Radha},
      title = "{On the Limitation of Convolutional Neural Networks in Recognizing Negative Images}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
       year = "2017",
      month = "Mar",
        eid = {arXiv:1703.06857},
      pages = {arXiv:1703.06857},
archivePrefix = {arXiv},
     eprint = {1703.06857},
primaryClass = {cs.CV},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170306857H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{Russakovsky2015,
author="Russakovsky, Olga
and Deng, Jia
and Su, Hao
and Krause, Jonathan
and Satheesh, Sanjeev
and Ma, Sean
and Huang, Zhiheng
and Karpathy, Andrej
and Khosla, Aditya
and Bernstein, Michael
and Berg, Alexander C.
and Fei-Fei, Li",
title="ImageNet Large Scale Visual Recognition Challenge",
journal="International Journal of Computer Vision",
year="2015",
month="Dec",
day="01",
volume="115",
number="3",
pages="211--252",
abstract="The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",
issn="1573-1405",
doi="10.1007/s11263-015-0816-y",
url="https://doi.org/10.1007/s11263-015-0816-y"
}

@ARTICLE{2017Kearns,
     author = {{Kearns}, Michael and {Neel}, Seth and {Roth}, Aaron and
       {Wu}, Zhiwei Steven},
      title = "{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}",
   keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computer Science and Game Theory},
       year = 2017,
      month = nov,
}

@inproceedings{mccoy_etal_2019_right,
  title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
  author = "McCoy, Tom  and
    Pavlick, Ellie  and
    Linzen, Tal",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/P19-1334",
  doi = "10.18653/v1/P19-1334",
  pages = "3428--3448",
  abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}

@misc{keras_pruning,
author = {{Keras TensorFlow}},
title = {Keras Tensorflow Magnitude Pruning Open Source Code},
howpublished = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}},
note = {Accessed: 2019-07-10}
}

@article{Aha1992ToleratingNI,
title={Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms},
author={David W. Aha},
journal={International Journal of Man-Machine Studies},
year={1992},
volume={36},
pages={267-287}
}


@article{domingo_1995,
author = {Domingos, Pedro},
year = {1995},
month = {05},
pages = {},
title = {Rule Induction and Instance-Based Learning: A Unified Approach}
}

@InProceedings{papadimitriou1980,
author="Papadimitriou, Christos H.
and Bentley, Jon Louis",
editor="de Bakker, Jaco
and van Leeuwen, Jan",
title="A worst-case analysis of nearest neighbor searching by projection",
booktitle="Automata, Languages and Programming",
year="1980",
}

@article{DBLP:journals/corr/abs-1903-04561,
author    = {Daniel Borkan and
             Lucas Dixon and
             Jeffrey Sorensen and
             Nithum Thain and
             Lucy Vasserman},
title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text
             Classification},
journal   = {CoRR},
volume    = {abs/1903.04561},
year      = {2019},
url       = {http://arxiv.org/abs/1903.04561},
archivePrefix = {arXiv},
eprint    = {1903.04561},
timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Aha_1989,
author = {Aha, David W. and Kibler, Dennis},
title = {Noise-tolerant Instance-based Learning Algorithms},
booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
series = {IJCAI'89},
year = {1989},
location = {Detroit, Michigan},
pages = {794--799},
numpages = {6},
url = {http://dl.acm.org/citation.cfm?id=1623755.1623881},
acmid = {1623881},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
} 

@misc{blocksparse-gpu-kernels,
title = {Block-Sparse GPU Kernels},
author = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year = {2017}
}

@incollection{NeurIPS2016Cortes,
title = {Boosting with Abstention},
author = {Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1660--1668},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.NeurIPS.cc/paper/6336-boosting-with-abstention.pdf}
}

@inproceedings{Corinna2016,
title={Learning with Rejection},
author={Corinna Cortes and Giulia DeSalvo and Mehryar Mohri},
booktitle={ALT},
year={2016}
}

@inproceedings{wang2017learning,
title={Learning to model the tail},
author={Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
booktitle={Advances in Neural Information Processing Systems},
pages={7029--7039},
year={2017}
}

@ARTICLE{2017benoit,
     author = {{Jacob}, Benoit and {Kligys}, Skirmantas and {Chen}, Bo and
       {Zhu}, Menglong and {Tang}, Matthew and {Howard}, Andrew and
       {Adam}, Hartwig and {Kalenichenko}, Dmitry},
      title = "{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = 2017,
      month = dec,
        eid = {arXiv:1712.05877},
      pages = {arXiv:1712.05877},
archivePrefix = {arXiv},
     eprint = {1712.05877},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Veale2017,
author = {Michael Veale and Reuben Binns},
title ={Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data},
journal = {Big Data \& Society},
volume = {4},
number = {2},
pages = {2053951717743530},
year = {2017},
doi = {10.1177/2053951717743530},

URL = { 
      https://doi.org/10.1177/2053951717743530
  
},
eprint = { 
      https://doi.org/10.1177/2053951717743530
  
}
}



@article{2018_sparse_evolutionary_training,
  author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio}, 
  journal = {Nature Communications}, 
  title = {Scalable {T}raining of {A}rtificial {N}eural {N}etworks with {A}daptive {S}parse {C}onnectivity {I}nspired by {N}etwork {S}cience}, 
  year = {2018}, 
}

@article{deep-rewiring,
author    = {Guillaume Bellec and
             David Kappel and
             Wolfgang Maass and
             Robert A. Legenstein},
title     = {Deep {R}ewiring: {T}raining {V}ery {S}parse {D}eep {N}etworks},
journal   = {CoRR},
volume    = {abs/1711.05136},
year      = {2017},
}

@article{lottery-ticket-hypothesis,
author    = {Jonathan Frankle and
             Michael Carbin},
title     = {The {L}ottery {T}icket {H}ypothesis: {T}raining {P}runed {N}eural {N}etworks},
journal   = {CoRR},
volume    = {abs/1803.03635},
year      = {2018},
url       = {http://arxiv.org/abs/1803.03635},
}

@article{rethinking-pruning,
author    = {Zhuang Liu and
             Mingjie Sun and
             Tinghui Zhou and
             Gao Huang and
             Trevor Darrell},
title     = {Rethinking the {V}alue of {N}etwork {P}runing},
journal   = {CoRR},
volume    = {abs/1810.05270},
year      = {2018}
}

@article{memory-bounded-convnet,
author    = {Maxwell D. Collins and
             Pushmeet Kohli},
title     = {Memory {B}ounded {D}eep {C}onvolutional {N}etworks},
journal   = {CoRR},
volume    = {abs/1412.1442},
year      = {2014},
url       = {http://arxiv.org/abs/1412.1442},
}

@inproceedings{variational-dropout,
author    = {Dmitry Molchanov and
             Arsenii Ashukha and
             Dmitry P. Vetrov},
title     = {Variational {D}ropout {S}parsifies {D}eep {N}eural {N}etworks},
booktitle = {Proceedings of the 34th International Conference on Machine Learning,
             {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
pages     = {2498--2507},
year      = {2017},
}

@article{variational-information-bottleneck,
author    = {Bin Dai and
             Chen Zhu and
             David P. Wipf},
title     = {Compressing {N}eural {N}etworks using the {V}ariational {I}nformation {B}ottleneck},
journal   = {CoRR},
volume    = {abs/1802.10399},
year      = {2018}
}

@ARTICLE{2017Cortes,
     author = {{Cortes}, Corinna and {DeSalvo}, Giulia and {Gentile}, Claudio and
       {Mohri}, Mehryar and {Yang}, Scott},
      title = "{Online Learning with Abstention}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning},
       year = "2017",
      month = "Mar",
        eid = {arXiv:1703.03478},
      pages = {arXiv:1703.03478},
archivePrefix = {arXiv},
     eprint = {1703.03478},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303478C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{bayesian-compression,
author    = {Christos Louizos and
             Karen Ullrich and
             Max Welling},
title     = {Bayesian {C}ompression for {D}eep {L}earning},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {3290--3300},
year      = {2017},
}

@inproceedings{optimal-brain-damage,
author    = {Yann LeCun and
             John S. Denker and
             Sara A. Solla},
title     = {Optimal {B}rain {D}amage},
booktitle = {{NeurIPS}},
pages     = {598--605},
publisher = {Morgan Kaufmann},
year      = {1989}
}

@inproceedings{optimal-brain-surgeon,
author    = {Babak Hassibi and
             David G. Stork},
title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
booktitle = {{NeurIPS}},
pages     = {164--171},
publisher = {Morgan Kaufmann},
year      = {1992}
}

@article{pruning-convnet-nvidia,
author    = {Pavlo Molchanov and
             Stephen Tyree and
             Tero Karras and
             Timo Aila and
             Jan Kautz},
title     = {Pruning {C}onvolutional {N}eural {N}etworks for {R}esource {E}fficient {T}ransfer {L}earning},
journal   = {CoRR},
volume    = {abs/1611.06440},
year      = {2016}
}

@inproceedings{automatic-model-compression,
author    = {Yihui He and
             Ji Lin and
             Zhijian Liu and
             Hanrui Wang and
             Li{-}Jia Li and
             Song Han},
title     = {{AMC:} AutoML for Model Compression and Acceleration on Mobile Devices},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VII}},
pages     = {815--832},
year      = {2018},
}


@article{2018Mittal,
author    = {Deepak Mittal and
             Shweta Bhardwaj and
             Mitesh M. Khapra and
             Balaraman Ravindran},
title     = {Recovering from Random Pruning: On the Plasticity of Deep Convolutional
             Neural Networks},
journal   = {CoRR},
volume    = {abs/1801.10447},
year      = {2018},
url       = {http://arxiv.org/abs/1801.10447},
archivePrefix = {arXiv},
eprint    = {1801.10447},
timestamp = {Mon, 13 Aug 2018 16:46:51 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-10447},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ciresan2011,
author    = {Dan C. Ciresan and
             Ueli Meier and
             Jonathan Masci and
             Luca Maria Gambardella and
             J{\"{u}}rgen Schmidhuber},
title     = {High-Performance Neural Networks for Visual Object Classification},
journal   = {CoRR},
volume    = {abs/1102.0183},
year      = {2011},
url       = {http://arxiv.org/abs/1102.0183},
archivePrefix = {arXiv},
eprint    = {1102.0183},
timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1102-0183},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{stock2018,
author    = {Pierre Stock and
             Moustapha Ciss{\'{e}}},
title     = {ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and
             Uncovering Biases},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VI}},
pages     = {504--519},
year      = {2018},
crossref  = {DBLP:conf/eccv/2018-6},
url       = {https://doi.org/10.1007/978-3-030-01231-1\_31},
doi       = {10.1007/978-3-030-01231-1\_31},
timestamp = {Tue, 14 May 2019 10:00:45 +0200},
biburl    = {https://dblp.org/rec/bib/conf/eccv/StockC18},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{2019Hendrycks_Dietterich,
     author = {{Hendrycks}, Dan and {Dietterich}, Thomas},
      title = "{Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
       year = "2019",
      month = "Mar",
        eid = {arXiv:1903.12261},
      pages = {arXiv:1903.12261},
archivePrefix = {arXiv},
     eprint = {1903.12261},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312261H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{namhoon2018,
author    = {Namhoon Lee and
             Thalaiyasingam Ajanthan and
             Philip H. S. Torr},
title     = {{SNIP:} Single-shot Network Pruning based on Connection Sensitivity},
journal   = {CoRR},
volume    = {abs/1810.02340},
year      = {2018},
url       = {http://arxiv.org/abs/1810.02340},
archivePrefix = {arXiv},
eprint    = {1810.02340},
timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-02340},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{RAKIC1994227,
title = "Synaptic development of the cerebral cortex: implications for learning, memory, and mental illness",
editor = "J. Van Pelt and M.A. Corner and H.B.M. Uylings and F.H. Lopes Da Silva",
series = "Progress in Brain Research",
publisher = "Elsevier",
volume = "102",
pages = "227 - 243",
year = "1994",
booktitle = "The Self-Organizing Brain: From Growth Cones to Functional Networks",
issn = "0079-6123",
doi = "https://doi.org/10.1016/S0079-6123(08)60543-9",
url = "http://www.sciencedirect.com/science/article/pii/S0079612308605439",
author = "Pasko Rakic and Jean-Pierre Bourgeois and Patricia S. Goldman-Rakic",
abstract = "Publisher Summary
This chapter explores various questions: Are synapses added as we learn? Are there more synapses in some cortical areas than in others? Are there gender differences in synaptic density and do we lose synapses as we age? If we lose synapses with age, what is the timing and rate of this dissolution? To address these issue this chapter present the finding reported in the rhesus monkey. The study of major structural and functional subdivisions of the cortex over the primate lifespan offers a particularly comprehensive view of synapse formation. From study, it is eminently clear that knowledge of the normal course and mechanisms of synapse formation, the influence of various exogenous and endogenous events upon synapse stability and turnover, are essential prerequisites to determining the locus and timing of etiological factors in diseases that affect the cortex and alter cognitive function."
}
@ARTICLE{reed_1993_pruning_algorithms_survey, 
author={R. {Reed}}, 
journal={IEEE Transactions on Neural Networks}, 
title={Pruning algorithms-a survey}, 
year={1993}, 
volume={4}, 
number={5}, 
pages={740-747}, 
keywords={learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods}, 
doi={10.1109/72.248452}, 
ISSN={1045-9227}, 
month={Sep.},}


@article{squeezenet2018,
author    = {Forrest N. Iandola and
             Matthew W. Moskewicz and
             Khalid Ashraf and
             Song Han and
             William J. Dally and
             Kurt Keutzer},
title     = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB
             model size},
journal   = {CoRR},
volume    = {abs/1602.07360},
year      = {2016},
url       = {http://arxiv.org/abs/1602.07360},
archivePrefix = {arXiv},
eprint    = {1602.07360},
timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/IandolaMAHDK16},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shooker2019,
author    = {Sara Hooker and
             Dumitru Erhan and
             Pieter{-}Jan Kindermans and
             Been Kim},
title     = {Evaluating Feature Importance Estimates},
journal   = {CoRR},
volume    = {abs/1806.10758},
year      = {2018},
url       = {http://arxiv.org/abs/1806.10758},
archivePrefix = {arXiv},
eprint    = {1806.10758},
}

@inproceedings{runtime-neural-pruning,
author    = {Ji Lin and
             Yongming Rao and
             Jiwen Lu and
             Jie Zhou},
title     = {Runtime Neural Pruning},
booktitle = {{NeurIPS}},
pages     = {2178--2188},
year      = {2017}
}

@article{variational-dropout-local-reparameterization,
author    = {Diederik P. Kingma and
             Tim Salimans and
             Max Welling},
title     = {Variational Dropout and the Local Reparameterization Trick},
journal   = {CoRR},
volume    = {abs/1506.02557},
year      = {2015}
}

@article{autoencoding-variational-bayes,
author    = {Diederik P. Kingma and
             Max Welling},
title     = {Auto-Encoding Variational Bayes},
journal   = {CoRR},
volume    = {abs/1312.6114},
year      = {2013}
}

@inproceedings{stochastic-backpropagation,
author    = {Danilo Jimenez Rezende and
             Shakir Mohamed and
             Daan Wierstra},
title     = {Stochastic {B}ackpropagation and {A}pproximate {I}nference in {D}eep {G}enerative
             Models},
booktitle = {{ICML}},
series    = {{JMLR} Workshop and Conference Proceedings},
volume    = {32},
pages     = {1278--1286},
publisher = {JMLR.org},
year      = {2014}
}

@article{spike-and-slab,
author = { T. J.   Mitchell  and  J. J.   Beauchamp },
title = {Bayesian {V}ariable {S}election in {L}inear {R}egression},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {404},
pages = {1023-1032},
year  = {1988},
publisher = {Taylor & Francis},
}

@inproceedings{scaling-nmt,
author    = {Myle Ott and
             Sergey Edunov and
             David Grangier and
             Michael Auli},
title     = {Scaling {N}eural {M}achine {T}ranslation},
booktitle = {Proceedings of the Third Conference on Machine Translation: Research
             Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1, 2018},
pages     = {1--9},
year      = {2018},
}

@article{adam-optimizer,
author    = {Diederik P. Kingma and
             Jimmy Ba},
title     = {Adam: {A} {M}ethod for {S}tochastic {O}ptimization},
journal   = {CoRR},
volume    = {abs/1412.6980},
year      = {2014},
url       = {http://arxiv.org/abs/1412.6980},
}

@inproceedings{network-slimming,
author    = {Zhuang Liu and
             Jianguo Li and
             Zhiqiang Shen and
             Gao Huang and
             Shoumeng Yan and
             Changshui Zhang},
title     = {Learning {E}fficient {C}onvolutional {N}etworks through {N}etwork {S}limming},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {2755--2763},
year      = {2017},
}

@inproceedings{thinet,
author    = {Jian{-}Hao Luo and
             Jianxin Wu and
             Weiyao Lin},
title     = {ThiNet: {A} {F}ilter {L}evel {P}runing {M}ethod for {D}eep {N}eural {N}etwork {C}ompression},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {5068--5076},
year      = {2017},
}

@inproceedings{wide-resnet,
author    = {Sergey Zagoruyko and
             Nikos Komodakis},
title     = {Wide {R}esidual {N}etworks},
booktitle = {Proceedings of the British Machine Vision Conference 2016, {BMVC}
             2016, York, UK, September 19-22, 2016},
year      = {2016},
}



@inproceedings{sparse-connection-1997,
author    = {Nikko Str\"om},
title     = {Sparse {C}onnection and {P}runing in {L}arge {D}ynamic {A}rtificial {N}eural {N}etworks},
booktitle = {EUROSPEECH},
year      = {1997},
}

@article{fbnet,
author    = {Bichen Wu and
             Xiaoliang Dai and
             Peizhao Zhang and
             Yanghan Wang and
             Fei Sun and
             Yiming Wu and
             Yuandong Tian and
             Peter Vajda and
             Yangqing Jia and
             Kurt Keutzer},
title     = {FBNet: {H}ardware-{A}ware {E}fficient {C}onv{N}et {D}esign via {D}ifferentiable
             Neural Architecture Search},
journal   = {CoRR},
volume    = {abs/1812.03443},
year      = {2018},
url       = {http://arxiv.org/abs/1812.03443},
}

@article{mobilenetv2,
author    = {Mark Sandler and
             Andrew G. Howard and
             Menglong Zhu and
             Andrey Zhmoginov and
             Liang{-}Chieh Chen},
title     = {{I}nverted {R}esiduals and {L}inear {B}ottlenecks: {M}obile {N}etworks for {C}lassification,
             Detection and Segmentation},
journal   = {CoRR},
volume    = {abs/1801.04381},
year      = {2018}
}



@inproceedings{langley00,
author    = {P. Langley},
title     = {Crafting Papers on Machine Learning},
year      = {2000},
pages     = {1207--1216},
editor    = {Pat Langley},
booktitle     = {Proceedings of the 17th International Conference
            on Machine Learning (ICML 2000)},
address   = {Stanford, CA},
publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
author = 	 "T. M. Mitchell",
title = 	 "The Need for Biases in Learning Generalizations",
institution =  "Computer Science Department, Rutgers University",
year = 	 "1980",
address =	 "New Brunswick, MA",
}
@inproceedings{mitchell2019model,
title={Model cards for model reporting},
author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
booktitle={Proceedings of the conference on fairness, accountability, and transparency},
pages={220--229},
year={2019}
}
@phdthesis{kearns89,
author = {M. J. Kearns},
title =  {Computational Complexity of Machine Learning},
school = {Department of Computer Science, Harvard University},
year =   {1989}
}

@ARTICLE{2018random_pruning,
 author = {{Mittal}, D. and {Bhardwaj}, S. and {Khapra}, M.~M. and {Ravindran}, B.
},
  title = "{Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1801.10447},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2018,
  month = jan,
 adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180110447M},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@Book{MachineLearningI,
editor = 	 "R. S. Michalski and J. G. Carbonell and T.
    M. Mitchell",
title = 	 "Machine Learning: An Artificial Intelligence
    Approach, Vol. I",
publisher = 	 "Tioga",
year = 	 "1983",
address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
author =       "R. O. Duda and P. E. Hart and D. G. Stork",
title =        "Pattern Classification",
publisher =    "John Wiley and Sons",
edition =      "2nd",
year =         "2000"
}

@misc{anonymous,
title= {Suppressed for Anonymity},
author= {Author, N. N.},
year= {2018}
}

@InCollection{Newell81,
author =       "A. Newell and P. S. Rosenbloom",
title =        "Mechanisms of Skill Acquisition and the Law of
                Practice", 
booktitle =    "Cognitive Skills and Their Acquisition",
pages =        "1--51",
publisher =    "Lawrence Erlbaum Associates, Inc.",
year =         "1981",
editor =       "J. R. Anderson",
chapter =      "1",
address =      "Hillsdale, NJ"
}


@Article{Samuel59,
author = 	 "A. L. Samuel",
title = 	 "Some Studies in Machine Learning Using the Game of
    Checkers",
journal =	 "IBM Journal of Research and Development",
year =	 "1959",
volume =	 "3",
number =	 "3",
pages =	 "211--229"
}


@ARTICLE{2017arXiv170604599G,
 author = {{Guo}, C. and {Pleiss}, G. and {Sun}, Y. and {Weinberger}, K.~Q.
},
  title = "{On Calibration of Modern Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1706.04599},
keywords = {Computer Science - Machine Learning},
   year = 2017,
  month = jun,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170604599G},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{lenet, 
author={Y. Lecun and L. Bottou and Y. Bengio and P. Haffner}, 
journal={Proceedings of the IEEE}, 
title={Gradient-based learning applied to document recognition}, 
year={1998}, 
volume={86}, 
number={11}, 
pages={2278-2324}, 
keywords={optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis}, 
doi={10.1109/5.726791}, 
ISSN={0018-9219}, 
month={Nov},}

@ARTICLE{2014Simonyan,
 author = {{Simonyan}, K. and {Zisserman}, A.},
  title = "{Very Deep Convolutional Networks for Large-Scale Image Recognition}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1409.1556},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2014,
  month = sep,
 adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.1556S},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{NeurIPSKingma,
title = {Variational Dropout and the Local Reparameterization Trick},
author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2575--2583},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.NeurIPS.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf}
}

@incollection{LeCun1990,
title = {Optimal Brain Damage},
author = {{LeCun}, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor = {D. S. Touretzky},
pages = {598--605},
year = {1990},
publisher = {Morgan-Kaufmann},
url = {http://papers.NeurIPS.cc/paper/250-optimal-brain-damage.pdf}
}


@ARTICLE{2017Molchanov,
 author = {{Molchanov}, D. and {Ashukha}, A. and {Vetrov}, D.},
  title = "{Variational Dropout Sparsifies Deep Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1701.05369},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
   year = 2017,
  month = jan,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016DropNeuron,
 author = {{Pan}, W. and {Dong}, H. and {Guo}, Y.},
  title = "{DropNeuron: Simplifying the Structure of Deep Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1606.07326},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
   year = 2016,
  month = jun,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160607326P},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016Molchanov,
 author = {{Molchanov}, P. and {Tyree}, S. and {Karras}, T. and {Aila}, T. and 
{Kautz}, J.},
  title = "{Pruning Convolutional Neural Networks for Resource Efficient Inference}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1611.06440},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
   year = 2016,
  month = nov,
 adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161106440M},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





@ARTICLE{2017l0_reg,
 author = {{Louizos}, C. and {Welling}, M. and {Kingma}, D.~P.},
  title = "{Learning Sparse Neural Networks through $L\_0$ Regularization}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1712.01312},
primaryClass = "stat.ML",
keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
   year = 2017,
  month = dec,
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{DBLPKornblith,
author    = {Simon Kornblith and
             Jonathon Shlens and
             Quoc V. Le},
title     = {Do Better ImageNet Models Transfer Better?},
journal   = {CoRR},
volume    = {abs/1805.08974},
year      = {2018},
url       = {http://arxiv.org/abs/1805.08974},
archivePrefix = {arXiv},
eprint    = {1805.08974},
timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-08974},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wilson2000,
author = {Wilson, D. Randall and Martinez, Tony R.},
title = {Reduction Techniques for Instance-BasedLearning Algorithms},
journal = {Mach. Learn.},
issue_date = {March 2000},
volume = {38},
number = {3},
month = mar,
year = {2000},
issn = {0885-6125},
pages = {257--286},
numpages = {30},
url = {https://doi.org/10.1023/A:1007626913721},
doi = {10.1023/A:1007626913721},
acmid = {343200},
publisher = {Kluwer Academic Publishers},
address = {Hingham, MA, USA},
keywords = {classification, instance reduction, instance-based learning, nearest neighbor, pruning},
} 

@ARTICLE{1993pruningsurvey, 
author={R. Reed}, 
journal={IEEE Transactions on Neural Networks}, 
title={Pruning algorithms-a survey}, 
year={1993}, 
volume={4}, 
number={5}, 
pages={740-747}, 
keywords={learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods}, 
doi={10.1109/72.248452}, 
ISSN={1045-9227}, 
month={Sept},}

@ARTICLE{2014memorybounded,
 author = {{Collins}, M.~D. and {Kohli}, P.},
  title = "{Memory Bounded Deep Convolutional Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1412.1442},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2014,
  month = dec,
 adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{villani2008optimal,
title={Optimal Transport: Old and New},
author={Villani, C.},
isbn={9783540710509},
lccn={2008932183},
series={Grundlehren der mathematischen Wissenschaften},
url={https://books.google.com/books?id=hV8o5R7\_5tkC},
year={2008},
publisher={Springer Berlin Heidelberg}
}


@ARTICLE{Novel2009, 
author={H. A. Fayed and A. F. Atiya}, 
journal={IEEE Transactions on Neural Networks}, 
title={A Novel Template Reduction Approach for the$K$-Nearest Neighbor Method}, 
year={2009}, 
volume={20}, 
number={5}, 
pages={890-896}, 
keywords={data reduction;pattern classification;template reduction approach;K-nearest neighbor method;pattern classification;large data set;condensing approach;pattern removal;computational burden;data reduction;Nearest neighbor searches;Prototypes;Pattern classification;Cellular neural networks;Classification algorithms;Satellites;Layout;Medical diagnosis;Design engineering;Mathematics;Condensing;cross validation;editing;$K$-nearest neighbor (KNN);template reduction}, 
doi={10.1109/TNN.2009.2018547}, 
ISSN={1045-9227}, 
month={May},}

@ARTICLE{2012Prototype,
 author = {{Bien}, J. and {Tibshirani}, R.},
  title = "{Prototype selection for interpretable classification}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1202.5933},
primaryClass = "stat.AP",
keywords = {Statistics - Applications},
   year = 2012,
  month = feb,
 adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1202.5933B},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2018Liu,
 author = {{Liu}, Y. and {Chen}, J. and {Chen}, H.},
  title = "{Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1801.02850},
primaryClass = "cs.CR",
keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
   year = 2018,
  month = jan,
 adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180102850L},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{2018Theis,
 author = {{Theis}, L. and {Korshunova}, I. and {Tejani}, A. and {Husz{\'a}r}, F.
},
  title = "{Faster gaze prediction with dense networks and Fisher pruning}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1801.05787},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
   year = 2018,
  month = jan,
 adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180105787T},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2015Han,
 author = {{Han}, S. and {Mao}, H. and {Dally}, W.~J.},
  title = "{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1510.00149},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
   year = 2015,
  month = oct,
 adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151000149H},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014Collins,
 author = {{Collins}, M.~D. and {Kohli}, P.},
  title = "{Memory Bounded Deep Convolutional Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1412.1442},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition},
   year = 2014,
  month = dec,
 adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2018Frankle,
 author = {{Frankle}, J. and {Carbin}, M.},
  title = "{The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1803.03635},
keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
   year = 2018,
  month = mar,
 adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303635F},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
  Mart\'{\i}n~Abadi and
  Ashish~Agarwal and
  Paul~Barham and
  Eugene~Brevdo and
  Zhifeng~Chen and
  Craig~Citro and
  Greg~S.~Corrado and
  Andy~Davis and
  Jeffrey~Dean and
  Matthieu~Devin and
  Sanjay~Ghemawat and
  Ian~Goodfellow and
  Andrew~Harp and
  Geoffrey~Irving and
  Michael~Isard and
  Yangqing Jia and
  Rafal~Jozefowicz and
  Lukasz~Kaiser and
  Manjunath~Kudlur and
  Josh~Levenberg and
  Dandelion~Man\'{e} and
  Rajat~Monga and
  Sherry~Moore and
  Derek~Murray and
  Chris~Olah and
  Mike~Schuster and
  Jonathon~Shlens and
  Benoit~Steiner and
  Ilya~Sutskever and
  Kunal~Talwar and
  Paul~Tucker and
  Vincent~Vanhoucke and
  Vijay~Vasudevan and
  Fernanda~Vi\'{e}gas and
  Oriol~Vinyals and
  Pete~Warden and
  Martin~Wattenberg and
  Martin~Wicke and
  Yuan~Yu and
  Xiaoqiang~Zheng},
year = {2015},
month = jan,
}


@article{Goyal2017,
 author = {{Goyal}, P. and {Doll{\'a}r}, P. and {Girshick}, R. and {Noordhuis}, P. and 
{Wesolowski}, L. and {Kyrola}, A. and {Tulloch}, A. and {Jia}, Y. and 
{He}, K.},
  title = "{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}",
journal = {ArXiv e-prints},
archivePrefix = "arXiv",
 eprint = {1706.02677},
primaryClass = "cs.CV",
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
   year = 2017,
  month = jun,
 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602677G},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}}

@article{DeVries2019,
author    = {Terrance DeVries and
            Ishan Misra and
            Changhan Wang and
            Laurens van der Maaten},
title     = {Does Object Recognition Work for Everyone?},
journal   = {CoRR},
volume    = {abs/1906.02659},
year      = {2019},
}

@article{Zhao2017,
author    = {Jieyu Zhao and
            Tianlu Wang and
            Mark Yatskar and
            Vicente Ordonez and
            Kai{-}Wei Chang},
title     = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
            Constraints},
journal   = {CoRR},
volume    = {abs/1707.09457},
year      = {2017},
}

@incollection{NeurIPS2018_7308,
title = {Sparse DNNs with Improved Adversarial Robustness},
author = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {242--251},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.NeurIPS.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf}
}

%% Citation entry on ArXiv
@misc{micikevicius2017mixed,
  title={Mixed Precision Training},
  author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  year={2017},
  eprint={1710.03740},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}


@article{Lattner2020MLIRAC,
title={MLIR: A Compiler Infrastructure for the End of Moore's Law},
author={Chris Lattner and Jacques A. Pienaar and Mehdi Amini and Uday Bondhugula and River Riddle and Albert Cohen and Tatiana Shpeisman and Andy Davis and Nicolas Vasilache and Oleksandr Zinenko},
journal={ArXiv},
year={2020},
volume={abs/2002.11054}
}


@article{10.1001/archopht.116.4.502,
author   = {Daw, Nigel W.},
title    = {{Critical Periods and Amblyopia}},
journal  = {Archives of Ophthalmology},
volume   = {116},
number   = {4},
pages    = {502-505},
year     = {1998},
month    = {04},
abstract = {{During the past 20 years, basic science has shown that there are different critical periods for different visual functions during the development of the visual system. Visual functions processed at higher anatomical levels within the system have a later critical period than functions processed at lower levels. This general principle suggests that treatments for amblyopia should be followed in a logical sequence, with treatment for each visual function to be started before its critical period is over. However, critical periods for some visual functions, such as stereopsis, are not yet fully determined, and the optimal treatment is, therefore, unknown. This article summarizes the current extent of our knowledge and points to the gaps that need to be filled.Arch Ophthalmol. 1998;116:502-505-->}},
issn     = {0003-9950},
doi      = {10.1001/archopht.116.4.502},
url      = {https://doi.org/10.1001/archopht.116.4.502},
eprint   = {https://jamanetwork.com/journals/jamaophthalmology/articlepdf/262202/emo7751.pdf}
}


@inproceedings{10.1145/2555243.2557966,
author    = {Olukotun, Kunle},
title     = {Beyond Parallel Programming with Domain Specific Languages},
year      = {2014},
isbn      = {9781450326568},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/2555243.2557966},
doi       = {10.1145/2555243.2557966},
abstract  = {Today, almost all computer architectures are parallel and heterogeneous; a combination of multiple CPUs, GPUs and specialized processors. This creates a challenging problem for application developers who want to develop high performance programs without the effort required to use low-level, architecture specific parallel programming models (e.g. OpenMP for CMPs, CUDA for GPUs, MPI for clusters). Domain-specific languages (DSLs) are a promising solution to this problem because they can provide an avenue for high-level application-specific abstractions with implicit parallelism to be mapped directly to low level architecture-specific programming models; providing both high programmer productivity and high execution performance.In this talk I will describe an approach to building high performance DSLs, which is based on DSL embedding in a general purpose programming language, metaprogramming and a DSL infrastructure called Delite. I will describe how we transform DSL programs into efficient first-order low-level code using domain specific optimization, parallelism and locality optimization with parallel patterns, and architecture-specific code generation. All optimizations and transformations are implemented in Delite: an extensible DSL compiler infrastucture that significantly reduces the effort required to develop new DSLs. Delite DSLs for machine learning, data querying, graph analysis, and scientific computing all achieve performance competitive with manually parallelized C++ code.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages     = {179–180},
numpages  = {2},
keywords  = {domain specific languages},
location  = {Orlando, Florida, USA},
series    = {PPoPP '14}
}

@inproceedings{10.1145/3079856.3080246,
author    = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title     = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year      = {2017},
isbn      = {9781450348928},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3079856.3080246},
doi       = {10.1145/3079856.3080246},
abstract  = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages     = {1–12},
numpages  = {12},
keywords  = {accelerator, CNN, RNN, neural network, TPU, deep learning, domain-specific architecture, LSTM, GPU, MLP, DNN, TensorFlow},
location  = {Toronto, ON, Canada},
series    = {ISCA '17}
}

@article{1050511,
author  = {R. H. {Dennard} and F. H. {Gaensslen} and H. {Yu} and V. L. {Rideout} and E. {Bassous} and A. R. {LeBlanc}},
journal = {IEEE Journal of Solid-State Circuits},
title   = {Design of ion-implanted MOSFET's with very small physical dimensions},
year    = {1974},
volume  = {9},
number  = {5},
pages   = {256-268}
}

@article{129422,
author  = {E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal = {IEEE Transactions on Neural Networks},
title   = {Application of the ANNA neural network chip to high-speed character recognition},
year    = {1992},
volume  = {3},
number  = {3},
pages   = {498-505}
}

@inproceedings{1575717,
author    = {D. {Steinkraus} and I. {Buck} and P. Y. {Simard}},
booktitle = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
title     = {Using GPUs for machine learning algorithms},
year      = {2005},
volume    = {},
number    = {},
pages     = {1115-1120 Vol. 2}
}

@article{1963steinbuch,
author  = {K, Steinbuch and U. Piske},
journal = {IEEE Transactions on Electronic Computers},
title   = {Learning matrices and their applications},
year    = {1963},
volume  = {EC-12},
number  = {6},
pages   = {846-862}
}


@article{1971Naturdistance_sets,
author  = {{Levandowsky}, M.},
title   = {{Distance between Sets}},
journal = {\nat},
year    = 1971,
month   = nov,
volume  = 234,
pages   = {34-35},
doi     = {10.1038/234034a0},
adsurl  = {https://ui.adsabs.harvard.edu/abs/1971Natur.234...34L},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{1985understandingcomputers,
author    = {Time},
title     = {Understanding computers: software},
publisher = {Time},
year      = {1985},
address   = {Virginia}
}

@misc{1986Rosenblatt,
author    = {Van Der Malsburg, C},
title     = {Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
year      = {1986},
publisher = {Springer Berlin Heidelberg},
pages     = {245--248}
}

@inbook{1988rumelhart,
author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title     = {Learning Representations by Back-Propagating Errors},
year      = {1988},
publisher = {MIT Press},
booktitle = {Neurocomputing: Foundations of Research},
pages     = {696–699},
numpages  = {4}
}





@article{1993PASJ,
author   = {{Ito}, Tomoyoshi and {Makino}, Junichiro and {Fukushige}, Toshiyuki and
       {Ebisuzaki}, Toshikazu and {Okumura}, Sachiko K. and
       {Sugimoto}, Daiichiro},
title    = {{A Special-Purpose Computer forN-Body Simulations: GRAPE-2A}},
journal  = {\pasj},
keywords = {MANY-BODY SYSTEM, SIMULATION, SPECIAL-PURPOSE COMPUTER, STELLAR DYNAMICS},
year     = 1993,
month    = jun,
volume   = {45},
pages    = {339-347},
adsurl   = {https://ui.adsabs.harvard.edu/abs/1993PASJ...45..339I},
adsnote  = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{1993pruningsurvey,
author   = {R. Reed},
journal  = {IEEE Transactions on Neural Networks},
title    = {Pruning algorithms-a survey},
year     = {1993},
volume   = {4},
number   = {5},
pages    = {740-747},
keywords = {learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods},
doi      = {10.1109/72.248452},
issn     = {1045-9227},
month    = {Sept}
}


@misc{1995thinkingmachines,
title  = {The Rise and Fall of Thinking Machines},
author = {Gary Taubes},
year   = {1995},
url    = {https://www.inc.com/magazine/19950915/2622.html}
}


@inbook{2003Gitelman,
title     = {How Users Define New Media: A History of the Amusement Phonograph},
author    = {Lisa Gitelman},
year      = {2003},
language  = {English (US)},
editor    = {Thorburn, {David } and Jenkins, {Henry }},
booktitle = {Rethinking Media Change},
publisher = {MIT Press}
}

@article{2007legg,
author        = {{Legg}, Shane and {Hutter}, Marcus},
title         = {{A Collection of Definitions of Intelligence}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Artificial Intelligence},
year          = 2007,
month         = jun,
eid           = {arXiv:0706.3639},
pages         = {arXiv:0706.3639},
archiveprefix = {arXiv},
eprint        = {0706.3639},
primaryclass  = {cs.AI},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2007arXiv0706.3639L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2012Prototype,
author        = {{Bien}, J. and {Tibshirani}, R.},
title         = {{Prototype selection for interpretable classification}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1202.5933},
primaryclass  = {stat.AP},
keywords      = {Statistics - Applications},
year          = 2012,
month         = feb,
adsurl        = {http://adsabs.harvard.edu/abs/2012arXiv1202.5933B},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2014certifying_removing_disparate_impact,
author        = {{Feldman}, Michael and {Friedler}, Sorelle and {Moeller}, John and
       {Scheidegger}, Carlos and {Venkatasubramanian}, Suresh},
title         = {{Certifying and removing disparate impact}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Computers and Society},
year          = {2014},
month         = {Dec},
eid           = {arXiv:1412.3756},
pages         = {arXiv:1412.3756},
archiveprefix = {arXiv},
eprint        = {1412.3756},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3756F},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2014Collins,
author        = {{Collins}, M.~D. and {Kohli}, P.},
title         = {{Memory Bounded Deep Convolutional Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1412.1442},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = dec,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{2014Horowitz,
author    = {M. {Horowitz}},
booktitle = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
title     = {1.1 Computing's energy problem (and what we can do about it)},
year      = {2014},
volume    = {},
number    = {},
pages     = {10-14}
}

@article{2014Mark1,
title   = {Grace Hopper, computing pioneer},
journal = {The Harvard Gazette},
year    = {2014},
url     = {https://news.harvard.edu/gazette/story/2014/12/grace-hopper-computing-pioneer/},
author  = {Walter Isaacson}
}

@article{2014memorybounded,
author        = {{Collins}, M.~D. and {Kohli}, P.},
title         = {{Memory Bounded Deep Convolutional Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1412.1442},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = dec,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.1442C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2014Nguyen,
author        = {{Nguyen}, Anh and {Yosinski}, Jason and {Clune}, Jeff},
title         = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
year          = {2014},
month         = {Dec},
eid           = {arXiv:1412.1897},
pages         = {arXiv:1412.1897},
archiveprefix = {arXiv},
eprint        = {1412.1897},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.1897N},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2014Simonyan,
author        = {{Simonyan}, K. and {Zisserman}, A.},
title         = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1409.1556},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2014,
month         = sep,
adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1409.1556S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2015Han,
author        = {{Han}, S. and {Mao}, H. and {Dally}, W.~J.},
title         = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1510.00149},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
year          = 2015,
month         = oct,
adsurl        = {http://adsabs.harvard.edu/abs/2015arXiv151000149H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2015szegedy,
author        = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and
{Shlens}, Jonathon and {Wojna}, Zbigniew},
title         = {{Rethinking the Inception Architecture for Computer Vision}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2015,
month         = dec,
eid           = {arXiv:1512.00567},
pages         = {arXiv:1512.00567},
archiveprefix = {arXiv},
eprint        = {1512.00567},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2015Szegedy,
author        = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and
       {Shlens}, Jonathon and {Wojna}, Zbigniew},
title         = {{Rethinking the Inception Architecture for Computer Vision}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2015},
month         = {Dec},
eid           = {arXiv:1512.00567},
pages         = {arXiv:1512.00567},
archiveprefix = {arXiv},
eprint        = {1512.00567},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016alexey,
author        = {{Kurakin}, Alexey and {Goodfellow}, Ian and {Bengio}, Samy},
title         = {{Adversarial examples in the physical world}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2016},
month         = {Jul},
eid           = {arXiv:1607.02533},
pages         = {arXiv:1607.02533},
archiveprefix = {arXiv},
eprint        = {1607.02533},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160702533K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016amodei,
author        = {{Amodei}, Dario and {Olah}, Chris and {Steinhardt}, Jacob and
       {Christiano}, Paul and {Schulman}, John and {Man{\'e}}, Dan},
title         = {{Concrete Problems in AI Safety}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
year          = {2016},
month         = {Jun},
eid           = {arXiv:1606.06565},
pages         = {arXiv:1606.06565},
archiveprefix = {arXiv},
eprint        = {1606.06565},
primaryclass  = {cs.AI},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160606565A},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2016DropNeuron,
author        = {{Pan}, W. and {Dong}, H. and {Guo}, Y.},
title         = {{DropNeuron: Simplifying the Structure of Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1606.07326},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2016,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160607326P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}





@article{2016fair_prediction,
author        = {{Chouldechova}, Alexandra},
title         = {{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Machine Learning},
year          = {2016},
month         = {Oct},
eid           = {arXiv:1610.07524},
pages         = {arXiv:1610.07524},
archiveprefix = {arXiv},
eprint        = {1610.07524},
primaryclass  = {stat.AP},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161007524C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}






@article{2016Lakshminarayanan,
author        = {{Lakshminarayanan}, Balaji and {Pritzel}, Alexander and
       {Blundell}, Charles},
title         = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2016},
month         = {Dec},
eid           = {arXiv:1612.01474},
pages         = {arXiv:1612.01474},
archiveprefix = {arXiv},
eprint        = {1612.01474},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161201474L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2016Molchanov,
author        = {{Molchanov}, P. and {Tyree}, S. and {Karras}, T. and {Aila}, T. and 
{Kautz}, J.},
title         = {{Pruning Convolutional Neural Networks for Resource Efficient Inference}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1611.06440},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2016,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv161106440M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2016Squeezenet,
author        = {{Iandola}, F.~N. and {Han}, S. and {Moskewicz}, M.~W. and {Ashraf}, K. and 
{Dally}, W.~J. and {Keutzer}, K.},
title         = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $<$0.5MB model size}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1602.07360},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
year          = 2016,
month         = feb,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160207360I},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2016Szegedy,
author        = {{Szegedy}, Christian and {Ioffe}, Sergey and {Vanhoucke}, Vincent and
       {Alemi}, Alex},
title         = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2016},
month         = {Feb},
eid           = {arXiv:1602.07261},
pages         = {arXiv:1602.07261},
archiveprefix = {arXiv},
eprint        = {1602.07261},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207261S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2016Zhang,
author        = {{Zhang}, Chiyuan and {Bengio}, Samy and {Hardt}, Moritz and
       {Recht}, Benjamin and {Vinyals}, Oriol},
title         = {{Understanding deep learning requires rethinking generalization}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning},
year          = {2016},
month         = {Nov},
eid           = {arXiv:1611.03530},
pages         = {arXiv:1611.03530},
archiveprefix = {arXiv},
eprint        = {1611.03530},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161103530Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2017arXiv170604599G,
author        = {{Guo}, C. and {Pleiss}, G. and {Sun}, Y. and {Weinberger}, K.~Q.
},
title         = {{On Calibration of Modern Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1706.04599},
keywords      = {Computer Science - Machine Learning},
year          = 2017,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170604599G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017arXivBenoit,
author        = {{Jacob}, Benoit and {Kligys}, Skirmantas and {Chen}, Bo and
       {Zhu}, Menglong and {Tang}, Matthew and {Howard}, Andrew and
       {Adam}, Hartwig and {Kalenichenko}, Dmitry},
title         = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2017,
month         = dec,
eid           = {arXiv:1712.05877},
pages         = {arXiv:1712.05877},
archiveprefix = {arXiv},
eprint        = {1712.05877},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017benoit,
author        = {{Jacob}, Benoit and {Kligys}, Skirmantas and {Chen}, Bo and
       {Zhu}, Menglong and {Tang}, Matthew and {Howard}, Andrew and
       {Adam}, Hartwig and {Kalenichenko}, Dmitry},
title         = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2017,
month         = dec,
eid           = {arXiv:1712.05877},
pages         = {arXiv:1712.05877},
archiveprefix = {arXiv},
eprint        = {1712.05877},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171205877J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2017Cortes,
author        = {{Cortes}, Corinna and {DeSalvo}, Giulia and {Gentile}, Claudio and
       {Mohri}, Mehryar and {Yang}, Scott},
title         = {{Online Learning with Abstention}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning},
year          = {2017},
month         = {Mar},
eid           = {arXiv:1703.03478},
pages         = {arXiv:1703.03478},
archiveprefix = {arXiv},
eprint        = {1703.03478},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303478C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017Gurumoorthy,
author        = {{Gurumoorthy}, Karthik S. and {Dhurandhar}, Amit and
       {Cecchi}, Guillermo and {Aggarwal}, Charu},
title         = {{Efficient Data Representation by Selecting Prototypes with Importance Weights}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning, 65K05, 68W25},
year          = {2017},
month         = {Jul},
eid           = {arXiv:1707.01212},
pages         = {arXiv:1707.01212},
archiveprefix = {arXiv},
eprint        = {1707.01212},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv170701212G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}






@article{2017Huang,
author        = {{Huang}, Gao and {Liu}, Shichen and {van der Maaten}, Laurens and
       {Weinberger}, Kilian Q.},
title         = {{CondenseNet: An Efficient DenseNet using Learned Group Convolutions}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.09224},
pages         = {arXiv:1711.09224},
archiveprefix = {arXiv},
eprint        = {1711.09224},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109224H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017jo,
author        = {{Jo}, Jason and {Bengio}, Yoshua},
title         = {{Measuring the tendency of CNNs to Learn Surface Statistical Regularities}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.11561},
pages         = {arXiv:1711.11561},
archiveprefix = {arXiv},
eprint        = {1711.11561},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111561J},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2017Kearns,
author   = {{Kearns}, Michael and {Neel}, Seth and {Roth}, Aaron and
       {Wu}, Zhiwei Steven},
title    = {{Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness}},
keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computer Science and Game Theory},
year     = 2017,
month    = nov
}



@article{2017Lee,
author        = {{Lee}, Kimin and {Lee}, Honglak and {Lee}, Kibok and {Shin}, Jinwoo},
title         = {{Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2017},
month         = {Nov},
eid           = {arXiv:1711.09325},
pages         = {arXiv:1711.09325},
archiveprefix = {arXiv},
eprint        = {1711.09325},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109325L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
} 


@article{2017Molchanov,
author        = {{Molchanov}, D. and {Ashukha}, A. and {Vetrov}, D.},
title         = {{Variational Dropout Sparsifies Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1701.05369},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = 2017,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
} 


@article{2017neurons,
title   = {On the frontiers of biomedicine with professor Rahul Sarpeshkar},
journal = {Dartmouth Magazine},
year    = {2017},
url     = {https://dartmouthalumnimagazine.com/articles/cell-power},
author  = {Kristin Sainani}
}






@article{2018_sparse_evolutionary_training,
author  = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
journal = {Nature Communications},
title   = {Scalable {T}raining of {A}rtificial {N}eural {N}etworks with {A}daptive {S}parse {C}onnectivity {I}nspired by {N}etwork {S}cience},
year    = {2018}
}



@misc{2018acceleratingai,
title  = {Accelerating AI: Past, Present, and Future},
year   = {2018},
url    = {https://www.youtube.com/watch?v=8n2HLp2gtYs&t=2116s},
author = {Krste Asanovic}
}



@misc{2018Amodei,
author = {Amodei,Dario and Hernandez, Danny and Sastry,Girish and Clark, Jack and Brockman, Greg and Sutskever, Ilya},
title  = {AI and Compute},
url    = {https://openai.com/blog/ai-and-compute/},
year   = {2018}
}


@article{2018Frankle,
author        = {{Frankle}, J. and {Carbin}, M.},
title         = {{The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1803.03635},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
year          = 2018,
month         = mar,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180303635F},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2018Hendrycks,
author        = {{Hendrycks}, Dan and {Dietterich}, Thomas G.},
title         = {{Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = {2018},
month         = {Jul},
eid           = {arXiv:1807.01697},
pages         = {arXiv:1807.01697},
archiveprefix = {arXiv},
eprint        = {1807.01697},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180701697H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{2018Liu,
author        = {{Liu}, Y. and {Chen}, J. and {Chen}, H.},
title         = {{Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.02850},
primaryclass  = {cs.CR},
keywords      = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180102850L},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}








@article{2018Mittal,
author        = {Deepak Mittal and
             Shweta Bhardwaj and
             Mitesh M. Khapra and
             Balaraman Ravindran},
title         = {Recovering from Random Pruning: On the Plasticity of Deep Convolutional
             Neural Networks},
journal       = {CoRR},
volume        = {abs/1801.10447},
year          = {2018},
url           = {http://arxiv.org/abs/1801.10447},
archiveprefix = {arXiv},
eprint        = {1801.10447},
timestamp     = {Mon, 13 Aug 2018 16:46:51 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1801-10447},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{2018Nalisnick,
author        = {{Nalisnick}, Eric and {Matsukawa}, Akihiro and {Whye Teh}, Yee and
       {Gorur}, Dilan and {Lakshminarayanan}, Balaji},
title         = {{Do Deep Generative Models Know What They Don't Know?}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
year          = {2018},
month         = {Oct},
eid           = {arXiv:1810.09136},
pages         = {arXiv:1810.09136},
archiveprefix = {arXiv},
eprint        = {1810.09136},
primaryclass  = {stat.ML},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009136N},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2018Parisi,
author        = {{Parisi}, German I. and {Kemker}, Ronald and {Part}, Jose L. and
       {Kanan}, Christopher and {Wermter}, Stefan},
title         = {{Continual Lifelong Learning with Neural Networks: A Review}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
year          = 2018,
month         = feb,
eid           = {arXiv:1802.07569},
pages         = {arXiv:1802.07569},
archiveprefix = {arXiv},
eprint        = {1802.07569},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180207569P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2018random_pruning,
author        = {{Mittal}, D. and {Bhardwaj}, S. and {Khapra}, M.~M. and {Ravindran}, B.
},
title         = {{Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.10447},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180110447M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{2018Sato,
author = {Kaz Sato},
title  = {What makes TPUs fine-tuned for deep learning?},
url    = {https://bit.ly/2ER3bIu},
year   = {2018}
}

@article{2018Theis,
author        = {{Theis}, L. and {Korshunova}, I. and {Tejani}, A. and {Husz{\'a}r}, F.
},
title         = {{Faster gaze prediction with dense networks and Fisher pruning}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1801.05787},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2018,
month         = jan,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180105787T},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{2018vacuum,
author = {Computer History Archives Project},
title  = {Computer History 1949 - 1960 Early Vacuum Tube Computers Overview},
url    = {https://www.youtube.com/watch?v=WnNm_uJYWhA},
year   = {2018}
}

@article{2018Vodrahalli,
author        = {{Vodrahalli}, Kailas and {Li}, Ke and {Malik}, Jitendra},
title         = {{Are All Training Examples Created Equal? An Empirical Study}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = {2018},
month         = {Nov},
eid           = {arXiv:1811.12569},
pages         = {arXiv:1811.12569},
archiveprefix = {arXiv},
eprint        = {1811.12569},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv181112569V},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2018Wang,
author        = {{Wang}, Tianyang and {Huan}, Jun and {Li}, Bo},
title         = {{Data Dropout: Optimizing Training Data for Convolutional Neural Networks}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2018},
month         = {Sep},
eid           = {arXiv:1809.00193},
pages         = {arXiv:1809.00193},
archiveprefix = {arXiv},
eprint        = {1809.00193},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180900193W},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv190110566Z,
author        = {{Zink}, Anna and {Rose}, Sherri},
title         = {{Fair Regression for Health Care Spending}},
journal       = {arXiv e-prints},
keywords      = {Statistics - Applications, Computer Science - Computers and Society, Statistics - Methodology, Statistics - Machine Learning},
year          = {2019},
month         = {Jan},
eid           = {arXiv:1901.10566},
pages         = {arXiv:1901.10566},
archiveprefix = {arXiv},
eprint        = {1901.10566},
primaryclass  = {stat.AP},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190110566Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv190602243S,
author        = {{Strubell}, Emma and {Ganesh}, Ananya and {McCallum}, Andrew},
title         = {{Energy and Policy Considerations for Deep Learning in NLP}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computation and Language},
year          = 2019,
month         = jun,
eid           = {arXiv:1906.02243},
pages         = {arXiv:1906.02243},
archiveprefix = {arXiv},
eprint        = {1906.02243},
primaryclass  = {cs.CL},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602243S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2019arXiv190608158K,
author        = {{Kirsch}, Andreas and {van Amersfoort}, Joost and {Gal}, Yarin},
title         = {{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = {2019},
month         = {Jun},
eid           = {arXiv:1906.08158},
pages         = {arXiv:1906.08158},
archiveprefix = {arXiv},
eprint        = {1906.08158},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608158K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2019arXiv190802900M,
author        = {{Mwebaze}, Ernest and {Gebru}, Timnit and {Frome}, Andrea and
       {Nsumba}, Solomon and {Tusubira}, Jeremy},
title         = {{iCassava 2019Fine-Grained Visual Categorization Challenge}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = {2019},
month         = {Aug},
eid           = {arXiv:1908.02900},
pages         = {arXiv:1908.02900},
archiveprefix = {arXiv},
eprint        = {1908.02900},
primaryclass  = {cs.CV},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190802900M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2019arXiv191111134E,
author   = {{Evci}, Utku and {Gale}, Trevor and {Menick}, Jacob and
{Castro}, Pablo Samuel and {Elsen}, Erich},
title    = {{Rigging the Lottery: Making All Tickets Winners}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year     = 2019,
month    = nov
}




@inproceedings{2019barham,
author    = {Barham, Paul and Isard, Michael},
title     = {Machine Learning Systems Are Stuck in a Rut},
year      = {2019},
isbn      = {9781450367271},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3317550.3321441},
doi       = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages     = {177–183},
numpages  = {7},
location  = {Bertinoro, Italy},
series    = {HotOS ’19}
}

@misc{2019cross,
author   = {{Cross}, Andrew W. and {Bishop}, Lev S. and {Sheldon}, Sarah and
{Nation}, Paul D. and {Gambetta}, Jay M.},
title    = {{Validating quantum computers using randomized model circuits}},
keywords = {Quantum Physics},
year     = 2019,
month    = sep,
volume   = {100},
number   = {3}
}

@misc{2019EdgeTpu,
author = {Gupta, Suyog and Tan, Mingxing},
title  = {EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML},
url    = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html},
year   = {2019}
}


@misc{2019Feldman,
title  = {The era of general purpose computers is ending},
year   = {2019},
url    = {https://bit.ly/3hP8XJh},
author = {Michael Feldman}
}




@article{2019Hendrycks_Dietterich,
author        = {{Hendrycks}, Dan and {Dietterich}, Thomas},
title         = {{Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = {2019},
month         = {Mar},
eid           = {arXiv:1903.12261},
pages         = {arXiv:1903.12261},
archiveprefix = {arXiv},
eprint        = {1903.12261},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190312261H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv200303033B,
author        = {{Blalock}, Davis and {Gonzalez Ortiz}, Jose Javier and
       {Frankle}, Jonathan and {Guttag}, John},
title         = {{What is the State of Neural Network Pruning?}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2020,
month         = mar,
eid           = {arXiv:2003.03033},
pages         = {arXiv:2003.03033},
archiveprefix = {arXiv},
eprint        = {2003.03033},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200303033B},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv200610901G,
author   = {{Gale}, Trevor and {Zaharia}, Matei and {Young}, Cliff and
{Elsen}, Erich},
title    = {{Sparse GPU Kernels for Deep Learning}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
year     = 2020,
month    = jun
}


@article{2020arXiv200906489H,
author        = {{Hooker}, Sara},
title         = {{The Hardware Lottery}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
year          = 2020,
month         = sep,
eid           = {arXiv:2009.06489},
pages         = {arXiv:2009.06489},
archiveprefix = {arXiv},
eprint        = {2009.06489},
primaryclass  = {cs.CY},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200906489H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{2020arXiv201003058H,
author        = {{Hooker}, Sara and {Moorosi}, Nyalleng and {Clark}, Gregory and {Bengio}, Samy and {Denton}, Emily},
title         = {{Characterising Bias in Compressed Models}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
year          = 2020,
month         = oct,
eid           = {arXiv:2010.03058},
pages         = {arXiv:2010.03058},
archiveprefix = {arXiv},
eprint        = {2010.03058},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201003058H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2020blalock,
author        = {{Blalock}, Davis and {Gonzalez Ortiz}, Jose Javier and
       {Frankle}, Jonathan and {Guttag}, John},
title         = {{What is the State of Neural Network Pruning?}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
year          = 2020,
month         = mar,
eid           = {arXiv:2003.03033},
pages         = {arXiv:2003.03033},
archiveprefix = {arXiv},
eprint        = {2003.03033},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200303033B},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{2020brown,
author   = {{Brown}, Tom B. and {Mann}, Benjamin and {Ryder}, Nick and
{Subbiah}, Melanie and {Kaplan}, Jared and {Dhariwal}, Prafulla and
{Neelakantan}, Arvind and {Shyam}, Pranav and {Sastry}, Girish and
{Askell}, Amanda and {Agarwal}, Sandhini and {Herbert-Voss}, Ariel and
{Krueger}, Gretchen and {Henighan}, Tom and {Child}, Rewon and
{Ramesh}, Aditya and {Ziegler}, Daniel M. and {Wu}, Jeffrey and
{Winter}, Clemens and {Hesse}, Christopher and {Chen}, Mark and
{Sigler}, Eric and {Litwin}, Mateusz and {Gray}, Scott and
{Chess}, Benjamin and {Clark}, Jack and {Berner}, Christopher and {McCand
lish}, Sam and {Radford}, Alec and {Sutskever}, Ilya and {Amodei}, Dario},
title    = {{Language Models are Few-Shot Learners}},
journal  = {arXiv e-prints},
keywords = {Computer Science - Computation and Language},
year     = 2020,
month    = may
}

@misc{2020cortexm,
author = {ARM},
title  = {Enhancing AI Performance for IoT Endpoint Devices},
url    = {https://www.arm.com/company/news/2020/02/new-ai-technology-from-arm},
year   = {2020}
}


@misc{2020matrix,
title    = {Understanding Matrix Capsules with EM
Routing.},
url      = {https://jhui.github.io/2017/11/14/
Matrix-Capsules-with-EM-routing-Capsule-Network},
accessed = {2020-04-20}
}



@article{2020Mirhoseini,
author        = {{Mirhoseini}, Azalia and {Goldie}, Anna and {Yazgan}, Mustafa and
       {Jiang}, Joe and {Songhori}, Ebrahim and {Wang}, Shen and
       {Lee}, Young-Joon and {Johnson}, Eric and {Pathak}, Omkar and
       {Bae}, Sungmin and {Nazi}, Azade and {Pak}, Jiwoo and {Tong}, Andy and
       {Srinivasa}, Kavya and {Hang}, William and {Tuncer}, Emre and
       {Babu}, Anand and {Le}, Quoc V. and {Laudon}, James and {Ho}, Richard and
       {Carpenter}, Roger and {Dean}, Jeff},
title         = {{Chip Placement with Deep Reinforcement Learning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
year          = 2020,
month         = apr,
eid           = {arXiv:2004.10746},
pages         = {arXiv:2004.10746},
archiveprefix = {arXiv},
eprint        = {2004.10746},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200410746M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2020sun,
author        = {{Sun}, Fei and {Qin}, Minghai and {Zhang}, Tianyun and {Liu}, Liu and
{Chen}, Yen-Kuang and {Xie}, Yuan},
title         = {{Computation on Sparse Neural Networks: an Inspiration for Future Hardware}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = 2020,
month         = apr,
eid           = {arXiv:2004.11946},
pages         = {arXiv:2004.11946},
archiveprefix = {arXiv},
eprint        = {2004.11946},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200411946S},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{6527325,
author  = {D. E. {Nikonov} and I. A. {Young}},
journal = {Proceedings of the IEEE},
title   = {Overview of Beyond-CMOS Devices and a Uniform Methodology for Their Benchmarking},
year    = {2013},
volume  = {101},
number  = {12},
pages   = {2498-2533}
}


@inproceedings{6909517,
author    = {X. {Zhu} and D. {Anguelov} and D. {Ramanan}},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
title     = {Capturing Long-Tail Distributions of Object Subcategories},
year      = {2014},
volume    = {},
number    = {},
pages     = {915-922}
}

@inproceedings{7298594,
author    = {C. {Szegedy} and  {Wei Liu} and  {Yangqing Jia} and P. {Sermanet} and S. {Reed} and D. {Anguelov} and D. {Erhan} and V. {Vanhoucke} and A. {Rabinovich}},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title     = {Going deeper with convolutions},
year      = {2015},
volume    = {},
number    = {},
pages     = {1-9}
}

@inproceedings{7459430,
author    = {G. {Fursin} and A. {Lokhmotov} and E. {Plowman}},
booktitle = {2016 Design, Automation   Test in Europe Conference   Exhibition (DATE)},
title     = {Collective Knowledge: Towards R D sustainability},
year      = {2016},
volume    = {},
number    = {},
pages     = {864-869}
}


@article{7866802,
author  = {B. {Falsafi} and B. {Dally} and D. {Singh} and D. {Chiou} and J. J. {Yi} and R. {Sendag}},
journal = {IEEE Micro},
title   = {FPGAs versus GPUs in Data centers},
year    = {2017},
volume  = {37},
number  = {1},
pages   = {60-72}
}

@inproceedings{8192487,
author    = {R. {Prabhakar} and Y. {Zhang} and D. {Koeplinger} and M. {Feldman} and T. {Zhao} and S. {Hadjis} and A. {Pedram} and C. {Kozyrakis} and K. {Olukotun}},
booktitle = {2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)},
title     = {Plasticine: A reconfigurable architecture for parallel patterns},
year      = {2017},
volume    = {},
number    = {},
pages     = {389-402}
}


@article{8476161,
author  = {J. {Dongarra} and M. {Gates} and J. {Kurzak} and P. {Luszczek} and Y. M. {Tsai}},
journal = {Proceedings of the IEEE},
title   = {Autotuning Numerical Dense Linear Algebra for Batched Computation With GPU Hardware Accelerators},
year    = {2018},
volume  = {106},
number  = {11},
pages   = {2040-2055}
}


@article{adam-optimizer,
author  = {Diederik P. Kingma and
             Jimmy Ba},
title   = {Adam: {A} {M}ethod for {S}tochastic {O}ptimization},
journal = {CoRR},
volume  = {abs/1412.6980},
year    = {2014},
url     = {http://arxiv.org/abs/1412.6980}
}



@inproceedings{Aha_1989,
author    = {Aha, David W. and Kibler, Dennis},
title     = {Noise-tolerant Instance-based Learning Algorithms},
booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
series    = {IJCAI'89},
year      = {1989},
location  = {Detroit, Michigan},
pages     = {794--799},
numpages  = {6},
url       = {http://dl.acm.org/citation.cfm?id=1623755.1623881},
acmid     = {1623881},
publisher = {Morgan Kaufmann Publishers Inc.},
address   = {San Francisco, CA, USA}
}




@article{Aha1991,
author  = {Aha, David W.
and Kibler, Dennis
and Albert, Marc K.},
title   = {Instance-based learning algorithms},
journal = {Machine Learning},
year    = {1991},
month   = {Jan},
day     = {01},
volume  = {6},
number  = {1},
pages   = {37--66},
url     = {https://doi.org/10.1007/BF00153759}
}


@article{Aha1992ToleratingNI,
title   = {Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms},
author  = {David W. Aha},
journal = {International Journal of Man-Machine Studies},
year    = {1992},
volume  = {36},
pages   = {267-287}
}






@article{ambrogio2018,
author  = {Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert and Boybat, Irem and Nolfo, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan and Killeen, Benjamin and Cheng, Christina and Jaoudi, Yassine and Burr, Geoffrey},
year    = {2018},
month   = {06},
pages   = {},
title   = {Equivalent-accuracy accelerated neural-network training using analogue memory},
volume  = {558},
journal = {Nature},
doi     = {10.1038/s41586-018-0180-5}
}

@article{amodei2016,
author        = {Dario Amodei and
             Chris Olah and
             Jacob Steinhardt and
             Paul F. Christiano and
             John Schulman and
             Dan Man{\'{e}}},
title         = {Concrete Problems in {AI} Safety},
journal       = {CoRR},
volume        = {abs/1606.06565},
year          = {2016},
url           = {http://arxiv.org/abs/1606.06565},
archiveprefix = {arXiv},
eprint        = {1606.06565},
timestamp     = {Mon, 13 Aug 2018 16:48:59 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/AmodeiOSCSM16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}




@misc{anonymous,
title  = {Suppressed for Anonymity},
author = {Author, N. N.},
year   = {2018}
}


@inproceedings{Ansel2014,
author    = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
title     = {OpenTuner: An Extensible Framework for Program Autotuning},
year      = {2014},
isbn      = {9781450328098},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/2628071.2628092},
doi       = {10.1145/2628071.2628092},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages     = {303–316},
numpages  = {14},
keywords  = {optimization, autotuner},
location  = {Edmonton, AB, Canada},
series    = {PACT '14}
}

@article{article,
author  = {Konishi, Masakazu},
year    = {1985},
month   = {02},
pages   = {125-70},
title   = {Birdsong: From Behavior to Neuron},
volume  = {8},
journal = {Annual review of neuroscience},
doi     = {10.1146/annurev.ne.08.030185.001013}
}



@techreport{Asanovic2006,
author      = {Asanović, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A.},
title       = {The Landscape of Parallel Computing Research: A View from Berkeley},
institution = {EECS Department, University of California, Berkeley},
year        = {2006},
month       = {Dec},
url         = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
number      = {UCB/EECS-2006-183}
}

@article{autoencoding-variational-bayes,
author  = {Diederik P. Kingma and
             Max Welling},
title   = {Auto-Encoding Variational Bayes},
journal = {CoRR},
volume  = {abs/1312.6114},
year    = {2013}
} 


@inproceedings{automatic-model-compression,
author    = {Yihui He and
             Ji Lin and
             Zhijian Liu and
             Hanrui Wang and
             Li{-}Jia Li and
             Song Han},
title     = {{AMC:} AutoML for Model Compression and Acceleration on Mobile Devices},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VII}},
pages     = {815--832},
year      = {2018}
}



@article{ba2016layer,
title   = {Layer normalization},
author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
journal = {arXiv preprint arXiv:1607.06450},
year    = {2016}
}

@article{Bach2015,
author        = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
date-added    = {2016-10-12 11:12:26 +0000},
date-modified = {2016-10-12 11:16:21 +0000},
journal       = {PloS one},
number        = {7},
pages         = {e0130140},
publisher     = {Public Library of Science},
title         = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
volume        = {10},
year          = 2015
}




@article{balduzzi2017shattered,
title   = {The shattered gradients problem: If resnets are the answer, then what is the question?},
author  = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
journal = {arXiv preprint arXiv:1702.08591},
year    = {2017}
}

@inproceedings{Barham2019,
author    = {Barham, Paul and Isard, Michael},
title     = {Machine Learning Systems Are Stuck in a Rut},
year      = {2019},
isbn      = {9781450367271},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/3317550.3321441},
doi       = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages     = {177–183},
numpages  = {7},
location  = {Bertinoro, Italy},
series    = {HotOS ’19}
}





@inproceedings{bayesian-compression,
author    = {Christos Louizos and
             Karen Ullrich and
             Max Welling},
title     = {Bayesian {C}ompression for {D}eep {L}earning},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {3290--3300},
year      = {2017}
}



@article{bellec2017deep,
title   = {Deep rewiring: Training very sparse deep networks},
author  = {Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
journal = {arXiv preprint arXiv:1711.05136},
year    = {2017}
}

@incollection{Bengio+chapter2007,
author    = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title     = {Scaling Learning Algorithms Towards {AI}},
year      = {2007}
}

@article{bengio1994learning,
title     = {Learning long-term dependencies with gradient descent is difficult},
author    = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
journal   = {IEEE transactions on neural networks},
volume    = {5},
number    = {2},
pages     = {157--166},
year      = {1994},
publisher = {IEEE}
}

@inproceedings{Bengio2009,
author    = {Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title     = {Curriculum Learning},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
series    = {ICML '09},
year      = {2009},
isbn      = {978-1-60558-516-1},
location  = {Montreal, Quebec, Canada},
pages     = {41--48},
numpages  = {8},
url       = {http://doi.acm.org/10.1145/1553374.1553380},
doi       = {10.1145/1553374.1553380},
acmid     = {1553380},
publisher = {ACM},
address   = {New York, NY, USA}
}


@article{benna2016,
author  = {Benna, Marcus and Fusi, Stefano},
year    = {2016},
month   = {10},
pages   = {},
title   = {Computational principles of synaptic memory consolidation},
volume  = {19},
journal = {Nature Neuroscience},
doi     = {10.1038/nn.4401}
}





@article{Bien_2011,
title     = {Prototype selection for interpretable classification},
volume    = {5},
issn      = {1932-6157},
url       = {http://dx.doi.org/10.1214/11-AOAS495},
doi       = {10.1214/11-aoas495},
number    = {4},
journal   = {The Annals of Applied Statistics},
publisher = {Institute of Mathematical Statistics},
author    = {Bien, Jacob and Tibshirani, Robert},
year      = {2011},
month     = {Dec},
pages     = {2403–2424}
}


@article{Birodkar2019SemanticRI,
title   = {Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need},
author  = {Vighnesh Birodkar and Hossein Mobahi and Samy Bengio},
journal = {ArXiv},
year    = {2019},
volume  = {abs/1901.11409}
}

@article{blalock2020state,
title   = {What is the state of neural network pruning?},
author  = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
journal = {arXiv preprint arXiv:2003.03033},
year    = {2020}
}

@misc{blocksparse-gpu-kernels,
title        = {Block-Sparse GPU Kernels},
author       = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year         = {2017}
}

misc{blocksparse-gpu-kernels,
title        = {Block-Sparse GPU Kernels},
author       = {Scott Gray and Alec Radford and Diedrik P. Kingma},
howpublished = {\url{https://blog.openai.com/block-sparse-gpu-kernels/}},
year         = {2017}
}

@misc{bremner2013,
author = {Bremner, Andrew and Lewkowicz, David and Spence, Charles},
year   = {2013},
month  = {11},
pages  = {},
title  = {Multisensory Development},
doi    = {10.1093/acprof:oso/9780199586059.003.0001}
}


@book{bruce1991,
author    = {Collier, Bruce},
title     = {Little Engines That Could’ve: The Calculating Machines of Charles Babbage},
year      = {1991},
isbn      = {0824000439},
publisher = {Garland Publishing, Inc.},
address   = {USA}
} 


@article{bubic2010,
author  = {Bubic, Andreja and Von Cramon, D. Yves and Schubotz, Ricarda},
title   = {Prediction, cognition and the brain},
journal = {Frontiers in Human Neuroscience},
volume  = {4},
pages   = {25},
year    = {2010},
url     = {https://www.frontiersin.org/article/10.3389/fnhum.2010.00025},
doi     = {10.3389/fnhum.2010.00025},
issn    = {1662-5161}
}


@misc{Cade2018,
title  = {Big Bets on A.I. Open a New Frontier for Chip Start-Ups, Too},
author = {Cade Metz},
year   = {2018},
url    = {https://www.nytimes.com/2018/01/14/technology/artificial-intelligence-chip-start-ups.html}
}

@inproceedings{CameronJones1995InstanceSB,
title  = {Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing},
author = {R. Mike Cameron-Jones},
year   = {1995}
}





@article{Cha2007,
author  = {Cha, Sung-Hyuk},
year    = {2007},
month   = {01},
pages   = {},
title   = {Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions},
volume  = {1},
journal = {Int. J. Math. Model. Meth. Appl. Sci.}
}


@inproceedings{chen2018gradnorm,
title        = {Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
author       = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
booktitle    = {International Conference on Machine Learning},
pages        = {794--803},
year         = {2018},
organization = {PMLR}
}

@inproceedings{Chierichetti2010,
author  = {Chierichetti, Flavio and Kumar, Ravi and Pandey, Sandeep and Vassilvitskii, Sergei},
year    = {2010},
month   = {01},
pages   = {293-311},
title   = {Finding the Jaccard Median},
journal = {Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms},
doi     = {10.1137/1.9781611973075.25}
}

@article{chouldechova2017fair,
title     = {Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
author    = {Chouldechova, Alexandra},
journal   = {Big data},
volume    = {5},
number    = {2},
pages     = {153--163},
year      = {2017},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@article{ciresan2011,
author  = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year    = {2011},
month   = {07},
pages   = {1237-1242},
title   = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
doi     = {10.5591/978-1-57735-516-8/IJCAI11-210}
}

@article{Ciresan2011,
author        = {Dan C. Ciresan and
             Ueli Meier and
             Jonathan Masci and
             Luca Maria Gambardella and
             J{\"{u}}rgen Schmidhuber},
title         = {High-Performance Neural Networks for Visual Object Classification},
journal       = {CoRR},
volume        = {abs/1102.0183},
year          = {2011},
url           = {http://arxiv.org/abs/1102.0183},
archiveprefix = {arXiv},
eprint        = {1102.0183},
timestamp     = {Mon, 13 Aug 2018 16:47:27 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1102-0183},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{claudiu2010,
author        = {{Claudiu Ciresan}, Dan and {Meier}, Ueli and {Gambardella}, Luca Maria and
{Schmidhuber}, Juergen},
title         = {{Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
year          = 2010,
month         = mar,
eid           = {arXiv:1003.0358},
pages         = {arXiv:1003.0358},
archiveprefix = {arXiv},
eprint        = {1003.0358},
primaryclass  = {cs.NE},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2010arXiv1003.0358C},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{clevert2015fast,
title   = {Fast and accurate deep network learning by exponential linear units (elus)},
author  = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
journal = {arXiv preprint arXiv:1511.07289},
year    = {2015}
}

@article{CLINTWHALEY20013,
title    = {Automated empirical optimizations of software and the ATLAS project},
journal  = {Parallel Computing},
volume   = {27},
number   = {1},
pages    = {3 - 35},
year     = {2001},
note     = {New Trends in High Performance Computing},
issn     = {0167-8191},
doi      = {https://doi.org/10.1016/S0167-8191(00)00087-9},
url      = {http://www.sciencedirect.com/science/article/pii/S0167819100000879},
author   = {R. {Clint Whaley} and Antoine Petitet and Jack J. Dongarra},
keywords = {ATLAS, BLAS, Portable performance, AEOS}
}

@article{CollinsK14,
author        = {Maxwell D. Collins and
             Pushmeet Kohli},
title         = {Memory Bounded Deep Convolutional Networks},
journal       = {CoRR},
volume        = {abs/1412.1442},
year          = {2014},
url           = {http://arxiv.org/abs/1412.1442},
archiveprefix = {arXiv},
eprint        = {1412.1442},
timestamp     = {Mon, 13 Aug 2018 16:47:16 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/CollinsK14},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Colwell2013,
author    = {R. {Colwell}},
booktitle = {2013 IEEE Hot Chips 25 Symposium (HCS)},
title     = {The chip design game at the end of Moore's law},
year      = {2013},
volume    = {},
number    = {},
pages     = {1-16}
}

@misc{computerhistorymuseum,
title  = {Moore's Law},
author = {CHM},
year   = {2020},
url    = {https://www.computerhistory.org/revolution/digital-logic/12/267}
}

@article{Cong2011,
author  = {J. {Cong} and V. {Sarkar} and G. {Reinman} and A. {Bui}},
journal = {IEEE Design   Test of Computers},
title   = {Customizable Domain-Specific Computing},
year    = {2011},
volume  = {28},
number  = {2},
pages   = {6-15}
}


@inproceedings{Corinna2016,
title     = {Learning with Rejection},
author    = {Corinna Cortes and Giulia DeSalvo and Mehryar Mohri},
booktitle = {ALT},
year      = {2016}
}



@misc{cotter2018twoplayer,
title         = {Two-Player Games for Efficient Non-Convex Constrained Optimization},
author        = {Andrew Cotter and Heinrich Jiang and Karthik Sridharan},
year          = {2018},
eprint        = {1804.06500},
archiveprefix = {arXiv},
primaryclass  = {cs.LG}
}

@misc{DARPA018,
title  = {DARPA Announces Next Phase of Electronics Resurgence Initiative},
author = {DARPA},
year   = {2018},
url    = {https://www.darpa.mil/news-events/2018-11-01a}
}



@article{DBLP_Sze,
author        = {Vivienne Sze and
             Yu{-}Hsin Chen and
             Tien{-}Ju Yang and
             Joel S. Emer},
title         = {Efficient Processing of Deep Neural Networks: {A} Tutorial and Survey},
journal       = {CoRR},
volume        = {abs/1703.09039},
year          = {2017},
url           = {http://arxiv.org/abs/1703.09039},
archiveprefix = {arXiv},
eprint        = {1703.09039}
}

@article{DBLPKornblith,
author        = {Simon Kornblith and
             Jonathon Shlens and
             Quoc V. Le},
title         = {Do Better ImageNet Models Transfer Better?},
journal       = {CoRR},
volume        = {abs/1805.08974},
year          = {2018},
url           = {http://arxiv.org/abs/1805.08974},
archiveprefix = {arXiv},
eprint        = {1805.08974},
timestamp     = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1805-08974},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{Dean202011TD,
title   = {1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
author  = {Jeffrey Dean},
journal = {2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
year    = {2020},
pages   = {8-14}
}

@article{deep-learning-scaling,
author  = {Joel Hestness and
             Sharan Narang and
             Newsha Ardalani and
             Gregory F. Diamos and
             Heewoo Jun and
             Hassan Kianinejad and
             Md. Mostofa Ali Patwary and
             Yang Yang and
             Yanqi Zhou},
title   = {Deep Learning Scaling is Predictable, Empirically},
journal = {CoRR},
volume  = {abs/1712.00409},
year    = {2017}
}


@article{deep-rewiring,
author  = {Guillaume Bellec and
             David Kappel and
             Wolfgang Maass and
             Robert A. Legenstein},
title   = {Deep {R}ewiring: {T}raining {V}ery {S}parse {D}eep {N}etworks},
journal = {CoRR},
volume  = {abs/1711.05136},
year    = {2017}
}


@misc{Demuth93neuralnetwork,
author = {Howard Demuth and Mark Beale},
title  = {Neural Network Toolbox For Use with Matlab - User Guide Verion 3.0},
year   = {1993}
}

@article{demvsar2006statistical,
title   = {Statistical comparisons of classifiers over multiple data sets},
author  = {Dem{\v{s}}ar, Janez},
journal = {Journal of Machine learning research},
volume  = {7},
number  = {Jan},
pages   = {1--30},
year    = {2006}
}

@article{dettmers2019sparse,
title   = {Sparse networks from scratch: Faster training without losing performance},
author  = {Dettmers, Tim and Zettlemoyer, Luke},
journal = {arXiv preprint arXiv:1907.04840},
year    = {2019}
}


@misc{Dettmers2020,
title  = {Which GPU for deep learning?},
author = {Tim Dettmers},
year   = {2020},
url    = {https://bit.ly/35qq8xe}
}


@article{DeVries2019,
author  = {Terrance DeVries and
            Ishan Misra and
            Changhan Wang and
            Laurens van der Maaten},
title   = {Does Object Recognition Work for Everyone?},
journal = {CoRR},
volume  = {abs/1906.02659},
year    = {2019}
}

@book{diamond98,
title     = {Guns, Germs, and Steel: The Fates of Human Societies},
author    = {Diamond, J.M. and Diamond, P.G.J. and Bernard Hames Collection},
isbn      = {9780393317558},
lccn      = {96037068},
series    = {National bestseller / W.W. Norton \& Company},
url       = {https://books.google.com/books?id=1lBu\_bqSsSMC},
year      = {1999},
publisher = {W.W. Norton}
}



@article{domingo_1995,
author = {Domingos, Pedro},
year   = {1995},
month  = {05},
pages  = {},
title  = {Rule Induction and Instance-Based Learning: A Unified Approach}
}

@misc{Dong2019,
author = {Zhen, Dong and Yao, Zhewei and Gholami, Amir and Mahoney, Michael and Keutzer, Kurt},
year   = {2019},
month  = {10},
pages  = {293-302},
title  = {HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision},
doi    = {10.1109/ICCV.2019.00038}
}
@article{dropout-journal,
author  = {Nitish Srivastava and
             Geoffrey E. Hinton and
             Alex Krizhevsky and
             Ilya Sutskever and
             Ruslan Salakhutdinov},
title   = {Dropout: a simple way to prevent neural networks from overfitting},
journal = {Journal of Machine Learning Research},
volume  = {15},
number  = {1},
pages   = {1929--1958},
year    = {2014}
}



@mastersthesis{dubowski2020activation,
title  = {Activation function impact on Sparse Neural Networks},
author = {Dubowski, Adam},
type   = {{B.S.} thesis},
year   = {2020},
school = {University of Twente}
}

@article{duchi2011adaptive,
title   = {Adaptive subgradient methods for online learning and stochastic optimization.},
author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
journal = {Journal of machine learning research},
volume  = {12},
number  = {7},
year    = {2011}
}

@book{DudaHart2nd,
author    = {R. O. Duda and P. E. Hart and D. G. Stork},
title     = {Pattern Classification},
publisher = {John Wiley and Sons},
edition   = {2nd},
year      = {2000}
}



@article{dwayne2001,
author  = {Dwayne Moore},
title   = {The Anna Karenina Principle Applied to Ecological Risk Assessments of Multiple Stressors},
journal = {Human and Ecological Risk Assessment: An International Journal},
volume  = {7},
number  = {2},
pages   = {231-237},
year    = {2001},
doi     = {10.1080/20018091094349}
}


@article{Eagleman2036,
author    = {Eagleman, David M. and Sejnowski, Terrence J.},
title     = {Motion Integration and Postdiction in Visual Awareness},
volume    = {287},
number    = {5460},
pages     = {2036--2038},
year      = {2000},
doi       = {10.1126/science.287.5460.2036},
publisher = {American Association for the Advancement of Science},
issn      = {0036-8075},
url       = {https://science.sciencemag.org/content/287/5460/2036},
eprint    = {https://science.sciencemag.org/content/287/5460/2036.full.pdf},
journal   = {Science}
}

@inproceedings{eldan2016power,
title     = {The power of depth for feedforward neural networks},
author    = {Eldan, Ronen and Shamir, Ohad},
booktitle = {Conference on learning theory},
pages     = {907--940},
year      = {2016}
}

@inproceedings{Elsen_2020_CVPR,
author    = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
title     = {Fast Sparse ConvNets},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2020}
}

@article{evci2020gradient,
title   = {Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win},
author  = {Evci, Utku and Ioannou, Yani A and Keskin, Cem and Dauphin, Yann},
journal = {arXiv preprint arXiv:2010.03533},
year    = {2020}
}

@article{false_discovery_rates,
author  = {Ridgeway, Greg and MacDonald, John},
year    = {2009},
month   = {06},
pages   = {661-668},
title   = {Doubly Robust Internal Benchmarking and False Discovery Rates for Detecting Racial Bias in Police Stops},
volume  = {104},
journal = {Journal of the American Statistical Association},
doi     = {10.1198/jasa.2009.0034}
}



@inproceedings{Fatahalian2004,
author    = {Fatahalian, K. and Sugerman, J. and Hanrahan, P.},
title     = {Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication},
year      = {2004},
isbn      = {3905673150},
publisher = {Association for Computing Machinery},
address   = {New York, NY, USA},
url       = {https://doi.org/10.1145/1058129.1058148},
doi       = {10.1145/1058129.1058148},
booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
pages     = {133–137},
numpages  = {5},
location  = {Grenoble, France},
series    = {HWWS ’04}
}

@article{fbnet,
author  = {Bichen Wu and
             Xiaoliang Dai and
             Peizhao Zhang and
             Yanghan Wang and
             Fei Sun and
             Yiming Wu and
             Yuandong Tian and
             Peter Vajda and
             Yangqing Jia and
             Kurt Keutzer},
title   = {FBNet: {H}ardware-{A}ware {E}fficient {C}onv{N}et {D}esign via {D}ifferentiable
             Neural Architecture Search},
journal = {CoRR},
volume  = {abs/1812.03443},
year    = {2018},
url     = {http://arxiv.org/abs/1812.03443}
}


@inproceedings{feldman2015certifying,
title     = {Certifying and removing disparate impact},
author    = {Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
booktitle = {proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
pages     = {259--268},
year      = {2015}
}

@article{feldman2019does,
title   = {Does learning require memorization? A short tale about a long tail},
author  = {Feldman, Vitaly},
journal = {arXiv preprint arXiv:1906.05271},
year    = {2019}
}






@article{Florenta2014,
author  = {Teodoridis, Florenta},
year    = {2014},
month   = {01},
pages   = {},
title   = {Generalists, Specialists, and the Direction of Inventive Activity},
journal = {SSRN Electronic Journal},
doi     = {10.2139/ssrn.2541383}
}


@inproceedings{Fong2017,
author    = {Ruth C. Fong and
             Andrea Vedaldi},
title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
booktitle = {{ICCV}},
pages     = {3449--3457},
publisher = {{IEEE} Computer Society},
year      = 2017
}





@article{frankle2018lottery,
title   = {The lottery ticket hypothesis: Finding sparse, trainable neural networks},
author  = {Frankle, Jonathan and Carbin, Michael},
journal = {arXiv preprint arXiv:1803.03635},
year    = {2018}
}



@article{frankle2019linear,
title   = {Linear mode connectivity and the lottery ticket hypothesis},
author  = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
journal = {arXiv preprint arXiv:1912.05671},
year    = {2019}
}




@article{FUKUSHIMA1982455,
title   = {Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position},
journal = {Pattern Recognition},
volume  = {15},
number  = {6},
pages   = {455 - 469},
year    = {1982},
issn    = {0031-3203},
url     = {http://www.sciencedirect.com/science/article/pii/0031320382900243},
author  = {Kunihiko Fukushima and Sei Miyake}
}


@inproceedings{Fumera2002,
author    = {Fumera, Giorgio and Roli, Fabio},
title     = {Support Vector Machines with Embedded Reject Option},
booktitle = {Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines},
series    = {SVM '02},
year      = {2002},
isbn      = {3-540-44016-X},
pages     = {68--82},
numpages  = {15},
url       = {http://dl.acm.org/citation.cfm?id=647230.719259},
acmid     = {719259},
publisher = {Springer-Verlag},
address   = {London, UK, UK}
}



@article{funahashi1989approximate,
title     = {On the approximate realization of continuous mappings by neural networks},
author    = {Funahashi, Ken-Ichi},
journal   = {Neural networks},
volume    = {2},
number    = {3},
pages     = {183--192},
year      = {1989},
publisher = {Elsevier}
}


@misc{Gallistel2009,
author = {Gallistel, Charles and King, Adam},
year   = {2009},
month  = {04},
pages  = {288-298},
title  = {Memory and the Computational Brain: Why Cognitive Science Will Transform Neuroscience},
doi    = {10.1002/9781444310498.refs}
}



%% Citation entry on ArXiv
@book{gilder2000telecosm,
title     = {Telecosm: How Infinite Bandwidth Will Revolutionize Our World},
author    = {Gilder, G.},
isbn      = {9780743215947},
url       = {https://books.google.com/books?id=Kzo-KTxdwcEC},
year      = {2000},
publisher = {Free Press}
}

% Citation entry on ArXiv
@inproceedings{glorot2010understanding,
title     = {Understanding the difficulty of training deep feedforward neural networks},
author    = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
pages     = {249--256},
year      = {2010}
}

% Citation entry on Semantic Scholar
@inproceedings{glorot2011deep,
title     = {Deep sparse rectifier neural networks},
author    = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
pages     = {315--323},
year      = {2011}
}

%% Same as above, from arxiv. For hybrid AKA dynamic range quantization.
@book{goodfellow2016deep,
title     = {Deep learning},
author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume    = {1},
year      = {2016},
publisher = {MIT Press}
}

@article{Goodman2016,
author        = {{Goodman}, B. and {Flaxman}, S.},
title         = {{European Union regulations on algorithmic decision-making and a ``right to explanation''}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1606.08813},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Learning},
year          = 2016,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160608813G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Goyal2017,
author        = {{Goyal}, P. and {Doll{\'a}r}, P. and {Girshick}, R. and {Noordhuis}, P. and 
{Wesolowski}, L. and {Kyrola}, A. and {Tulloch}, A. and {Jia}, Y. and 
{He}, K.},
title         = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1706.02677},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
year          = 2017,
month         = jun,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170602677G},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{gradcam2017,
author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month     = {Oct},
year      = 2017
}


@misc{Gray2017GPUKF,
title  = {GPU Kernels for Block-Sparse Weights},
author = {Scott Gray and A. Radford and Diederik P. Kingma},
year   = {2017}
}



@article{Guo2016,
author        = {Yiwen Guo and
             Anbang Yao and
             Yurong Chen},
title         = {Dynamic Network Surgery for Efficient DNNs},
journal       = {CoRR},
volume        = {abs/1608.04493},
year          = {2016},
url           = {http://arxiv.org/abs/1608.04493},
archiveprefix = {arXiv},
eprint        = {1608.04493},
timestamp     = {Mon, 13 Aug 2018 16:48:43 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/GuoYC16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}




@article{gupta2018proxy,
title   = {Proxy fairness},
author  = {Gupta, Maya and Cotter, Andrew and Fard, Mahdi Milani and Wang, Serena},
journal = {arXiv preprint arXiv:1806.11212},
year    = {2018}
}


@inproceedings{hanson1989comparing,
title     = {Comparing biases for minimal network construction with back-propagation},
author    = {Hanson, Stephen Jos{\'e} and Pratt, Lorien Y},
booktitle = {Advances in neural information processing systems},
pages     = {177--185},
year      = {1989}
}

@inproceedings{Hardt2016,
author    = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title     = {Equality of Opportunity in Supervised Learning},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
series    = {NeurIPS'16},
year      = {2016},
isbn      = {978-1-5108-3881-9},
location  = {Barcelona, Spain},
pages     = {3323--3331},
numpages  = {9},
url       = {http://dl.acm.org/citation.cfm?id=3157382.3157469},
acmid     = {3157469},
publisher = {Curran Associates Inc.},
address   = {USA}
}


@inproceedings{hardt2016equality,
title     = {Equality of opportunity in supervised learning},
author    = {Hardt, Moritz and Price, Eric and Srebro, Nati},
booktitle = {Advances in neural information processing systems},
pages     = {3315--3323},
year      = {2016}
}


@manual{Harwell:2019,
title  = {A face-scanning algorithm increasingly decides whether you deserve the job},
author = {Drew Harwell},
url    = {https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/},
year   = {2019 (accessed May 19, 2020)}
}





@book{Hauck2007,
author    = {Hauck, Scott and DeHon, Andre},
title     = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation},
year      = {2007},
isbn      = {9780080556017},
publisher = {Morgan Kaufmann Publishers Inc.},
address   = {San Francisco, CA, USA}
}





@book{Haugeland,
author    = {Haugeland, John},
title     = {Artificial Intelligence: The Very Idea},
year      = {1985},
isbn      = {0262081539},
publisher = {Massachusetts Institute of Technology},
address   = {USA}
}



@inproceedings{he2015delving,
title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages     = {1026--1034},
year      = {2015}
}


@inproceedings{he2016deep,
title     = {Deep residual learning for image recognition},
author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages     = {770--778},
year      = {2016}
}

@article{Heeger1773,
author    = {Heeger, David J.},
title     = {Theory of cortical function},
volume    = {114},
number    = {8},
pages     = {1773--1782},
year      = {2017},
doi       = {10.1073/pnas.1619788114},
publisher = {National Academy of Sciences},
issn      = {0027-8424},
url       = {https://www.pnas.org/content/114/8/1773},
eprint    = {https://www.pnas.org/content/114/8/1773.full.pdf},
journal   = {Proceedings of the National Academy of Sciences}
}



@article{Hendrycksimageneta,
author        = {{Hendrycks}, Dan and {Zhao}, Kevin and {Basart}, Steven and
       {Steinhardt}, Jacob and {Song}, Dawn},
title         = {{Natural Adversarial Examples}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2019,
month         = jul,
eid           = {arXiv:1907.07174},
pages         = {arXiv:1907.07174},
archiveprefix = {arXiv},
eprint        = {1907.07174},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190707174H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@misc{Hennessy2019,
title  = {The End of Moore's Law, CPUs (as we
know them), and the Rise of Domain
Specific Architectures},
author = {John Hennessy},
url    = {https://www.kisacoresearch.com/sites/default/files/presentations/09.00_-_alphabet_-_john_hennessy.pdf},
year   = {2019}
}






@article{Hinton06,
author  = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages   = {1527--1554},
title   = {A Fast Learning Algorithm for Deep Belief Nets},
volume  = {18},
year    = {2006}
}

@book{Hinton1989,
author    = {Hinton, Geoffrey E. and Anderson, J. A.},
title     = {Parallel Models of Associative Memory},
year      = {1989},
isbn      = {080580269X},
publisher = {L. Erlbaum Associates Inc.},
address   = {USA}
}

@article{hinton2012neural,
title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
journal = {Cited on},
volume  = {14},
number  = {8},
year    = {2012}
}

@article{hochreiter1991untersuchungen,
title   = {Untersuchungen zu dynamischen neuronalen Netzen},
author  = {Hochreiter, Sepp},
journal = {Diploma, Technische Universit{\"a}t M{\"u}nchen},
volume  = {91},
number  = {1},
year    = {1991}
}




@misc{hochreiter2001gradient,
title     = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
author    = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
year      = {2001},
publisher = {A field guide to dynamical recurrent neural networks. IEEE Press}
}

@article{holi210171,
author  = {J. L. {Holi} and J. -. {Hwang}},
journal = {IEEE Transactions on Computers},
title   = {Finite precision error analysis of neural network hardware implementations},
year    = {1993},
volume  = {42},
number  = {3},
pages   = {281-290}
}    


inproceedings{Hooker2019ABF,
title     = {A Benchmark for Interpretability Methods in Deep Neural Networks},
author    = {Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
booktitle = {NeurIPS 2019},
year      = {2019},
url       = {https://papers.NeurIPS.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf}
}


@article{hornik1989multilayer,
title   = {Multilayer feedforward networks are universal approximators.},
author  = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
journal = {Neural networks},
volume  = {2},
number  = {5},
pages   = {359--366},
year    = {1989}
}



@misc{HotelSoftwarePF,
title  = {Software Productivity for Extreme-Scale Science},
author = {H. Hotel and H. Johansen and D. Bernholdt and M. H{\'e}roux and R. Hornung},
year   = {2014}
}

@inbook{Howe1994,
author    = {Howe, Denis B.
and Asanovi{\'{c}}, Krste},
title     = {SPACE: Symbolic Processing in Associative Computing Elements},
booktitle = {VLSI for Neural Networks and Artificial Intelligence},
year      = {1994},
publisher = {Springer US},
address   = {Boston, MA},
pages     = {243--252},
isbn      = {978-1-4899-1331-9},
doi       = {10.1007/978-1-4899-1331-9_24},
url       = {https://doi.org/10.1007/978-1-4899-1331-9_24}
}



@article{Hughes2017,
author        = {{Wu}, M. and {Hughes}, M.~C. and {Parbhoo}, S. and {Zazzi}, M. and 
{Roth}, V. and {Doshi-Velez}, F.},
title         = {{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.06178},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171106178W},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Hutchinson2020,
title     = {Social Biases in NLP Models as Barriers for Persons with Disabilities},
author    = {Ben Hutchinson and Vinodkumar Prabhakaran and Emily Denton and Kellie Webster and Yu Zhong and Stephen Craig Denuyl},
year      = {2020},
booktitle = {Proceedings of ACL 2020}
}


@article{inductive_biases,
author        = {Peter W. Battaglia and
Jessica B. Hamrick and
Victor Bapst and
Alvaro Sanchez{-}Gonzalez and
Vin{\'{\i}}cius Flores Zambaldi and
Mateusz Malinowski and
Andrea Tacchetti and
David Raposo and
Adam Santoro and
Ryan Faulkner and
{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
H. Francis Song and
Andrew J. Ballard and
Justin Gilmer and
George E. Dahl and
Ashish Vaswani and
Kelsey R. Allen and
Charles Nash and
Victoria Langston and
Chris Dyer and
Nicolas Heess and
Daan Wierstra and
Pushmeet Kohli and
Matthew Botvinick and
Oriol Vinyals and
Yujia Li and
Razvan Pascanu},
title         = {Relational inductive biases, deep learning, and graph networks},
journal       = {CoRR},
volume        = {abs/1806.01261},
year          = {2018},
url           = {http://arxiv.org/abs/1806.01261},
archiveprefix = {arXiv},
eprint        = {1806.01261},
timestamp     = {Wed, 24 Jul 2019 18:56:21 +0200},
biburl        = {https://dblp.org/rec/journals/corr/abs-1806-01261.bib},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{jeff1990,
title  = {Parallel Implementations of neural network training: two back-propagation approaches.},
author = {Jeff Dean},
url    = {https://drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view},
year   = {1990}
}

@article{jiang2019fantastic,
title   = {Fantastic generalization measures and where to find them},
author  = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
journal = {arXiv preprint arXiv:1912.02178},
year    = {2019}
}

@article{jin2015deep,
title   = {Deep learning with s-shaped rectified linear activation units},
author  = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
journal = {arXiv preprint arXiv:1512.07030},
year    = {2015}
}

@article{Jouppi2017,
author     = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title      = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year       = {2017},
issue_date = {May 2017},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {45},
number     = {2},
issn       = {0163-5964},
url        = {https://doi.org/10.1145/3140659.3080246},
doi        = {10.1145/3140659.3080246},
journal    = {SIGARCH Comput. Archit. News},
month      = jun,
pages      = {1–12},
numpages   = {12},
keywords   = {LSTM, CNN, RNN, TPU, DNN, accelerator, neural network, deep learning, MLP, domain-specific architecture, TensorFlow, GPU}
}

@article{kallus2019assessing,
title   = {Assessing algorithmic fairness with unobserved protected class using data combination},
author  = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
journal = {arXiv preprint arXiv:1906.00285},
year    = {2019}
}


@article{karnin1990simple,
title     = {A simple procedure for pruning back-propagation trained neural networks},
author    = {Karnin, Ehud D},
journal   = {IEEE transactions on neural networks},
volume    = {1},
number    = {2},
pages     = {239--242},
year      = {1990},
publisher = {IEEE}
}




@misc{karpathy2014,
author       = {Karpathy, Andrej},
year         = {2014},
title        = {What I learned from competing against a ConvNet on ImageNet},
howpublished = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}},
note         = {Accessed: 2019-07-07}
}

misc{karpathy2014,
author       = {Karpathy, Andrej},
year         = {2014},
title        = {What I learned from competing against a ConvNet on ImageNet},
howpublished = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}},
note         = {Accessed: 2019-07-07}
}

@inproceedings{kay2015unequal,
title     = {Unequal representation and gender stereotypes in image search results for occupations},
author    = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages     = {3819--3828},
year      = {2015}
}

@phdthesis{kearns89,
author = {M. J. Kearns},
title  = {Computational Complexity of Machine Learning},
school = {Department of Computer Science, Harvard University},
year   = {1989}
}

phdthesis{kearns89,
author = {M. J. Kearns},
title  = {Computational Complexity of Machine Learning},
school = {Department of Computer Science, Harvard University},
year   = {1989}
}

@article{kendall1938new,
title     = {A new measure of rank correlation},
author    = {Kendall, Maurice G},
journal   = {Biometrika},
volume    = {30},
number    = {1/2},
pages     = {81--93},
year      = {1938},
publisher = {JSTOR}
}

@article{Kennedy2000SignalprocessingMA,
title   = {Signal-processing machines at the postsynaptic density.},
author  = {Mary B. Kennedy},
journal = {Science},
year    = {2000},
volume  = {290 5492},
pages   = {
750-4
}
}

@misc{keras_pruning,
author       = {{Keras TensorFlow}},
title        = {Keras Tensorflow Magnitude Pruning Open Source Code},
howpublished = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}},
note         = {Accessed: 2019-07-10}
}

misc{keras_pruning,
author       = {{Keras TensorFlow}},
title        = {Keras Tensorflow Magnitude Pruning Open Source Code},
howpublished = {\url{https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras}},
note         = {Accessed: 2019-07-10}
}


@article{kim2017,
author        = {{Kim}, B. and {Wattenberg}, M. and {Gilmer}, J. and {Cai}, C. and 
{Wexler}, J. and {Viegas}, F. and {Sayres}, R.},
title         = {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.11279},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171111279K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{kingma2014adam,
title   = {Adam: A method for stochastic optimization},
author  = {Kingma, Diederik P and Ba, Jimmy},
journal = {arXiv preprint arXiv:1412.6980},
year    = {2014}
}

@misc{kingsburyhipnet,
author = {Kingsbury, Brian and Morgan, Nelson and Wawrzynek, John},
year   = {1998},
month  = {03},
pages  = {},
title  = {HiPNeT-1: A Highly Pipelined Architecture for Neural Network Training}
}

@book{kipling1899,
author        = { Kipling, Rudyard},
title         = { American notes / by Rudyard Kipling },
publisher     = { Brown and Company Boston },
pages         = { 1 online resource (137 pages, 1 unnumbered leaf of plates) : },
year          = { 1899 },
type          = { Book, Online },
language      = { English },
subjects      = { United States -- Description and travel.; United States -- Social life and customs -- 1865-1918. },
life-dates    = { 1899 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn6847688 }
}




@article{kitty_paper2017,
author        = {{Kindermans}, P.-J. and {Hooker}, S. and {Adebayo}, J. and {Alber}, M. and 
{Sch{\"u}tt}, K.~T. and {D{\"a}hne}, S. and {Erhan}, D. and 
{Kim}, B.},
title         = {{The (Un)reliability of saliency methods}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1711.00867},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Learning},
year          = 2017,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171100867K},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{kleinberg2016inherent,
title   = {Inherent trade-offs in the fair determination of risk scores},
author  = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
journal = {arXiv preprint arXiv:1609.05807},
year    = {2016}
}


@inproceedings{koh2017,
title     = {Understanding Black-box Predictions via Influence Functions},
author    = {Pang Wei Koh and Percy Liang},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
pages     = {1885--1894},
year      = {2017},
editor    = {Doina Precup and Yee Whye Teh},
volume    = {70},
series    = {Proceedings of Machine Learning Research},
address   = {International Convention Centre, Sydney, Australia},
month     = {06--11 Aug},
publisher = {PMLR},
pdf       = {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf}
}

@article{kornblith2018,
author        = {Simon Kornblith and
Jonathon Shlens and
Quoc V. Le},
title         = {Do Better ImageNet Models Transfer Better?},
journal       = {CoRR},
volume        = {abs/1805.08974},
year          = {2018},
url           = {http://arxiv.org/abs/1805.08974},
archiveprefix = {arXiv},
eprint        = {1805.08974},
timestamp     = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl        = {https://dblp.org/rec/journals/corr/abs-1805-08974.bib},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Kornblith2018DoBI,
title   = {Do Better ImageNet Models Transfer Better?},
author  = {Simon Kornblith and Jonathon Shlens and Quoc V. Le},
journal = {ArXiv},
year    = {2018},
volume  = {abs/1805.08974}
}


@article{Kriegman1853,
author    = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
title     = {A scalable pipeline for designing reconfigurable organisms},
volume    = {117},
number    = {4},
pages     = {1853--1859},
year      = {2020},
doi       = {10.1073/pnas.1910837117},
publisher = {National Academy of Sciences},
issn      = {0027-8424},
url       = {https://www.pnas.org/content/117/4/1853},
eprint    = {https://www.pnas.org/content/117/4/1853.full.pdf},
journal   = {Proceedings of the National Academy of Sciences}
}




@article{krizhevsky2009cifar,
title   = {Cifar-10 and cifar-100 datasets},
author  = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
journal = {URl: https://www. cs. toronto. edu/kriz/cifar. html},
volume  = {6},
pages   = {1},
year    = {2009}
}


@inproceedings{krogh1992simple,
title     = {A simple weight decay can improve generalization},
author    = {Krogh, Anders and Hertz, John A},
booktitle = {Advances in neural information processing systems},
pages     = {950--957},
year      = {1992}
}

@inproceedings{Kubat97addressingthe,
author    = {Miroslav Kubat and Stan Matwin},
title     = {Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
booktitle = {In Proceedings of the Fourteenth International Conference on Machine Learning},
year      = {1997},
pages     = {179--186},
publisher = {Morgan Kaufmann}
}


@misc{Kubota2018,
title  = {China Plans $47$ Billion Fund to Boost Its Semiconductor Industry},
author = {Yoko Kubota},
year   = {2018},
url    = {https://on.wsj.com/32L7Kwn}
}

@book{Kuhn1962,
added-at  = {2011-09-20T15:32:20.000+0200},
address   = {Chicago},
author    = {Kuhn, Thomas S.},
biburl    = {https://www.bibsonomy.org/bibtex/231d37d5bf096e1cf5e893934336464d3/voj},
interhash = {329d7457ec19e3c93e0737215924fdfc},
intrahash = {31d37d5bf096e1cf5e893934336464d3},
keywords  = {paradigm},
publisher = {University of Chicago Press},
timestamp = {2013-01-07T08:31:59.000+0100},
title     = {The Structure of Scientific Revolutions},
year      = 1962
}



@article{l0-regularization,
author  = {Christos Louizos and
             Max Welling and
             Diederik P. Kingma},
title   = {Learning {S}parse {N}eural {N}etworks {t}hrough {L}\({}_{\mbox{0}}\) {R}egularization},
journal = {CoRR},
volume  = {abs/1712.01312},
year    = {2017}
}


@inproceedings{labatie2019characterizing,
title        = {Characterizing well-behaved vs. pathological deep neural networks},
author       = {Labatie, Antoine},
booktitle    = {International Conference on Machine Learning},
pages        = {3611--3621},
year         = {2019},
organization = {PMLR}
}



@inproceedings{Lakshminarayanan2017,
author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title     = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
series    = {NeurIPS'17},
year      = {2017},
isbn      = {978-1-5108-6096-4},
location  = {Long Beach, California, USA},
pages     = {6405--6416},
numpages  = {12},
url       = {http://dl.acm.org/citation.cfm?id=3295222.3295387},
acmid     = {3295387},
publisher = {Curran Associates Inc.},
address   = {USA}
}



@inproceedings{langley00,
author    = {P. Langley},
title     = {Crafting Papers on Machine Learning},
year      = {2000},
pages     = {1207--1216},
editor    = {Pat Langley},
booktitle = {Proceedings of the 17th International Conference
            on Machine Learning (ICML 2000)},
address   = {Stanford, CA},
publisher = {Morgan Kaufmann}
}


@misc{Lapedus2017,
title  = {Foundry Challenges In 2018},
author = {Mark Lapedus},
year   = {2017},
url    = {https://semiengineering.com/foundry-challenges-in-2018/}
}

@article{larus2008spending,
author     = {Larus, James},
title      = {Spending Moore's Dividend},
year       = {2009},
issue_date = {May 2009},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {52},
number     = {5},
issn       = {0001-0782},
url        = {https://doi.org/10.1145/1506409.1506425},
doi        = {10.1145/1506409.1506425},
abstract   = {Multicore computers shift the burden of software performance from chip designers and processor architects to software developers.},
journal    = {Commun. ACM},
month      = may,
pages      = {62–69},
numpages   = {8}
}

@article{lasso,
author  = {Robert Tibshirani},
title   = {Regression {S}hrinkage and {S}election {V}ia the {L}asso},
journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
year    = {1994},
volume  = {58},
pages   = {267--288}
}


@misc{LeCun1989,
author    = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
year      = {1989},
publisher = {MIT Press},
volume    = {1},
number    = {4},
url       = {https://doi.org/10.1162/neco.1989.1.4.541},
pages     = {541–551}
}

@incollection{LeCun1990,
title     = {Optimal Brain Damage},
author    = {{LeCun}, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor    = {D. S. Touretzky},
pages     = {598--605},
year      = {1990},
publisher = {Morgan-Kaufmann},
url       = {http://papers.NeurIPS.cc/paper/250-optimal-brain-damage.pdf}
}


@inproceedings{lecun1990optimal,
title     = {Optimal brain damage},
author    = {LeCun, Yann and Denker, John S and Solla, Sara A},
booktitle = {Advances in neural information processing systems},
pages     = {598--605},
year      = {1990}
}

@article{Lee2011,
author  = {H. {Lee} and K. {Brown} and A. {Sujeeth} and H. {Chafi} and T. {Rompf} and M. {Odersky} and K. {Olukotun}},
journal = {IEEE Micro},
title   = {Implementing Domain-Specific Languages for Heterogeneous Parallel Computing},
year    = {2011},
volume  = {31},
number  = {5},
pages   = {42-53}
}


@misc{Lee2018,
title  = {The next step in Facebook AI hardware infrastructure},
author = {Lee, Kevin  and Wang,Xiaodong},
year   = {2018},
url    = {https://bit.ly/3bgZFDn}
}


@article{lee2018snip,
title   = {Snip: Single-shot network pruning based on connection sensitivity},
author  = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
journal = {arXiv preprint arXiv:1810.02340},
year    = {2018}
}

@inproceedings{lee2018training,
title     = {Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},
author    = {Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
booktitle = {International Conference on Learning Representations},
year      = {2018},
url       = {https://openreview.net/forum?id=ryiAv2xAZ}
}



@article{lee2019signal,
title   = {A signal propagation perspective for pruning neural networks at initialization},
author  = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
journal = {arXiv preprint arXiv:1906.06307},
year    = {2019}
}

@article{Leisersoneaam9744,
author       = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
title        = {There{\textquoteright}s plenty of room at the Top: What will drive computer performance after Moore{\textquoteright}s law?},
volume       = {368},
number       = {6495},
elocation-id = {eaam9744},
year         = {2020},
doi          = {10.1126/science.aam9744},
publisher    = {American Association for the Advancement of Science},
issn         = {0036-8075},
url          = {https://science.sciencemag.org/content/368/6495/eaam9744},
eprint       = {https://science.sciencemag.org/content/368/6495/eaam9744.full.pdf},
journal      = {Science}
}


@article{lenet,
author   = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
journal  = {Proceedings of the IEEE},
title    = {Gradient-based learning applied to document recognition},
year     = {1998},
volume   = {86},
number   = {11},
pages    = {2278-2324},
keywords = {optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
doi      = {10.1109/5.726791},
issn     = {0018-9219},
month    = {Nov}
}


@inproceedings{liang2018enhancing,
title     = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
author    = {Shiyu Liang and Yixuan Li and R. Srikant},
booktitle = {International Conference on Learning Representations},
year      = {2018},
url       = {https://openreview.net/forum?id=H1VGkIxRZ}
}




@article{LILLICRAP201982,
title   = {Backpropagation through time and the brain},
journal = {Current Opinion in Neurobiology},
volume  = {55},
pages   = {82 - 89},
year    = {2019},
note    = {Machine Learning, Big Data, and Neuroscience},
issn    = {0959-4388},
doi     = {https://doi.org/10.1016/j.conb.2019.01.011},
url     = {http://www.sciencedirect.com/science/article/pii/S0959438818302009},
author  = {Timothy P Lillicrap and Adam Santoro}
}


@article{Lin1004,
author    = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
title     = {All-optical machine learning using diffractive deep neural networks},
volume    = {361},
number    = {6406},
pages     = {1004--1008},
year      = {2018},
doi       = {10.1126/science.aat8084},
publisher = {American Association for the Advancement of Science},
issn      = {0036-8075},
url       = {https://science.sciencemag.org/content/361/6406/1004},
eprint    = {https://science.sciencemag.org/content/361/6406/1004.full.pdf},
journal   = {Science}
}

@inproceedings{Lindsey1994,
author       = {Lindsey, Clark S. and Lindblad, Thomas},
title        = {{Review of hardware neural networks: A User's perspective}},
booktitle    = {{3rd Workshop on Neural Networks: From Biology to High-energy Physics}},
reportnumber = {TRITA-FYS-9012},
pages        = {0215--224},
month        = {9},
year         = {1994}
}


@article{Linnainmaa1976TaylorEO,
title   = {Taylor expansion of the accumulated rounding error},
author  = {Seppo Linnainmaa},
journal = {BIT Numerical Mathematics},
year    = {1976},
volume  = {16},
pages   = {146-160}
}

@misc{lispcode,
title  = {Course: 15-880(A) -- Introduction to Neural Networks},
author = {Dave Touretzky and Alex Waibel},
year   = {1995},
url    = {shorturl.at/evKX9}
}



@article{liu2020finding,
title   = {Finding trainable sparse networks through Neural Tangent Transfer},
author  = {Liu, Tianlin and Zenke, Friedemann},
journal = {arXiv preprint arXiv:2006.08228},
year    = {2020}
}

@inproceedings{LiuLWT15,
added-at  = {2018-10-09T00:00:00.000+0200},
author    = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
biburl    = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
booktitle = {ICCV},
crossref  = {conf/iccv/2015},
ee        = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
interhash = {3f735aaa11957e73914bbe2ca9d5e702},
intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},
isbn      = {978-1-4673-8391-2},
keywords  = {dblp},
pages     = {3730-3738},
publisher = {IEEE Computer Society},
timestamp = {2018-10-11T11:43:28.000+0200},
title     = {Deep Learning Face Attributes in the Wild.},
url       = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
year      = 2015
}



@article{loshchilov2017decoupled,
title   = {Decoupled weight decay regularization},
author  = {Loshchilov, Ilya and Hutter, Frank},
journal = {arXiv preprint arXiv:1711.05101},
year    = {2017}
}

@article{lottery-ticket-hypothesis,
author  = {Jonathan Frankle and
             Michael Carbin},
title   = {The {L}ottery {T}icket {H}ypothesis: {T}raining {P}runed {N}eural {N}etworks},
journal = {CoRR},
volume  = {abs/1803.03635},
year    = {2018},
url     = {http://arxiv.org/abs/1803.03635}
}






@article{LRP2016,
author    = {Avanti Shrikumar and
             Peyton Greenside and
             Anna Shcherbina and
             Anshul Kundaje},
title     = {Not Just a Black Box: Learning Important Features Through Propagating
             Activation Differences},
journal   = {CoRR},
volume    = {abs/1605.01713},
year      = 2016,
url       = {http://arxiv.org/abs/1605.01713},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShrikumarGSK16},
bibsource = {dblp computer science bibliography, http://dblp.org}
}


@inproceedings{lu2017expressive,
title     = {The expressive power of neural networks: A view from the width},
author    = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
booktitle = {Advances in neural information processing systems},
pages     = {6231--6239},
year      = {2017}
}

@misc{lucas1991,
author  = {Lucas, Peter and van der Gaag, Linda},
title   = {Principles of Expert Systems},
year    = {1991},
address = {USA}
}


@misc{lush2002,
title    = {Technical report: Lush reference manual, code available at http://lush.sourceforge.net},
author   = {Yann Lecun and Leon Bottou},
year     = {2002},
language = {English (US)},
type     = {Other}
}



@book{MachineLearningI,
editor    = {R. S. Michalski and J. G. Carbonell and T.
    M. Mitchell},
title     = {Machine Learning: An Artificial Intelligence
    Approach, Vol. I},
publisher = {Tioga},
year      = {1983},
address   = {Palo Alto, CA}
}


@article{macia2014,
author  = {Macía, Javier and Sole, Ricard},
year    = {2014},
month   = {02},
pages   = {e81248},
title   = {How to Make a Synthetic Multicellular Computer},
volume  = {9},
journal = {PloS one},
doi     = {10.1371/journal.pone.0081248}
}

@inproceedings{madaan2018analyze,
title     = {Analyze, detect and remove gender stereotyping from bollywood movies},
author    = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
booktitle = {Conference on Fairness, Accountability and Transparency},
pages     = {92--105},
year      = {2018}
}

@inproceedings{malsburg_1986,
author    = {Van Der Malsburg, C.},
editor    = {Palm, G{\"u}nther
and Aertsen, Ad},
title     = {Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
booktitle = {Brain Theory},
year      = {1986},
publisher = {Springer Berlin Heidelberg},
address   = {Berlin, Heidelberg},
pages     = {245--248},
abstract  = {Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.},
isbn      = {978-3-642-70911-1}
}

@article{Marblestone2016,
author  = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
title   = {Toward an Integration of Deep Learning and Neuroscience},
journal = {Frontiers in Computational Neuroscience},
volume  = {10},
pages   = {94},
year    = {2016},
url     = {https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
doi     = {10.3389/fncom.2016.00094},
issn    = {1662-5188}
}

@article{marcus2014,
title   = {The atoms of neural computation},
author  = {Gary Marcus and Adam Marblestone and Tom Dean},
year    = {2014},
note    = {Computational Neuroscience},
journal = {Science},
pages   = {551-552},
volume  = {346}
}


@misc{MCCLOSKEY1989109,
title     = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor    = {Gordon H. Bower},
series    = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume    = {24},
pages     = {109 - 165},
year      = {1989},
issn      = {0079-7421},
doi       = {https://doi.org/10.1016/S0079-7421(08)60536-8},
author    = {Michael McCloskey and Neal J. Cohen}
}

@inproceedings{mccoy_etal_2019_right,
title     = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
author    = {McCoy, Tom  and
    Pavlick, Ellie  and
    Linzen, Tal},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
month     = jul,
year      = {2019},
address   = {Florence, Italy},
publisher = {Association for Computational Linguistics},
url       = {https://www.aclweb.org/anthology/P19-1334},
doi       = {10.18653/v1/P19-1334},
pages     = {3428--3448},
abstract  = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.}
}



@book{mcdonald2009handbook,
title     = {Handbook of biological statistics},
author    = {McDonald, John H},
volume    = {2},
year      = {2009},
publisher = {sparky house publishing Baltimore, MD}
}


@article{Mernik2005,
author     = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
title      = {When and How to Develop Domain-Specific Languages},
year       = {2005},
issue_date = {December 2005},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {37},
number     = {4},
issn       = {0360-0300},
url        = {https://doi.org/10.1145/1118890.1118892},
doi        = {10.1145/1118890.1118892},
journal    = {ACM Comput. Surv.},
month      = dec,
pages      = {316–344},
numpages   = {29},
keywords   = {language development system, domain analysis, application language, Domain-specific language}
}


@inproceedings{merrill2016merge,
title        = {Merge-based parallel sparse matrix-vector multiplication},
author       = {Merrill, Duane and Garland, Michael},
booktitle    = {SC'16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages        = {678--689},
year         = {2016},
organization = {IEEE}
} 
[download]

@misc{micikevicius2017mixed,
title         = {Mixed Precision Training},
author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
year          = {2017},
eprint        = {1710.03740},
archiveprefix = {arXiv},
primaryclass  = {cs.AI}
}


@misc{mirhoseini2020chip,
title         = {Chip Placement with Deep Reinforcement Learning},
author        = {Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Jiang and Ebrahim Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Sungmin Bae and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and William Hang and Emre Tuncer and Anand Babu and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
year          = {2020},
eprint        = {2004.10746},
archiveprefix = {arXiv},
primaryclass  = {cs.LG}
}

@article{MISRA2010239,
title   = {Artificial neural networks in hardware: A survey of two decades of progress},
journal = {Neurocomputing},
volume  = {74},
number  = {1},
pages   = {239 - 255},
year    = {2010},
note    = {Artificial Brains},
issn    = {0925-2312},
doi     = {https://doi.org/10.1016/j.neucom.2010.03.021},
url     = {http://www.sciencedirect.com/science/article/pii/S092523121000216X},
author  = {Janardan Misra and Indranil Saha}
}


@inproceedings{mitchell2019model,
title     = {Model cards for model reporting},
author    = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
booktitle = {Proceedings of the conference on fairness, accountability, and transparency},
pages     = {220--229},
year      = {2019}
} 
[download]

@techreport{mitchell80,
author      = {T. M. Mitchell},
title       = {The Need for Biases in Learning Generalizations},
institution = {Computer Science Department, Rutgers University},
year        = {1980},
address     = {New Brunswick, MA}
}


@article{mobilenetv2,
author  = {Mark Sandler and
             Andrew G. Howard and
             Menglong Zhu and
             Andrey Zhmoginov and
             Liang{-}Chieh Chen},
title   = {{I}nverted {R}esiduals and {L}inear {B}ottlenecks: {M}obile {N}etworks for {C}lassification,
             Detection and Segmentation},
journal = {CoRR},
volume  = {abs/1801.04381},
year    = {2018}
}






@article{Montavon2011,
author        = {Montavon, Gregoire and Braun, Mikio L and M{\"u}ller, Klaus-Robert},
date-added    = {2016-10-12 11:13:21 +0000},
date-modified = {2016-10-19 16:36:27 +0000},
journal       = {Journal of Machine Learning Research},
number        = {Sep},
pages         = {2563--2581},
title         = {Kernel analysis of deep networks},
volume        = {12},
year          = 2011
}


@article{Montavon2017,
author        = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
date-modified = {2017-02-10 22:17:33 +0000},
journal       = {Pattern Recognition},
pages         = {211--222},
publisher     = {Elsevier},
title         = {Explaining nonlinear classification decisions with deep taylor decomposition},
volume        = {65},
year          = 2017
}




@article{moore1965,
author  = {Moore, Gordon},
journal = {Electronics},
month   = {April},
number  = {8},
title   = {Cramming more components onto integrated circuits},
url     = {https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf},
volume  = {38},
year    = {1965}
} 

@article{Moravec98whenwill,
author  = {Hans Moravec},
title   = {When will computer hardware match the human brain},
journal = {Journal of Transhumanism},
year    = {1998},
volume  = {1}
}



@article{morgan1983,
author  = {Morgan, M. Granger},
title   = {The fifth generation: Artificial intelligence and Japan's computer challenge to the world, by Edward A. Feigenbaum and Pamela McCorduck. Reading, MA: Addison-Wesley, 1983, 275 pp. Price: \$15.35},
journal = {Journal of Policy Analysis and Management},
volume  = {3},
number  = {1},
pages   = {156-156},
doi     = {10.2307/3324061},
url     = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3324061},
eprint  = {https://onlinelibrary.wiley.com/doi/pdf/10.2307/3324061},
year    = {1983}
}


@article{MORGAN1992248,
title   = {The ring array processor: A multiprocessing peripheral for connectionist applications},
journal = {Journal of Parallel and Distributed Computing},
volume  = {14},
number  = {3},
pages   = {248 - 259},
year    = {1992},
issn    = {0743-7315},
doi     = {https://doi.org/10.1016/0743-7315(92)90067-W},
url     = {http://www.sciencedirect.com/science/article/pii/074373159290067W},
author  = {Nelson Morgan and James Beck and Phil Kohn and Jeff Bilmes and Eric Allman and Joachim Beer}
}

@article{mostafa2019parameter,
title   = {Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
author  = {Mostafa, Hesham and Wang, Xin},
journal = {arXiv preprint arXiv:1902.05967},
year    = {2019}
}

@inproceedings{nair2010rectified,
title     = {Rectified linear units improve restricted boltzmann machines},
author    = {Nair, Vinod and Hinton, Geoffrey E},
booktitle = {ICML},
year      = {2010}
}

@article{namhoon2018,
author        = {Namhoon Lee and
             Thalaiyasingam Ajanthan and
             Philip H. S. Torr},
title         = {{SNIP:} Single-shot Network Pruning based on Connection Sensitivity},
journal       = {CoRR},
volume        = {abs/1810.02340},
year          = {2018},
url           = {http://arxiv.org/abs/1810.02340},
archiveprefix = {arXiv},
eprint        = {1810.02340},
timestamp     = {Tue, 30 Oct 2018 10:49:09 +0100},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-02340},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}




@article{neal1992connectionist,
title     = {Connectionist learning of belief networks},
author    = {Neal, Radford M},
journal   = {Artificial intelligence},
volume    = {56},
number    = {1},
pages     = {71--113},
year      = {1992},
publisher = {Elsevier}
}


@inproceedings{network-slimming,
author    = {Zhuang Liu and
             Jianguo Li and
             Zhiqiang Shen and
             Gao Huang and
             Shoumeng Yan and
             Changshui Zhang},
title     = {Learning {E}fficient {C}onvolutional {N}etworks through {N}etwork {S}limming},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {2755--2763},
year      = {2017}
}


@incollection{Newell81,
author    = {A. Newell and P. S. Rosenbloom},
title     = {Mechanisms of Skill Acquisition and the Law of
                Practice},
booktitle = {Cognitive Skills and Their Acquisition},
pages     = {1--51},
publisher = {Lawrence Erlbaum Associates, Inc.},
year      = {1981},
editor    = {J. R. Anderson},
chapter   = {1},
address   = {Hillsdale, NJ}
}



@inproceedings{NeurIPS1988_119,
title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
author={Mozer, Michael C and Smolensky, Paul},
booktitle={Advances in neural information processing systems},
pages={107--115},
year={1989}
}



@misc{NeurIPS2012_4824,
title     = {ImageNet Classification with Deep Convolutional Neural Networks},
author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
pages     = {1097--1105},
year      = {2012},
url       = {https://bit.ly/2GneDwp}
}

@incollection{NeurIPS2016_6300,
title     = {Examples are not enough, learn to criticize! Criticism for Interpretability},
author    = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {2280--2288},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf}
}


@incollection{NeurIPS2016_6316,
title     = {Satisfying Real-world Goals with Dataset Constraints},
author    = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {2415--2423},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf}
}


@incollection{NeurIPS2016Cortes,
title     = {Boosting with Abstention},
author    = {Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 29},
editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages     = {1660--1668},
year      = {2016},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/6336-boosting-with-abstention.pdf}
}


@misc{NeurIPS2017_6975,
title     = {Dynamic Routing Between Capsules},
author    = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 30},
pages     = {3856--3866},
year      = {2017},
url       = {http://papers.NeurIPS.cc/paper/6975-dynamic-routing-between-capsules.pdf}
}

@incollection{NeurIPS2018_7308,
title     = {Sparse DNNs with Improved Adversarial Robustness},
author    = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
booktitle = {Advances in Neural Information Processing Systems 31},
editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages     = {242--251},
year      = {2018},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf}
}


@incollection{NeurIPS2019_8410,
title     = {Model Compression with Adversarial Robustness: A Unified Optimization Framework},
author    = {Gui, Shupeng and Wang, Haotao N and Yang, Haichuan and Yu, Chen and Wang, Zhangyang and Liu, Ji},
booktitle = {Advances in Neural Information Processing Systems 32},
editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages     = {1285--1296},
year      = {2019},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework.pdf}
}

@incollection{NeurIPSKingma,
title     = {Variational Dropout and the Local Reparameterization Trick},
author    = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 28},
editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages     = {2575--2583},
year      = {2015},
publisher = {Curran Associates, Inc.},
url       = {http://papers.NeurIPS.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf}
}



@article{nocedal2002behavior,
title     = {On the behavior of the gradient norm in the steepest descent method},
author    = {Nocedal, Jorge and Sartenaer, Annick and Zhu, Ciyou},
journal   = {Computational Optimization and Applications},
volume    = {22},
number    = {1},
pages     = {5--35},
year      = {2002},
publisher = {Springer}
}

@article{Novel2009,
author   = {H. A. Fayed and A. F. Atiya},
journal  = {IEEE Transactions on Neural Networks},
title    = {A Novel Template Reduction Approach for the$K$-Nearest Neighbor Method},
year     = {2009},
volume   = {20},
number   = {5},
pages    = {890-896},
keywords = {data reduction;pattern classification;template reduction approach;K-nearest neighbor method;pattern classification;large data set;condensing approach;pattern removal;computational burden;data reduction;Nearest neighbor searches;Prototypes;Pattern classification;Cellular neural networks;Classification algorithms;Satellites;Layout;Medical diagnosis;Design engineering;Mathematics;Condensing;cross validation;editing;$K$-nearest neighbor (KNN);template reduction},
doi      = {10.1109/TNN.2009.2018547},
issn     = {1045-9227},
month    = {May}
}


@misc{nvidia2020,
title  = {NVIDIA Ampere Architecture In-Depth.},
author = {Krashinsky, Ronny and Giroux, Olivier and Jones, Stephen and Stam, Nick and Ramaswamy,Sridhar},
url    = {https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/},
year   = {2020}
}

@article{olah2017feature,
author  = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
title   = {Feature Visualization},
journal = {Distill},
year    = 2017,
note    = {https://distill.pub/2017/feature-visualization},
doi     = {10.23915/distill.00007}
}



@article{Olukotun2014,
author     = {Olukotun, Kunle},
title      = {Beyond Parallel Programming with Domain Specific Languages},
year       = {2014},
issue_date = {August 2014},
publisher  = {Association for Computing Machinery},
address    = {New York, NY, USA},
volume     = {49},
number     = {8},
issn       = {0362-1340},
url        = {https://doi.org/10.1145/2692916.2557966},
doi        = {10.1145/2692916.2557966},
journal    = {SIGPLAN Not.},
month      = feb,
pages      = {179–180},
numpages   = {2},
keywords   = {domain specific languages}
}

@inproceedings{optimal-brain-damage,
author    = {Yann LeCun and
             John S. Denker and
             Sara A. Solla},
title     = {Optimal {B}rain {D}amage},
booktitle = {{NeurIPS}},
pages     = {598--605},
publisher = {Morgan Kaufmann},
year      = {1989}
}


@inproceedings{optimal-brain-surgeon,
author    = {Babak Hassibi and
             David G. Stork},
title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
booktitle = {{NeurIPS}},
pages     = {164--171},
publisher = {Morgan Kaufmann},
year      = {1992}
}


@inproceedings{papadimitriou1980,
author    = {Papadimitriou, Christos H.
and Bentley, Jon Louis},
editor    = {de Bakker, Jaco
and van Leeuwen, Jan},
title     = {A worst-case analysis of nearest neighbor searching by projection},
booktitle = {Automata, Languages and Programming},
year      = {1980}
}




@article{papernot2015,
author        = {{Papernot}, N. and {McDaniel}, P. and {Jha}, S. and {Fredrikson}, M. and 
{Berkay Celik}, Z. and {Swami}, A.},
title         = {{The Limitations of Deep Learning in Adversarial Settings}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1511.07528},
primaryclass  = {cs.CR},
keywords      = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year          = 2015,
month         = nov,
adsurl        = {http://adsabs.harvard.edu/abs/2015arXiv151107528P},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{pascanu2013difficulty,
title     = {On the difficulty of training recurrent neural networks},
author    = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
booktitle = {International conference on machine learning},
pages     = {1310--1318},
year      = {2013}
}


@misc{piaget1954,
title  = {The construction of reality in the child},
author = {Jean Piaget},
year   = {1954}
}


@article{PMID15632230,
title   = {How the brain decides what we see},
author  = {Smythies, John},
doi     = {10.1177/014107680509800106},
number  = {1},
volume  = {98},
month   = {January},
year    = {2005},
journal = {Journal of the Royal Society of Medicine},
issn    = {0141-0768},
pages   = {18—20},
url     = {https://europepmc.org/articles/PMC1079232}
}

@inproceedings{pmlrbuolamwini18a,
title     = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
author    = {Joy Buolamwini and Timnit Gebru},
booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
pages     = {77--91},
year      = {2018},
editor    = {Sorelle A. Friedler and Christo Wilson},
volume    = {81},
series    = {Proceedings of Machine Learning Research},
address   = {New York, NY, USA},
month     = {23--24 Feb},
publisher = {PMLR},
url       = {http://proceedings.mlr.press/v81/buolamwini18a.html}
}



@article{polyak1964some,
title     = {Some methods of speeding up the convergence of iteration methods},
author    = {Polyak, Boris T},
journal   = {USSR Computational Mathematics and Mathematical Physics},
volume    = {4},
number    = {5},
pages     = {1--17},
year      = {1964},
publisher = {Elsevier}
}

@book{posselt1888jacquard,
title     = {The Jacquard Machine Analyzed and Explained: The Preparation of Jacquard Cards and Practical Hints to Learners of Jacquard Designing},
author    = {Posselt, E.A.},
series    = {Posselt's textile library},
url       = {https://books.google.com/books?id=-6FtmgEACAAJ},
year      = {1888},
publisher = {E.A. Posselt}
}

@article{pruning-convnet-nvidia,
author  = {Pavlo Molchanov and
             Stephen Tyree and
             Tero Karras and
             Timo Aila and
             Jan Kautz},
title   = {Pruning {C}onvolutional {N}eural {N}etworks for {R}esource {E}fficient {T}ransfer {L}earning},
journal = {CoRR},
volume  = {abs/1611.06440},
year    = {2016}
}




@inproceedings{quoc2012,
author    = {Le, Quoc V. and Ranzato, Marc’Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
title     = {Building High-Level Features Using Large Scale Unsupervised Learning},
year      = {2012},
isbn      = {9781450312851},
publisher = {Omnipress},
address   = {Madison, WI, USA},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages     = {507–514},
numpages  = {8},
location  = {Edinburgh, Scotland},
series    = {ICML’12}
}

@inproceedings{Raghu2017,
author    = {Maithra Raghu and
             Justin Gilmer and
             Jason Yosinski and
             Jascha Sohl{-}Dickstein},
title     = {{SVCCA:} Singular Vector Canonical Correlation Analysis for Deep Learning
             Dynamics and Interpretability},
booktitle = {{NeurIPS}},
pages     = {6078--6087},
year      = 2017
}






@incollection{RAKIC1994227,
title     = {Synaptic development of the cerebral cortex: implications for learning, memory, and mental illness},
editor    = {J. Van Pelt and M.A. Corner and H.B.M. Uylings and F.H. Lopes Da Silva},
series    = {Progress in Brain Research},
publisher = {Elsevier},
volume    = {102},
pages     = {227 - 243},
year      = {1994},
booktitle = {The Self-Organizing Brain: From Growth Cones to Functional Networks},
issn      = {0079-6123},
doi       = {https://doi.org/10.1016/S0079-6123(08)60543-9},
url       = {http://www.sciencedirect.com/science/article/pii/S0079612308605439},
author    = {Pasko Rakic and Jean-Pierre Bourgeois and Patricia S. Goldman-Rakic},
abstract  = {Publisher Summary
This chapter explores various questions: Are synapses added as we learn? Are there more synapses in some cortical areas than in others? Are there gender differences in synaptic density and do we lose synapses as we age? If we lose synapses with age, what is the timing and rate of this dissolution? To address these issue this chapter present the finding reported in the rhesus monkey. The study of major structural and functional subdivisions of the cortex over the primate lifespan offers a particularly comprehensive view of synapse formation. From study, it is eminently clear that knowledge of the normal course and mechanisms of synapse formation, the influence of various exogenous and endogenous events upon synapse stability and turnover, are essential prerequisites to determining the locus and timing of etiological factors in diseases that affect the cortex and alter cognitive function.}
}


@book{Raymond1990,
author    = {Kurzweil, Raymond},
title     = {The Age of Intelligent Machines},
year      = {1990},
publisher = {MIT Press},
address   = {Cambridge, MA, USA}
}





@article{Recht2019,
author        = {Benjamin Recht and
             Rebecca Roelofs and
             Ludwig Schmidt and
             Vaishaal Shankar},
title         = {Do ImageNet Classifiers Generalize to ImageNet?},
journal       = {CoRR},
volume        = {abs/1902.10811},
year          = {2019},
url           = {http://arxiv.org/abs/1902.10811},
archiveprefix = {arXiv},
eprint        = {1902.10811},
timestamp     = {Tue, 21 May 2019 18:03:38 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1902-10811},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Reddi2020,
author    = {V. J. {Reddi} and C. {Cheng} and D. {Kanter} and P. {Mattson} and G. {Schmuelling} and C. {Wu} and B. {Anderson} and M. {Breughe} and M. {Charlebois} and W. {Chou} and R. {Chukka} and C. {Coleman} and S. {Davis} and P. {Deng} and G. {Diamos} and J. {Duke} and D. {Fick} and J. S. {Gardner} and I. {Hubara} and S. {Idgunji} and T. B. {Jablin} and J. {Jiao} and T. S. {John} and P. {Kanwar} and D. {Lee} and J. {Liao} and A. {Lokhmotov} and F. {Massa} and P. {Meng} and P. {Micikevicius} and C. {Osborne} and G. {Pekhimenko} and A. T. R. {Rajan} and D. {Sequeira} and A. {Sirasao} and F. {Sun} and H. {Tang} and M. {Thomson} and F. {Wei} and E. {Wu} and L. {Xu} and K. {Yamada} and B. {Yu} and G. {Yuan} and A. {Zhong} and P. {Zhang} and Y. {Zhou}},
booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
title     = {MLPerf Inference Benchmark},
year      = {2020},
volume    = {},
number    = {},
pages     = {446-459}
}

@article{reed_1993_pruning_algorithms_survey,
author   = {R. {Reed}},
journal  = {IEEE Transactions on Neural Networks},
title    = {Pruning algorithms-a survey},
year     = {1993},
volume   = {4},
number   = {5},
pages    = {740-747},
keywords = {learning (artificial intelligence);neural nets;neural network pruning algorithms;neural net training;Testing;Training data;Thumb;Neural networks;Multilayer perceptrons;Tree data structures;Sampling methods},
doi      = {10.1109/72.248452},
issn     = {1045-9227},
month    = {Sep.}
}




@article{rethinking-pruning,
author  = {Zhuang Liu and
             Mingjie Sun and
             Tinghui Zhou and
             Gao Huang and
             Trevor Darrell},
title   = {Rethinking the {V}alue of {N}etwork {P}runing},
journal = {CoRR},
volume  = {abs/1810.05270},
year    = {2018}
}



@article{ridgeway2009,
title     = {Doubly robust internal benchmarking and false discovery rates for detecting racial bias in police stops},
author    = {Ridgeway, Greg and MacDonald, John M},
journal   = {Journal of the American Statistical Association},
volume    = {104},
number    = {486},
pages     = {661--668},
year      = {2009},
publisher = {Taylor \& Francis}
}


@inproceedings{rn50,
author    = {Kaiming He and
             Xiangyu Zhang and
             Shaoqing Ren and
             Jian Sun},
title     = {{D}eep {R}esidual {L}earning for {I}mage {R}ecognition},
booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
             {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
pages     = {770--778},
year      = {2016}
}


@article{robbins1951stochastic,
title     = {A stochastic approximation method},
author    = {Robbins, Herbert and Monro, Sutton},
journal   = {The annals of mathematical statistics},
pages     = {400--407},
year      = {1951},
publisher = {JSTOR}
}

@article{rosenblatt1958perceptron,
title     = {The perceptron: a probabilistic model for information storage and organization in the brain.},
author    = {Rosenblatt, Frank},
journal   = {Psychological review},
volume    = {65},
number    = {6},
pages     = {386},
year      = {1958},
publisher = {American Psychological Association}
}

@article{RossFinale2017,
author  = {Andrew Slavin Ross and
             Finale Doshi{-}Velez},
title   = {Improving the Adversarial Robustness and Interpretability of Deep
             Neural Networks by Regularizing their Input Gradients},
journal = {CoRR},
volume  = {abs/1711.09404},
year    = 2017
}


@article{ruder2016overview,
title   = {An overview of gradient descent optimization algorithms},
author  = {Ruder, Sebastian},
journal = {arXiv preprint arXiv:1609.04747},
year    = {2016}
}

@book{rumelhart1987,
editor    = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
title     = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
year      = {1986},
isbn      = {026268053X},
publisher = {MIT Press},
address   = {Cambridge, MA, USA}
}

@inproceedings{runtime-neural-pruning,
author    = {Ji Lin and
             Yongming Rao and
             Jiwen Lu and
             Jie Zhou},
title     = {Runtime Neural Pruning},
booktitle = {{NeurIPS}},
pages     = {2178--2188},
year      = {2017}
} 


@article{Russakovsky2015,
author   = {Russakovsky, Olga
and Deng, Jia
and Su, Hao
and Krause, Jonathan
and Satheesh, Sanjeev
and Ma, Sean
and Huang, Zhiheng
and Karpathy, Andrej
and Khosla, Aditya
and Bernstein, Michael
and Berg, Alexander C.
and Fei-Fei, Li},
title    = {ImageNet Large Scale Visual Recognition Challenge},
journal  = {International Journal of Computer Vision},
year     = {2015},
month    = {Dec},
day      = {01},
volume   = {115},
number   = {3},
pages    = {211--252},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
issn     = {1573-1405},
doi      = {10.1007/s11263-015-0816-y},
url      = {https://doi.org/10.1007/s11263-015-0816-y}
}



@article{Sackinger129422,
author  = {E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal = {IEEE Transactions on Neural Networks},
title   = {Application of the ANNA neural network chip to high-speed character recognition},
year    = {1992},
volume  = {3},
number  = {3},
pages   = {498-505}
}



@article{samek2017,
author   = {W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. Müller},
journal  = {IEEE Transactions on Neural Networks and Learning Systems},
title    = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
year     = 2017,
month    = {Nov},
volume   = {28},
number   = {11},
pages    = {2660-2673},
keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;DNN;ILSVRC2012;MIT Places data sets;SUN397;complex machine learning tasks;data visualization;deconvolution method;deep neural network;heatmap;multilayer nonlinear structure;relevance propagation algorithm;sensitivity-based approach;Algorithm design and analysis;Biological neural networks;Deconvolution;Heating;Learning systems;Neurons;Sensitivity;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
doi      = {10.1109/TNNLS.2016.2599820},
issn     = {2162-237X}
}



@article{Samuel59,
author  = {A. L. Samuel},
title   = {Some Studies in Machine Learning Using the Game of
    Checkers},
journal = {IBM Journal of Research and Development},
year    = {1959},
volume  = {3},
number  = {3},
pages   = {211--229}
}




@article{saxe2013exact,
title   = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
author  = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
journal = {arXiv preprint arXiv:1312.6120},
year    = {2013}
}

@inproceedings{scaling-nmt,
author    = {Myle Ott and
             Sergey Edunov and
             David Grangier and
             Michael Auli},
title     = {Scaling {N}eural {M}achine {T}ranslation},
booktitle = {Proceedings of the Third Conference on Machine Translation: Research
             Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1, 2018},
pages     = {1--9},
year      = {2018}
}



@misc{sehwag2020pruning,
title         = {On Pruning Adversarially Robust Neural Networks},
author        = {Vikash Sehwag and Shiqi Wang and Prateek Mittal and Suman Jana},
year          = {2020},
eprint        = {2002.10509},
archiveprefix = {arXiv},
primaryclass  = {cs.CV}
}

@article{Shalf2020TheFO,
title   = {The future of computing beyond Moore’s Law},
author  = {John Shalf},
journal = {Philosophical Transactions of the Royal Society A},
year    = {2020},
volume  = {378}
}


%Todo differentiate shooker2019 refs 
@article{shooker2019,
author        = {Sara Hooker and
             Dumitru Erhan and
             Pieter{-}Jan Kindermans and
             Been Kim},
title         = {Evaluating Feature Importance Estimates},
journal       = {CoRR},
volume        = {abs/1806.10758},
year          = {2018},
url           = {http://arxiv.org/abs/1806.10758},
archiveprefix = {arXiv},
eprint        = {1806.10758}
}


article{shooker2019,
author        = {{Hooker}, Sara and {Courville}, Aaron and {Dauphin}, Yann and
       {Frome}, Andrea},
title         = {{Selective Brain Damage: Measuring the Disparate Impact of Model Pruning}},
journal       = {arXiv e-prints},
keywords      = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
year          = 2019,
month         = nov,
eid           = {arXiv:1911.05248},
pages         = {arXiv:1911.05248},
archiveprefix = {arXiv},
eprint        = {1911.05248},
primaryclass  = {cs.LG},
adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
} 



@article{singh_article,
author  = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year    = {2015},
month   = {01},
pages   = {Pages 76-85},
title   = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume  = {Volume 58},
journal = {Communications of the ACM},
doi     = {10.1145/2688498.2688499}
}

@article{Singh2015,
author  = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year    = {2015},
month   = {01},
pages   = {Pages 76-85},
title   = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume  = {Volume 58},
journal = {Communications of the ACM},
doi     = {10.1145/2688498.2688499}
}

@inproceedings{lubana2021a,
title     = {A Gradient Flow Framework For Analyzing Network Pruning},
author    = {Ekdeep Singh Lubana and Robert Dick},
booktitle = {International Conference on Learning Representations},
year      = {2021},
url       = {https://openreview.net/forum?id=rumv7QmLUue}
}

@article{singledirection2018,
author        = {{Morcos}, A.~S. and {Barrett}, D.~G.~T. and {Rabinowitz}, N.~C. and 
{Botvinick}, M.},
title         = {{On the importance of single directions for generalization}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1803.06959},
primaryclass  = {stat.ML},
keywords      = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
year          = 2018,
month         = mar,
adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180306959M},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{sparse-connection-1997,
author    = {Nikko Str\"om},
title     = {Sparse {C}onnection and {P}runing in {L}arge {D}ynamic {A}rtificial {N}eural {N}etworks},
booktitle = {EUROSPEECH},
year      = {1997}
}


@article{spelke2007,
author   = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
title    = {Core knowledge},
journal  = {Developmental Science},
volume   = {10},
number   = {1},
pages    = {89-96},
doi      = {10.1111/j.1467-7687.2007.00569.x},
url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00569.x},
eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-7687.2007.00569.x},
abstract = {Abstract Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
year     = {2007}
}


@article{spike-and-slab,
author    = { T. J.   Mitchell  and  J. J.   Beauchamp },
title     = {Bayesian {V}ariable {S}election in {L}inear {R}egression},
journal   = {Journal of the American Statistical Association},
volume    = {83},
number    = {404},
pages     = {1023-1032},
year      = {1988},
publisher = {Taylor & Francis}
}



@article{squeezenet2018,
author        = {Forrest N. Iandola and
             Matthew W. Moskewicz and
             Khalid Ashraf and
             Song Han and
             William J. Dally and
             Kurt Keutzer},
title         = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB
             model size},
journal       = {CoRR},
volume        = {abs/1602.07360},
year          = {2016},
url           = {http://arxiv.org/abs/1602.07360},
archiveprefix = {arXiv},
eprint        = {1602.07360},
timestamp     = {Mon, 13 Aug 2018 16:46:12 +0200},
biburl        = {https://dblp.org/rec/bib/journals/corr/IandolaMAHDK16},
bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{srivastava2015highway,
title   = {Highway networks},
author  = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
journal = {arXiv preprint arXiv:1505.00387},
year    = {2015}
}



@book{stein2004handbook,
title     = {The Handbook of Multisensory Processes},
author    = {Stein, G.C.C.S.B.E. and Calvert, G. and Spence, C. and Spence, D.E.P.C. and Stein, B.E. and Stein, P.C.B.E.},
isbn      = {9780262033213},
lccn      = {2004042612},
series    = {A Bradford book},
url       = {https://books.google.com/books?id=CZS\_yDoFV7AC},
year      = {2004},
publisher = {MIT Press}
}

@incollection{Stevenson2018,
author  = {{Megan}, Stevenson},
title   = {{Assessing Risk Assessment in Action}},
journal = {Minnesota Law Review},
year    = {2018},
volume  = {58},
url     = {https://scholarship.law.umn.edu/mlr/58}
}


@inproceedings{stochastic-backpropagation,
author    = {Danilo Jimenez Rezende and
             Shakir Mohamed and
             Daan Wierstra},
title     = {Stochastic {B}ackpropagation and {A}pproximate {I}nference in {D}eep {G}enerative
             Models},
booktitle = {{ICML}},
series    = {{JMLR} Workshop and Conference Proceedings},
volume    = {32},
pages     = {1278--1286},
publisher = {JMLR.org},
year      = {2014}
}



@inproceedings{stock2018,
author    = {Pierre Stock and
             Moustapha Ciss{\'{e}}},
title     = {ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and
             Uncovering Biases},
booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
             Germany, September 8-14, 2018, Proceedings, Part {VI}},
pages     = {504--519},
year      = {2018},
crossref  = {DBLP:conf/eccv/2018-6},
url       = {https://doi.org/10.1007/978-3-030-01231-1\_31},
doi       = {10.1007/978-3-030-01231-1\_31},
timestamp = {Tue, 14 May 2019 10:00:45 +0200},
biburl    = {https://dblp.org/rec/bib/conf/eccv/StockC18},
bibsource = {dblp computer science bibliography, https://dblp.org}
}





@misc{strubell2019energy,
title         = {Energy and Policy Considerations for Deep Learning in NLP},
author        = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
year          = {2019},
eprint        = {1906.02243},
archiveprefix = {arXiv},
primaryclass  = {cs.CL}
}

@inproceedings{sutskever2013importance,
title     = {On the importance of initialization and momentum in deep learning},
author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle = {International conference on machine learning},
pages     = {1139--1147},
year      = {2013}
}


@article{taiji_1998,
title     = {Grape-4: A Teraflops Machine for N-Body Simulations},
volume    = {11},
doi       = {10.1017/S1539299600018244},
number    = {2},
journal   = {Highlights of Astronomy},
publisher = {Cambridge University Press},
author    = {Taiji, Makoto},
year      = {1998},
pages     = {600–602}
}

@article{tan2007,
author  = {Tan, Cheemeng and Song, Hao and Niemi, Jarad and You, Lingchong},
year    = {2007},
month   = {06},
pages   = {343-53},
title   = {A synthetic biology challenge: Making cells compute},
volume  = {3},
journal = {Molecular bioSystems},
doi     = {10.1039/b618473c}
}

@article{tanaka2020pruning,
title   = {Pruning neural networks without any data by iteratively conserving synaptic flow},
author  = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
journal = {arXiv preprint arXiv:2006.05467},
year    = {2020}
}

@book{tani2016exploring,
title     = {Exploring Robotic Minds: Actions, Symbols, and Consciousness as Self-organizing Dynamic Phenomena},
author    = {Tani, J. and Oxford University Press},
isbn      = {9780190281083},
lccn      = {2016014889},
series    = {Oxford series on cognitive models and architectures},
url       = {https://books.google.com/books?id=QswnnQAACAAJ},
year      = {2016},
publisher = {Oxford University Press}
}

@inproceedings{tartaglione2018learning,
title     = {Learning sparse neural networks via sensitivity-driven regularization},
author    = {Tartaglione, Enzo and Leps{\o}y, Skjalg and Fiandrotti, Attilio and Francini, Gianluca},
booktitle = {Advances in neural information processing systems},
pages     = {3878--3888},
year      = {2018}
}

@article{tensor2tensor,
author  = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and
  Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and
  \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and
  Noam Shazeer and Jakob Uszkoreit},
title   = {Tensor2Tensor for {N}eural {M}achine {T}ranslation},
journal = {CoRR},
volume  = {abs/1803.07416},
year    = {2018},
url     = {http://arxiv.org/abs/1803.07416}
}




@misc{tensorflow2015,
title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url    = {https://www.tensorflow.org/},
note   = {Software available from tensorflow.org},
author = {
  Mart\'{\i}n~Abadi and
  Ashish~Agarwal and
  Paul~Barham and
  Eugene~Brevdo and
  Zhifeng~Chen and
  Craig~Citro and
  Greg~S.~Corrado and
  Andy~Davis and
  Jeffrey~Dean and
  Matthieu~Devin and
  Sanjay~Ghemawat and
  Ian~Goodfellow and
  Andrew~Harp and
  Geoffrey~Irving and
  Michael~Isard and
  Yangqing Jia and
  Rafal~Jozefowicz and
  Lukasz~Kaiser and
  Manjunath~Kudlur and
  Josh~Levenberg and
  Dandelion~Man\'{e} and
  Rajat~Monga and
  Sherry~Moore and
  Derek~Murray and
  Chris~Olah and
  Mike~Schuster and
  Jonathon~Shlens and
  Benoit~Steiner and
  Ilya~Sutskever and
  Kunal~Talwar and
  Paul~Tucker and
  Vincent~Vanhoucke and
  Vijay~Vasudevan and
  Fernanda~Vi\'{e}gas and
  Oriol~Vinyals and
  Pete~Warden and
  Martin~Wattenberg and
  Martin~Wicke and
  Yuan~Yu and
  Xiaoqiang~Zheng},
year   = {2015},
month  = jan
}


@misc{thebitterlesson2019,
title  = {The Bitter Lesson},
author = {Rich Sutton},
year   = {2019},
url    = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@inproceedings{thinet,
author    = {Jian{-}Hao Luo and
             Jianxin Wu and
             Weiyao Lin},
title     = {ThiNet: {A} {F}ilter {L}evel {P}runing {M}ethod for {D}eep {N}eural {N}etwork {C}ompression},
booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
             Italy, October 22-29, 2017},
pages     = {5068--5076},
year      = {2017}
}



@misc{Thompson2018TheDO,
title  = {The Decline of Computers As a General Purpose Technology: Why Deep Learning and the End of Moore’s Law are Fragmenting Computing},
author = {Neil Thompson and Svenja Spanuth},
year   = {2018}
}


@article{tibshirani1996regression,
title     = {Regression shrinkage and selection via the lasso},
author    = {Tibshirani, Robert},
journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume    = {58},
number    = {1},
pages     = {267--288},
year      = {1996},
publisher = {Wiley Online Library}
}

@book{tolstoy2016anna,
title     = {Anna Karenina},
author    = {Tolstoy, L. and Bartlett, R.},
isbn      = {9780198748847},
lccn      = {2015943753},
series    = {Oxford world's classics},
url       = {https://books.google.com/books?id=1DooDwAAQBAJ},
year      = {2016},
publisher = {Oxford University Press}
}

@misc{Torch2002,
author = {Collobert, Ronan and Bengio, Samy and Marithoz, Johnny},
year   = {2002},
month  = {11},
title  = {Torch: A Modular Machine Learning Software Library}
}

@inproceedings{transformer,
author    = {Ashish Vaswani and
             Noam Shazeer and
             Niki Parmar and
             Jakob Uszkoreit and
             Llion Jones and
             Aidan N. Gomez and
             Lukasz Kaiser and
             Illia Polosukhin},
title     = {{A}ttention is {A}ll you {N}eed},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
             on Neural Information Processing Systems 2017, 4-9 December 2017,
             Long Beach, CA, {USA}},
pages     = {6000--6010},
year      = {2017}
}


@article{Tulving2002,
author  = {Tulving, Endel},
title   = {Episodic Memory: From Mind to Brain},
journal = {Annual Review of Psychology},
volume  = {53},
number  = {1},
pages   = {1-25},
year    = {2002},
doi     = {10.1146/annurev.psych.53.100901.135114},
note    = {PMID: 11752477},
url     = { 
      https://doi.org/10.1146/annurev.psych.53.100901.135114
  
},
eprint  = { 
      https://doi.org/10.1146/annurev.psych.53.100901.135114
  
}
}




@inproceedings{variational-dropout,
author    = {Dmitry Molchanov and
             Arsenii Ashukha and
             Dmitry P. Vetrov},
title     = {Variational {D}ropout {S}parsifies {D}eep {N}eural {N}etworks},
booktitle = {Proceedings of the 34th International Conference on Machine Learning,
             {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
pages     = {2498--2507},
year      = {2017}
}



@article{variational-dropout-local-reparameterization,
author  = {Diederik P. Kingma and
             Tim Salimans and
             Max Welling},
title   = {Variational Dropout and the Local Reparameterization Trick},
journal = {CoRR},
volume  = {abs/1506.02557},
year    = {2015}
}



@article{variational-information-bottleneck,
author  = {Bin Dai and
             Chen Zhu and
             David P. Wipf},
title   = {Compressing {N}eural {N}etworks using the {V}ariational {I}nformation {B}ottleneck},
journal = {CoRR},
volume  = {abs/1802.10399},
year    = {2018}
}



@article{Veale2017,
author  = {Michael Veale and Reuben Binns},
title   = {Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data},
journal = {Big Data \& Society},
volume  = {4},
number  = {2},
pages   = {2053951717743530},
year    = {2017},
doi     = {10.1177/2053951717743530},
url     = { 
      https://doi.org/10.1177/2053951717743530
  
},
eprint  = { 
      https://doi.org/10.1177/2053951717743530
  
}
}

@book{villani2008optimal,
title     = {Optimal Transport: Old and New},
author    = {Villani, C.},
isbn      = {9783540710509},
lccn      = {2008932183},
series    = {Grundlehren der mathematischen Wissenschaften},
url       = {https://books.google.com/books?id=hV8o5R7\_5tkC},
year      = {2008},
publisher = {Springer Berlin Heidelberg}
}

@inproceedings{wang2017learning,
title     = {Learning to model the tail},
author    = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
booktitle = {Advances in Neural Information Processing Systems},
pages     = {7029--7039},
year      = {2017}
}

@article{Wang2018HAQHA,
title   = {HAQ: Hardware-Aware Automated Quantization},
author  = {Kuan Wang and Zhijian Liu and Yujun Lin and Ji Lin and Song Han},
journal = {ArXiv},
year    = {2018},
volume  = {abs/1811.08886}
}


@article{wang2020picking,
title   = {Picking winning tickets before training by preserving gradient flow},
author  = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
journal = {arXiv preprint arXiv:2002.07376},
year    = {2020}
}



@article{wang2020robust,
title   = {Robust Optimization for Fairness with Noisy Protected Groups},
author  = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael I},
journal = {arXiv preprint arXiv:2002.09343},
year    = {2020}
}


@inproceedings{wavenet,
author    = {A{\"{a}}ron van den Oord and
             Sander Dieleman and
             Heiga Zen and
             Karen Simonyan and
             Oriol Vinyals and
             Alex Graves and
             Nal Kalchbrenner and
             Andrew W. Senior and
             Koray Kavukcuoglu},
title     = {WaveNet: {A} {Generative} {Model} for {Raw} {Audio}},
booktitle = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
             September 2016},
pages     = {125},
year      = {2016}
}

@misc{welling2019,
title  = {Do we still need models or just more data and compute?},
author = {Max Welling},
year   = {2019},
url    = {shorturl.at/qABIY}
}


@incollection{werbos1982applications,
title     = {Applications of advances in nonlinear sensitivity analysis},
author    = {Werbos, Paul J},
booktitle = {System modeling and optimization},
pages     = {762--770},
year      = {1982},
publisher = {Springer}
}


@inproceedings{wide-resnet,
author    = {Sergey Zagoruyko and
             Nikos Komodakis},
title     = {Wide {R}esidual {N}etworks},
booktitle = {Proceedings of the British Machine Vision Conference 2016, {BMVC}
             2016, York, UK, September 19-22, 2016},
year      = {2016}
}



@misc{wigger2020,
title  = {OpenAI launches an API to commercialize its research},
author = {Kyle Wiggers},
year   = {2020},
url    = {https://bit.ly/31NAJQB}
}

@article{wilcoxon1945individual,
title     = {Individual Comparisons by Ranking Methods},
author    = {Wilcoxon, Frank},
journal   = {Biometrics Bulletin},
volume    = {1},
number    = {6},
pages     = {80--83},
year      = {1945},
publisher = {JSTOR}
}

@article{Wilson2000,
author     = {Wilson, D. Randall and Martinez, Tony R.},
title      = {Reduction Techniques for Instance-BasedLearning Algorithms},
journal    = {Mach. Learn.},
issue_date = {March 2000},
volume     = {38},
number     = {3},
month      = mar,
year       = {2000},
issn       = {0885-6125},
pages      = {257--286},
numpages   = {30},
url        = {https://doi.org/10.1023/A:1007626913721},
doi        = {10.1023/A:1007626913721},
acmid      = {343200},
publisher  = {Kluwer Academic Publishers},
address    = {Hingham, MA, USA},
keywords   = {classification, instance reduction, instance-based learning, nearest neighbor, pruning}
}


@article{xiao2017fashion,
title   = {Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
author  = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
journal = {arXiv preprint arXiv:1708.07747},
year    = {2017}
}




@article{xiao2018dynamical,
title   = {Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks},
author  = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
journal = {arXiv preprint arXiv:1806.05393},
year    = {2018}
}

@article{XIE2019109,
title   = {Automated pulmonary nodule detection in CT images using deep convolutional neural networks},
journal = {Pattern Recognition},
volume  = {85},
pages   = {109 - 119},
year    = {2019},
issn    = {0031-3203},
doi     = {https://doi.org/10.1016/j.patcog.2018.07.031},
url     = {http://www.sciencedirect.com/science/article/pii/S0031320318302711},
author  = {Hongtao Xie and Dongbao Yang and Nannan Sun and Zhineng Chen and Yongdong Zhang}
}

@inproceedings{xie2019exploring,
title     = {Exploring randomly wired neural networks for image recognition},
author    = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
pages     = {1284--1293},
year      = {2019}
}


@misc{Xu2010,
author  = {Xu, Harry and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Sevitsky, Gary},
year    = {2010},
month   = {01},
pages   = {421-426},
title   = {Software bloat analysis: Finding, removing, and preventing performance problems in modern large-scale object-oriented applications},
journal = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
doi     = {10.1145/1882362.1882448}
}

@article{yang2019mean,
title   = {A mean field theory of batch normalization},
author  = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
journal = {arXiv preprint arXiv:1902.08129},
year    = {2019}
}}



@inproceedings{zaheer2018adaptive,
title     = {Adaptive methods for nonconvex optimization},
author    = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Advances in neural information processing systems},
pages     = {9793--9803},
year      = {2018}
}


@article{zeiler2012adadelta,
title   = {Adadelta: an adaptive learning rate method},
author  = {Zeiler, Matthew D},
journal = {arXiv preprint arXiv:1212.5701},
year    = {2012}
}


@inproceedings{Zeiler2014,
author        = {Zeiler, Matthew D and Fergus, Rob},
booktitle     = {European Conference on Computer Vision},
date-added    = {2016-10-12 11:14:28 +0000},
date-modified = {2016-10-12 11:16:57 +0000},
organization  = {Springer},
pages         = {818--833},
title         = {Visualizing and understanding convolutional networks},
year          = 2014
}






@article{zhang2016,
author        = {{Zhang}, J. and {Lin}, Z. and {Brandt}, J. and {Shen}, X. and 
{Sclaroff}, S.},
title         = {{Top-down Neural Attention by Excitation Backprop}},
journal       = {ArXiv e-prints},
archiveprefix = {arXiv},
eprint        = {1608.00507},
primaryclass  = {cs.CV},
keywords      = {Computer Science - Computer Vision and Pattern Recognition},
year          = 2016,
month         = aug,
adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160800507Z},
adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{zhang2016understanding,
title   = {Understanding deep learning requires rethinking generalization},
author  = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
journal = {arXiv preprint arXiv:1611.03530},
year    = {2016}
}

@article{zhang2019dive,
title   = {Dive into deep learning},
author  = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
journal = {Unpublished Draft. Retrieved},
volume  = {19},
pages   = {2019},
year    = {2019}
}


@article{Zhao2017,
author  = {Jieyu Zhao and
            Tianlu Wang and
            Mark Yatskar and
            Vicente Ordonez and
            Kai{-}Wei Chang},
title   = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
            Constraints},
journal = {CoRR},
volume  = {abs/1707.09457},
year    = {2017}
}

@inproceedings{zhao2018bridging,
title     = {Bridging the gap between deep learning and sparse matrix format selection},
author    = {Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng},
booktitle = {Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
pages     = {94--108},
year      = {2018}
}}

@article{Zhu2014,
author = {Zhu, Xiangxin and Anguelov, Dragomir and Ramanan, Deva},
year   = {2014},
month  = {09},
pages  = {915-922},
title  = {Capturing Long-Tail Distributions of Object Subcategories},
doi    = {10.1109/CVPR.2014.122}
}

@article{liebenwein,
author    = {Lucas Liebenwein and
             Cenk Baykal and
             Brandon Carter and
             David Gifford and
             Daniela Rus},
title     = {Lost in Pruning: The Effects of Pruning Neural Networks beyond Test
             Accuracy},
journal   = {CoRR},
volume    = {abs/2103.03014},
year      = {2021},
url       = {https://arxiv.org/abs/2103.03014},
archivePrefix = {arXiv},
eprint    = {2103.03014},
timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03014.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sparse_tensor_core,
author = {Zhu, Maohua and Zhang, Tao and Gu, Zhenyu and Xie, Yuan},
title = {Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-Wise Sparse Neural Networks on Modern GPUs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358269},
doi = {10.1145/3352460.3358269},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {359–371},
numpages = {13},
keywords = {graphics processing units, neural networks, pruning},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@article{rethinking_model_size_li,
author    = {Zhuohan Li and
             Eric Wallace and
             Sheng Shen and
             Kevin Lin and
             Kurt Keutzer and
             Dan Klein and
             Joseph E. Gonzalez},
title     = {Train Large, Then Compress: Rethinking Model Size for Efficient Training
             and Inference of Transformers},
journal   = {ICML},
year      = {2020},
url       = {https://arxiv.org/abs/2002.11794},
}

@article{kaleab2021,
author    = {Kale{-}ab Tessera and
             Sara Hooker and
             Benjamin Rosman},
title     = {Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network
             Optimization},
journal   = {CoRR},
volume    = {abs/2102.01670},
year      = {2021},
url       = {https://arxiv.org/abs/2102.01670},
eprinttype = {arXiv},
eprint    = {2102.01670},
timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-2102-01670.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wang2022,
doi = {10.48550/ARXIV.2206.06479},

url = {https://arxiv.org/abs/2206.06479},

author = {Wang, Serena and Narasimhan, Harikrishna and Zhou, Yichen and Hooker, Sara and Lukasik, Michal and Menon, Aditya Krishna},

keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Robust Distillation for Worst-class Performance},

publisher = {arXiv},

year = {2022},

copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{flores,
title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
year={2021}
}


@ARTICLE{2019arXiv190209574G,
     author = {{Gale}, Trevor and {Elsen}, Erich and {Hooker}, Sara},
      title = "{The State of Sparsity in Deep Neural Networks}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
       year = 2019,
      month = feb,
        eid = {arXiv:1902.09574},
      pages = {arXiv:1902.09574},
archivePrefix = {arXiv},
     eprint = {1902.09574},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190209574G},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{tessera2021gradients,
    title={Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization}, 
    author={Kale-ab Tessera and Sara Hooker and Benjamin Rosman},
    year={2021},
    eprint={2102.01670},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{Hutchinson2020,
title	= {Social Biases in NLP Models as Barriers for Persons with Disabilities},
author	= {Ben Hutchinson and Vinodkumar Prabhakaran and Emily Denton and Kellie Webster and Yu Zhong and Stephen Craig Denuyl},
year	= {2020},
booktitle	= {Proceedings of ACL 2020}
}

@inproceedings{Hardt2016,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of Opportunity in Supervised Learning},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
series = {NeurIPS'16},
year = {2016},
isbn = {978-1-5108-3881-9},
location = {Barcelona, Spain},
pages = {3323--3331},
numpages = {9},
url = {http://dl.acm.org/citation.cfm?id=3157382.3157469},
acmid = {3157469},
publisher = {Curran Associates Inc.},
address = {USA},
} 

@article{Cha2007,
  author = {Cha, Sung-Hyuk},
  year = {2007},
  month = {01},
  pages = {},
  title = {Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions},
  volume = {1},
  journal = {Int. J. Math. Model. Meth. Appl. Sci.}
}


@inproceedings{Chierichetti2010,
author = {Chierichetti, Flavio and Kumar, Ravi and Pandey, Sandeep and Vassilvitskii, Sergei},
year = {2010},
month = {01},
pages = {293-311},
title = {Finding the Jaccard Median},
journal = {Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms},
doi = {10.1137/1.9781611973075.25}
}

@inproceedings{e9ee2143e19d49cf9cbe8861950b6b2a,
  title = "Optimal brain damage",
  author = "Yann Lecun and Denker, {J. S.} and Solla, {Sara A.} and Howard, {R. E.} and L.D. Jackel",
  year = "1990",
  language = "English (US)",
  volume = "2",
  editor = "David Touretzky",
  booktitle = "Advances in Neural Information Processing Systems (NeurIPS 1989), Denver, CO",
  publisher = "Morgan Kaufmann",
}
@article{article,
  author = {Hassibi, Babak and G.Stork, David},
  year = {1992},
  month = {10},
  pages = {},
  title = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
  volume = {5},
  journal = {Adv Neural Inform Proc Syst}
}
@inproceedings{10.5555/2969735.2969756,
  author = {Hanson, Stephen Jos\'{e} and Pratt, Lorien Y.},
  title = {Comparing Biases for Minimal Network Construction with Back-Propagation},
  year = {1988},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {Rumelhart (1987), has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units, (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart's minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general, the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
  booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
  pages = {177–185},
  numpages = {9},
  series = {NeurIPS'88}
}


@misc{narang2017exploring,
    title={Exploring Sparsity in Recurrent Neural Networks}, 
    author={Sharan Narang and Erich Elsen and Gregory Diamos and Shubho Sengupta},
    year={2017},
    eprint={1704.05119},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{molchanov2017variational,
    title={Variational Dropout Sparsifies Deep Neural Networks}, 
    author={Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov},
    year={2017},
    eprint={1701.05369},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@book{Aho:72,
  author  = {Alfred V. Aho and Jeffrey D. Ullman},
  title   = {The Theory of Parsing, Translation and Compiling},
  year    = "1972",
  volume  = "1",
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, NJ}
}

@book{APA:83,
  author  = {{American Psychological Association}},
  title   = {Publications Manual},
  year    = "1983",
  publisher = {American Psychological Association},
  address = {Washington, DC}
}

@article{Chandra:81,
author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
year = "1981",
title = {Alternation},
journal = {Journal of the Association for Computing Machinery},
volume = "28",
number = "1",
pages = "114--133",
doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
title={Scalable training of {L1}-regularized log-linear models},
author={Andrew, Galen and Gao, Jianfeng},
booktitle={Proceedings of the 24th International Conference on Machine Learning},
pages={33--40},
year={2007},
}

@book{Gusfield:97,
  author  = {Dan Gusfield},
  title   = {Algorithms on Strings, Trees and Sequences},
  year    = "1997",
  publisher = {Cambridge University Press},
  address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
  author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal   = {Computing Research Repository},
  volume    = {arXiv:1503.06733},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.06733},
  note    = {version 2}
}

@article{Ando2005,
Acmid = {1194905},
Author = {Ando, Rie Kubota and Zhang, Tong},
Issn = {1532-4435},
Issue_Date = {12/1/2005},
Journal = {Journal of Machine Learning Research},
Month = dec,
Numpages = {37},
Pages = {1817--1853},
Publisher = {JMLR.org},
Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
Volume = {6},
Year = {2005}
}
@misc{han2015learning,
  title={Learning both Weights and Connections for Efficient Neural Networks}, 
  author={Song Han and Jeff Pool and John Tran and William J. Dally},
  year={2015},
  eprint={1506.02626},
  archivePrefix={arXiv},
  primaryClass={cs.NE}
}
@misc{sanh2020movement,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning}, 
  author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
  year={2020},
  eprint={2005.07683},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@InProceedings{TIEDEMANN12.463,
author = {Jörg Tiedemann},
title = {Parallel Data, Tools and Interfaces in OPUS},
booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
year = {2012},
month = {may},
date = {23-25},
address = {Istanbul, Turkey},
editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
publisher = {European Language Resources Association (ELRA)},
isbn = {978-2-9517408-7-7},
language = {english}
}

@misc{li2017pruning,
    title={Pruning Filters for Efficient ConvNets}, 
    author={Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
    year={2017},
    eprint={1608.08710},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{gale2020sparse,
    title={Sparse GPU Kernels for Deep Learning}, 
    author={Trevor Gale and Matei Zaharia and Cliff Young and Erich Elsen},
    year={2020},
    eprint={2006.10901},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{liebenwein2021lost,
    title={Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}, 
    author={Lucas Liebenwein and Cenk Baykal and Brandon Carter and David Gifford and Daniela Rus},
    year={2021},
    eprint={2103.03014},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@book{Aho:72,
  author  = {Alfred V. Aho and Jeffrey D. Ullman},
  title   = {The Theory of Parsing, Translation and Compiling},
  year    = "1972",
  volume  = "1",
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, NJ}
}

@article{tay2020efficient,
author    = {Yi Tay and
             Mostafa Dehghani and
             Dara Bahri and
             Donald Metzler},
title     = {Efficient Transformers: {A} Survey},
journal   = {CoRR},
volume    = {abs/2009.06732},
year      = {2020},
url       = {https://arxiv.org/abs/2009.06732},
eprinttype = {arXiv},
eprint    = {2009.06732},
timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06732.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{APA:83,
  author  = {{American Psychological Association}},
  title   = {Publications Manual},
  year    = "1983",
  publisher = {American Psychological Association},
  address = {Washington, DC}
}

@article{Chandra:81,
author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
year = "1981",
title = {Alternation},
journal = {Journal of the Association for Computing Machinery},
volume = "28",
number = "1",
pages = "114--133",
doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
title={Scalable training of {$L_1$}-regularized log-linear models},
author={Andrew, Galen and Gao, Jianfeng},
booktitle={Proceedings of the 24th International Conference on Machine Learning},
pages={33--40},
year={2007},
url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
  author  = {Dan Gusfield},
  title   = {Algorithms on Strings, Trees and Sequences},
  year    = "1997",
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
  author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal   = {Computing Research Repository},
  volume    = {arXiv:1503.06733},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.06733},
  note    = {version 2}
}

@article{Ando2005,
Acmid = {1194905},
Author = {Ando, Rie Kubota and Zhang, Tong},
Issn = {1532-4435},
Issue_Date = {12/1/2005},
Journal = {Journal of Machine Learning Research},
Month = dec,
Numpages = {37},
Pages = {1817--1853},
Publisher = {JMLR.org},
Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
Volume = {6},
Year = {2005},
url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
title={An algorithm for the machine calculation of complex {F}ourier series},
author={Cooley, James W. and Tukey, John W.},
journal={Mathematics of Computation},
volume={19},
number={90},
pages={297--301},
year={1965},
url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@misc{Sajjad-et-al-effect,
doi = {10.48550/ARXIV.2004.03844},

url = {https://arxiv.org/abs/2004.03844},

author = {Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},

keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {On the Effect of Dropping Layers of Pre-trained Transformer Models},

publisher = {arXiv},

year = {2020},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Gordon2020CompressingBS,
title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
booktitle={REPL4NLP},
year={2020}
}


@article{Hooker2020CharacterisingBI,
title={Characterising Bias in Compressed Models},
author={Sara Hooker and Nyalleng Moorosi and Gregory Clark and Samy Bengio and Emily L. Denton},
journal={ArXiv},
year={2020},
volume={abs/2010.03058}
}

@article{Brown2020LanguageMA,
title={Language Models are Few-Shot Learners},
author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
journal={ArXiv},
year={2020},
volume={abs/2005.14165}
}
@article{Zhang2022OPTOP,
title={OPT: Open Pre-trained Transformer Language Models},
author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
journal={ArXiv},
year={2022},
volume={abs/2205.01068}
}
@article{Chowdhery2022PaLMSL,
title={PaLM: Scaling Language Modeling with Pathways},
author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek B Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Oliveira Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
journal={ArXiv},
year={2022},
volume={abs/2204.02311}
}

@article{deshpande2022when,
author    = {Ameet Deshpande and
             Partha Talukdar and
             Karthik Narasimhan},
title     = {When is {BERT} Multilingual? Isolating Crucial Ingredients for Cross-lingual
             Transfer},
journal   = {CoRR},
volume    = {abs/2110.14782},
year      = {2021},
url       = {https://arxiv.org/abs/2110.14782},
eprinttype = {arXiv},
eprint    = {2110.14782},
timestamp = {Tue, 02 Nov 2021 15:31:04 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-2110-14782.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sanh2019distilbert,
title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
journal={arXiv preprint arXiv:1910.01108},
year={2019}
}

@article{michel2019sixteen,
title={Are sixteen heads really better than one?},
author={Michel, Paul and Levy, Omer and Neubig, Graham},
journal={Advances in neural information processing systems (NeurIPS)},
volume={32},
year={2019}
}

@inproceedings{shen2020q,
title={Q-bert: Hessian based ultra low precision quantization of bert},
author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={34},
number={05},
pages={8815--8821},
year={2020}
}

@inproceedings{Radford2019LanguageMA,
title={Language Models are Unsupervised Multitask Learners},
author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
year={2019}
}


@article{kaplan2020scaling,
author    = {Jared Kaplan and
             Sam McCandlish and
             Tom Henighan and
             Tom B. Brown and
             Benjamin Chess and
             Rewon Child and
             Scott Gray and
             Alec Radford and
             Jeffrey Wu and
             Dario Amodei},
title     = {Scaling Laws for Neural Language Models},
journal   = {CoRR},
volume    = {abs/2001.08361},
year      = {2020},
url       = {https://arxiv.org/abs/2001.08361},
eprinttype = {arXiv},
eprint    = {2001.08361},
timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Lin2021FewshotLW,
title={Few-shot Learning with Multilingual Language Models},
author={Xi Victoria Lin and Todor Mihaylov and Mikel Artetxe and Tianlu Wang and Shuohui Chen and Daniel Simig and Myle Ott and Naman Goyal and Shruti Bhosale and Jingfei Du and Ramakanth Pasunuru and Sam Shleifer and Punit Singh Koura and Vishrav Chaudhary and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Zornitsa Kozareva and Mona Diab and Ves Stoyanov and Xian Li},
journal={ArXiv},
year={2021},
volume={abs/2112.10668}
}
@inproceedings{NEURIPS2019_c04c19c2,
author = {Conneau, Alexis and Lample, Guillaume},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Cross-lingual Language Model Pretraining},
url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
volume = {32},
year = {2019}
}

@inproceedings{zhang-etal-2017-towards,
  title = "Towards Compact and Fast Neural Machine Translation Using a Combined Method",
  author = "Zhang, Xiaowei  and
    Chen, Wei  and
    Wang, Feng  and
    Xu, Shuang  and
    Xu, Bo",
  booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  month = sep,
  year = "2017",
  address = "Copenhagen, Denmark",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D17-1154",
  doi = "10.18653/v1/D17-1154",
  pages = "1475--1481",
}


@inproceedings{xu-etal-2021-rethinking,
  title = "Rethinking Network Pruning {--} under the Pre-train and Fine-tune Paradigm",
  author = "Xu, Dongkuan  and
    Yen, Ian En-Hsu  and
    Zhao, Jinxi  and
    Xiao, Zhibin",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.188",
  doi = "10.18653/v1/2021.naacl-main.188",
  pages = "2376--2382",
}

@article{Artetxe2019MassivelyMS,
title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
author={Mikel Artetxe and Holger Schwenk},
journal={Transactions of the Association for Computational Linguistics},
year={2019},
volume={7},
pages={597-610}
}


@inproceedings{Sachan2018EffectiveUO,
title={Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition},
author={Devendra Singh Sachan and Pengtao Xie and Mrinmaya Sachan and Eric P. Xing},
booktitle={MLHC},
year={2018}
}

@article{Koutsikakis2020GREEKBERTTG,
title={GREEK-BERT: The Greeks visiting Sesame Street},
author={John Koutsikakis and Ilias Chalkidis and Prodromos Malakasiotis and Ion Androutsopoulos},
journal={11th Hellenic Conference on Artificial Intelligence},
year={2020}
}

@inproceedings{Parikh2016ADA,
title={A Decomposable Attention Model for Natural Language Inference},
author={Ankur P. Parikh and Oscar T{\"a}ckstr{\"o}m and Dipanjan Das and Jakob Uszkoreit},
booktitle={EMNLP},
year={2016}
}

@inproceedings{Bowman2015ALA,
title={A large annotated corpus for learning natural language inference},
author={Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
booktitle={EMNLP},
year={2015}
}

@inproceedings{Wang2018GLUEAM,
title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={BlackboxNLP@EMNLP},
year={2018}
}

@article{Ganesh2021CompressingLT,
title={Compressing Large-Scale Transformer-Based Models: A Case Study on BERT},
author={Prakhar Ganesh and Yao Chen and Xin Lou and Mohammad Ali Khan and Y. Yang and Deming Chen and Marianne Winslett and Hassan Sajjad and Preslav Nakov},
journal={Transactions of the Association for Computational Linguistics},
year={2021},
volume={9},
pages={1061-1080}
}



@inproceedings{Zhong2021AreLP,
title={Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level},
author={Ruiqi Zhong and Dhruba Ghosh and Dan Klein and Jacob Steinhardt},
booktitle={FINDINGS},
year={2021}
}






@article{Bommasani2021FoundationModels,
title={On the Opportunities and Risks of Foundation Models},
author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
journal={ArXiv},
year={2021},
url={https://crfm.stanford.edu/assets/report.pdf}
}

@inproceedings{chen-2023-large,
  title = "Large Language Models are few(1)-shot Table Reasoners",
  author = "Chen, Wenhu",
  booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
  month = may,
  year = "2023",
  address = "Dubrovnik, Croatia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-eacl.83",
  pages = "1120--1130",
}

@article{Zhuang2021RandomnessIN,
  title          = {{Randomness In Neural Network Training: Characterizing The Impact of Tooling}},
  author         = {Donglin Zhuang and Xingyao Zhang and Shuaiwen Leon Song and Sara Hooker},
  year           = {2021},
  journal        = {ArXiv},
  volume         = {abs/2106.11872}
}
@book{Hauck2007,
  title          = {{Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation}},
  author         = {Hauck, Scott and DeHon, Andre},
  year           = {2007},
  publisher      = {Morgan Kaufmann Publishers Inc.},
  address        = {San Francisco, CA, USA}
}
@article{2018Mittal,
  title          = {{Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks}},
  author         = {Deepak Mittal and Shweta Bhardwaj and Mitesh M. Khapra and Balaraman Ravindran},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1801.10447},
  url            = {http://arxiv.org/abs/1801.10447},
  timestamp      = {Mon, 13 Aug 2018 16:46:51 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1801-10447},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@article{2018random_pruning,
  title          = {{Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks}},
  author         = {Mittal, D. and Bhardwaj, S. and Khapra, M.~M. and Ravindran, B.},
  year           = {2018},
  month          = jan,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2018arXiv180110447M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{nair2010rectified,
  title          = {{Rectified linear units improve restricted boltzmann machines}},
  author         = {Nair, Vinod and Hinton, Geoffrey E},
  year           = {2010},
  booktitle      = {ICML}
}
@inproceedings{socher_recursive_2013,
  title          = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
  author         = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  year           = {2013},
  month          = oct,
  booktitle      = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  publisher      = {Association for Computational Linguistics},
  address        = {Seattle, Washington, USA},
  pages          = {1631--1642},
  url            = {https://www.aclweb.org/anthology/D13-1170}
}
@misc{fan2019reducing,
  title          = {{Reducing Transformer Depth on Demand with Structured Dropout}},
  author         = {Angela Fan and Edouard Grave and Armand Joulin},
  year           = {2019}
}
@article{Wilson2000,
  title          = {{Reduction Techniques for Instance-BasedLearning Algorithms}},
  author         = {Wilson, D. Randall and Martinez, Tony R.},
  year           = {2000},
  month          = mar,
  journal        = {Mach. Learn.},
  publisher      = {Kluwer Academic Publishers},
  address        = {Hingham, MA, USA},
  volume         = {38},
  number         = {3},
  pages          = {257--286},
  issn           = {0885-6125},
  url            = {https://doi.org/10.1023/A:1007626913721},
  issue_date     = {March 2000},
  numpages       = {30},
  acmid          = {343200}
}
@article{lasso,
  title          = {{Regression Shrinkage and Selection Via the Lasso}},
  author         = {Robert Tibshirani},
  year           = {1994},
  journal        = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
  volume         = {58},
  pages          = {267--288}
}
@article{tibshirani1996regression,
  title          = {{Regression shrinkage and selection via the lasso}},
  author         = {Tibshirani, Robert},
  year           = {1996},
  journal        = {Journal of the Royal Statistical Society: Series B (Methodological)},
  publisher      = {Wiley Online Library},
  volume         = {58},
  number         = {1},
  pages          = {267--288}
}
@article{inductive_biases,
  title          = {{Relational inductive biases, deep learning, and graph networks}},
  author         = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez{-}Gonzalez and Vin{\'{\i}}cius Flores Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and H. Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matthew Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1806.01261},
  url            = {http://arxiv.org/abs/1806.01261},
  timestamp      = {Wed, 24 Jul 2019 18:56:21 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1806-01261.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}

@article{2015szegedy,
  title          = {{Rethinking the Inception Architecture for Computer Vision}},
  author         = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year           = {2015},
  month          = dec,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1512.00567},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{rethinking-pruning,
  title          = {{Rethinking the Value of Network Pruning}},
  author         = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1810.05270}
}
@inproceedings{kovaleva_revealing_2019,
  title          = {{Revealing the Dark Secrets of BERT}},
  author         = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  year           = {2019},
  month          = nov,
  booktitle      = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  publisher      = {Association for Computational Linguistics},
  address        = {Hong Kong, China},
  pages          = {4365--4374},
  url            = {https://www.aclweb.org/anthology/D19-1445},
  urldate        = {2021-01-22}
}
@inproceedings{Lindsey1994,
  title          = {{Review of hardware neural networks: A User's perspective}},
  author         = {Lindsey, Clark S. and Lindblad, Thomas},
  year           = {1994},
  month          = {9},
  booktitle      = {{3rd Workshop on Neural Networks: From Biology to High-energy Physics}},
  pages          = {0215--224},
  reportnumber   = {TRITA-FYS-9012}
}

@article{2020arXiv201114826O,
  title          = {{Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research}},
  author         = {Obando-Ceron, Johan S. and Castro, Pablo Samuel},
  year           = {2020},
  month          = nov,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2011.14826},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv201114826O},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{2019arXiv191111134E,
  title          = {{Rigging the Lottery: Making All Tickets Winners}},
  author         = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  year           = {2019},
  month          = nov,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1911.11134},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv191111134E},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{mccoy_etal_2019_right,
  title          = {{Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference}},
  author         = {McCoy, Tom  and Pavlick, Ellie  and Linzen, Tal},
  year           = {2019},
  month          = jul,
  booktitle      = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher      = {Association for Computational Linguistics},
  address        = {Florence, Italy},
  pages          = {3428--3448},
  url            = {https://www.aclweb.org/anthology/P19-1334}
}
@article{wang2020robust,
  title          = {{Robust Optimization for Fairness with Noisy Protected Groups}},
  author         = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael I},
  year           = {2020},
  journal        = {arXiv preprint arXiv:2002.09343}
}
@article{domingo_1995,
  title          = {{Rule Induction and Instance-Based Learning: A Unified Approach}},
  author         = {Domingos, Pedro},
  year           = {1995},
  month          = {05}
}
@inproceedings{runtime-neural-pruning,
  title          = {{Runtime Neural Pruning}},
  author         = {Ji Lin and Yongming Rao and Jiwen Lu and Jie Zhou},
  year           = {2017},
  booktitle      = {NeurIPS},
  pages          = {2178--2188}
}
@inproceedings{lu_sanger_2021,
  title          = {{Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture}},
  author         = {Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
  year           = {2021},
  month          = oct,
  booktitle      = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {MICRO '21},
  pages          = {977--991},
  url            = {http://doi.org/10.1145/3466752.3480125},
  urldate        = {2022-06-16}
}
@incollection{NeurIPS2016_6316,
  title          = {{Satisfying Real-world Goals with Dataset Constraints}},
  author         = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
  year           = {2016},
  booktitle      = {Advances in Neural Information Processing Systems 29},
  publisher      = {Curran Associates, Inc.},
  pages          = {2415--2423},
  url            = {http://papers.NeurIPS.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf},
  editor         = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett}
}

@inproceedings{andrew2007scalable,
  title          = {{Scalable training of L1-regularized log-linear models}},
  author         = {Andrew, Galen and Gao, Jianfeng},
  year           = {2007},
  booktitle      = {Proceedings of the 24th International Conference on Machine Learning},
  pages          = {33--40}
}
@misc{rae2021scaling,
  title          = {{Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}},
  author         = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
  year           = {2021}
}
@article{Kaplan2020,
  title          = {{Scaling Laws for Neural Language Models}},
  author         = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year           = {2020},
  journal        = {CoRR},
  volume         = {abs/2001.08361},
  url            = {https://arxiv.org/abs/2001.08361},
  eprinttype     = {arXiv},
  timestamp      = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@incollection{Bengio+chapter2007,
  title          = {{Scaling Learning Algorithms Towards AI}},
  author         = {Bengio, Yoshua and LeCun, Yann},
  year           = {2007},
  booktitle      = {Large Scale Kernel Machines},
  publisher      = {MIT Press}
}
@inproceedings{scaling-nmt,
  title          = {{Scaling Neural Machine Translation}},
  author         = {Myle Ott and Sergey Edunov and David Grangier and Michael Auli},
  year           = {2018},
  booktitle      = {Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018},
  pages          = {1--9}
}

@article{shooker2019,
  title          = {{Selective Brain Damage: Measuring the Disparate Impact of Model Pruning}},
  author         = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
  year           = {2019},
  month          = nov,
  journal        = {arXiv e-prints},
  volume         = {abs/1806.10758},
  pages          = {arXiv:1911.05248},
  url            = {http://arxiv.org/abs/1806.10758},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Birodkar2019SemanticRI,
  title          = {{Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need}},
  author         = {Vighnesh Birodkar and Hossein Mobahi and Samy Bengio},
  year           = {2019},
  journal        = {ArXiv},
  volume         = {abs/1901.11409}
}
@article{Kennedy2000SignalprocessingMA,
  title          = {{Signal-processing machines at the postsynaptic density.}},
  author         = {Mary B. Kennedy},
  year           = {2000},
  journal        = {Science},
  volume         = {290 5492},
  pages          = {750--4}
}
@inproceedings{wu_similarity_2020,
  title          = {{Similarity Analysis of Contextual Word Representation Models}},
  author         = {Wu, John and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
  year           = {2020},
  month          = jul,
  booktitle      = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {4638--4655},
  url            = {https://www.aclweb.org/anthology/2020.acl-main.422},
  urldate        = {2021-01-23}
}
@article{2016Lakshminarayanan,
  title          = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
  author         = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year           = {2016},
  month          = {Dec},
  journal        = {arXiv e-prints},
  booktitle      = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  location       = {Long Beach, California, USA},
  publisher      = {Curran Associates Inc.},
  address        = {USA},
  series         = {NeurIPS'17},
  pages          = {arXiv:1612.01474},
  url            = {http://dl.acm.org/citation.cfm?id=3295222.3295387},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv161201474L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System},
  numpages       = {12},
  acmid          = {3295387}
}

@inproceedings{NeurIPS1988_119,
  title          = {{Skeletonization: A technique for trimming the fat from a network via relevance assessment}},
  author         = {Mozer, Michael C and Smolensky, Paul},
  year           = {1989},
  booktitle      = {Advances in neural information processing systems},
  pages          = {107--115}
}
@article{dixon2017,
  title          = {{Skin Color and Colorism: Global Research, Concepts, and Measurement}},
  author         = {Dixon, Angela R. and Telles, Edward E.},
  year           = {2017},
  journal        = {Annual Review of Sociology},
  volume         = {43},
  number         = {1},
  pages          = {405--424},
  url            = {https://doi.org/10.1146/annurev-soc-060116-053315}
}
@article{lee2018snip,
  title          = {{Snip: Single-shot network pruning based on connection sensitivity}},
  author         = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  year           = {2018},
  journal        = {arXiv preprint arXiv:1810.02340}
}
@article{namhoon2018,
  title          = {{SNIP: Single-shot Network Pruning based on Connection Sensitivity}},
  author         = {Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1810.02340},
  url            = {http://arxiv.org/abs/1810.02340},
  timestamp      = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1810-02340},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Hutchinson2020,
  title          = {{Social Biases in NLP Models as Barriers for Persons with Disabilities}},
  author         = {Ben Hutchinson and Vinodkumar Prabhakaran and Emily Denton and Kellie Webster and Yu Zhong and Stephen Craig Denuyl},
  year           = {2020},
  booktitle      = {Proceedings of ACL 2020}
}
@inproceedings{raviSoDAOndeviceConversational2021,
  title          = {{SoDA: On-device Conversational Slot Extraction}},
  author         = {Ravi, Sujith and Kozareva, Zornitsa},
  year           = {2021},
  month          = jul,
  booktitle      = {Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  publisher      = {Association for Computational Linguistics},
  address        = {Singapore and Online},
  pages          = {56--65},
  url            = {https://aclanthology.org/2021.sigdial-1.7},
  urldate        = {2022-07-04}
}

@misc{Xu2010,
  title          = {{Software bloat analysis: Finding, removing, and preventing performance problems in modern large-scale object-oriented applications}},
  author         = {Xu, Harry and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Sevitsky, Gary},
  year           = {2010},
  month          = {01},
  journal        = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
  pages          = {421--426}
}
@misc{HotelSoftwarePF,
  title          = {{Software Productivity for Extreme-Scale Science}},
  author         = {H. Hotel and H. Johansen and D. Bernholdt and M. H{\'e}roux and R. Hornung},
  year           = {2014}
}
@article{polyak1964some,
  title          = {{Some methods of speeding up the convergence of iteration methods}},
  author         = {Polyak, Boris T},
  year           = {1964},
  journal        = {USSR Computational Mathematics and Mathematical Physics},
  publisher      = {Elsevier},
  volume         = {4},
  number         = {5},
  pages          = {1--17}
}
@article{Samuel59,
  title          = {{Some Studies in Machine Learning Using the Game of Checkers}},
  author         = {A. L. Samuel},
  year           = {1959},
  journal        = {IBM Journal of Research and Development},
  volume         = {3},
  number         = {3},
  pages          = {211--229}
}
@inbook{Howe1994,
  title          = {{SPACE: Symbolic Processing in Associative Computing Elements}},
  author         = {Howe, Denis B. and Asanovi{\'{c}}, Krste},
  year           = {1994},
  booktitle      = {VLSI for Neural Networks and Artificial Intelligence},
  publisher      = {Springer US},
  address        = {Boston, MA},
  pages          = {243--252},
  url            = {https://doi.org/10.1007/978-1-4899-1331-9_24}
}
@inproceedings{sparse-connection-1997,
  title          = {{Sparse Connection and Pruning in Large Dynamic Artificial Neural Networks}},
  author         = {Nikko Str\"om},
  year           = {1997},
  booktitle      = {EUROSPEECH}
}

@incollection{NeurIPS2018_7308,
  title          = {{Sparse DNNs with Improved Adversarial Robustness}},
  author         = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
  year           = {2018},
  booktitle      = {Advances in Neural Information Processing Systems 31},
  publisher      = {Curran Associates, Inc.},
  pages          = {242--251},
  url            = {http://papers.NeurIPS.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf},
  editor         = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@article{2020arXiv200610901G,
  title          = {{Sparse GPU Kernels for Deep Learning}},
  author         = {Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  year           = {2020},
  month          = jun,
  journal        = {arXiv e-prints}
}

@article{dettmers2019sparse,
  title          = {{Sparse networks from scratch: Faster training without losing performance}},
  author         = {Dettmers, Tim and Zettlemoyer, Luke},
  year           = {2019},
  journal        = {arXiv preprint arXiv:1907.04840}
}

@article{shi_sparsebert_2021,
  title          = {{SparseBERT: Rethinking the Importance Analysis in Self-attention}},
  author         = {Shi, Han and Gao, Jiahui and Ren, Xiaozhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kwok, James T.},
  year           = {2021},
  month          = feb,
  journal        = {arXiv:2102.12871 [cs]},
  url            = {http://arxiv.org/abs/2102.12871},
  urldate        = {2021-04-12},
  note           = {arXiv: 2102.12871}
}
@inproceedings{wang_spatten_2021,
  title          = {{SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning}},
  author         = {Wang, Hanrui and Zhang, Zhekai and Han, Song},
  year           = {2021},
  month          = feb,
  booktitle      = {2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages          = {97--110},
  note           = {ISSN: 2378-203X}
}
@article{Jaderberg2014SpeedingUC,
  title          = {{Speeding up Convolutional Neural Networks with Low Rank Expansions}},
  author         = {Max Jaderberg and Andrea Vedaldi and Andrew Zisserman},
  year           = {2014},
  journal        = {ArXiv},
  volume         = {abs/1405.3866}
}
@article{larus2008spending,
  title          = {{Spending Moore's Dividend}},
  author         = {Larus, James},
  year           = {2009},
  month          = may,
  journal        = {Commun. ACM},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  volume         = {52},
  number         = {5},
  pages          = {62–69},
  issn           = {0001-0782},
  url            = {https://doi.org/10.1145/1506409.1506425},
  issue_date     = {May 2009},
  numpages       = {8}
}
@inproceedings{iandolaSqueezeBERTWhatCan2020a,
  title          = {{SqueezeBERT: What can computer vision teach NLP about efficient neural networks?}},
  author         = {Iandola, Forrest and Shaw, Albert and Krishna, Ravi and Keutzer, Kurt},
  year           = {2020},
  month          = nov,
  booktitle      = {Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing},
  publisher      = {Association for Computational Linguistics},
  address        = {Online},
  pages          = {124--135},
  url            = {https://aclanthology.org/2020.sustainlp-1.17},
  urldate        = {2022-07-04}
}
@article{squeezenet2018,
  title          = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \textless1MB model size}},
  author         = {Forrest N. Iandola and Matthew W. Moskewicz and Khalid Ashraf and Song Han and William J. Dally and Kurt Keutzer},
  year           = {2016},
  journal        = {CoRR},
  volume         = {abs/1602.07360},
  url            = {http://arxiv.org/abs/1602.07360},
  timestamp      = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/IandolaMAHDK16},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}

@article{demvsar2006statistical,
  title          = {{Statistical comparisons of classifiers over multiple data sets}},
  author         = {Dem{\v{s}}ar, Janez},
  year           = {2006},
  journal        = {Journal of Machine learning research},
  volume         = {7},
  number         = {Jan},
  pages          = {1--30}
}
@inproceedings{stochastic-backpropagation,
  title          = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
  author         = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  year           = {2014},
  booktitle      = {ICML},
  publisher      = {JMLR.org},
  series         = {JMLR Workshop and Conference Proceedings},
  volume         = {32},
  pages          = {1278--1286}
}
@inproceedings{langhammer_stratix_2021,
  title          = {{Stratix 10 NX Architecture and Applications}},
  author         = {Langhammer, Martin and Nurvitadhi, Eriko and Pasca, Bogdan and Gribok, Sergey},
  year           = {2021},
  month          = feb,
  booktitle      = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {FPGA '21},
  pages          = {57--67},
  url            = {http://doi.org/10.1145/3431920.3439293},
  urldate        = {2021-07-01}
}

@article{AbbasiAsl2021StructuralCO,
  title          = {{Structural Compression of Convolutional Neural Networks with Applications in Interpretability}},
  author         = {Reza Abbasi-Asl and Bin Yu},
  year           = {2021},
  journal        = {Frontiers in Big Data},
  volume         = {4}
}

@misc{xilinx_inc_supercharge_2019,
  title          = {{Supercharge Your AI and Database Applications with Xilinx’s HBM-Enabled UltraScale+ Devices Featuring Samsung HBM2}},
  author         = {Xilinx, Inc.},
  year           = {2019},
  month          = jul,
  url            = {https://www.xilinx.com/support/documentation/white_papers/wp508-hbm2.pdf}
}

@Article{Hornik2008,
author={Hornik, R. and Jacobsohn, L. and Orwin, R. and Piesse, A. and Kalton, G.},
title={{Effects of the National Youth Anti-Drug Media Campaign on youths}},
journal={American Journal of Public Health},
year=2008,
volume={98},
number={12},
pages={2229-2236},
month={},
keywords={},
doi={10.2105/AJPH.2007.125849},
abstract={Objectives. We examined the cognitive and behavioral effects of the National Youth Anti-Drug Media Campaign on youths aged 12.5 to 18 years and report core evaluation results. Methods. From September 1999 to June 2004, 3 nationally representative cohorts of US youths aged 9 to 18 years were surveyed at home 4 times. Sample size ranged from 8117 in the first to 5126 in the fourth round (65% first-round response rate, with 86%-93% of still eligible youths interviewed subsequently). Main outcomes were self-reported lifetime, past-year, and past-30-day marijuana use and related cognitions. Results. Most analyses showed no effects from the campaign. At one round, however, more ad exposure predicted less intention to avoid marijuana use (γ= -0.07; 95% confidence interval [CI]=-0.13, -0.01) and weaker antidrug social norms (γ=-0.05; 95% CI=-0.08, -0.02) at the subsequent round. Exposure at round 3 predicted marijuana initiation at round 4 (γ=0.11; 95% CI=0.00, 0.22). Conclusions. Through June 2004, the campaign is unlikely to have had favorable effects on youths and may have had delayed unfavorable effects. The evaluation challenges the usefulness of the campaign.},
url={https://ideas.repec.org/a/aph/ajpbhl/10.2105-ajph.2007.125849_2.html}
}

@book{moote2006great,
title={The Great Plague: The Story of London's Most Deadly Year},
author={Moote, A.L. and Moote, D.C.},
isbn={9780801892301},
series={The Great Plague},
url={https://books.google.com/books?id=XGd3dIEtx\_cC},
year={2006},
publisher={Johns Hopkins University Press}
}


@inproceedings{NEURIPS2019_84b20b1f,
author = {Paquette, Philip and Lu, Yuchen and BOCCO, SETON STEVEN and Smith, Max and O.-G., Satya and Kummerfeld, Jonathan K. and Pineau, Joelle and Singh, Satinder and Courville, Aaron C},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {No-Press Diplomacy: Modeling Multi-Agent Gameplay},
url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf},
volume = {32},
year = {2019}
}



@InProceedings{pmlr-v48-almahairi16,
title = 	 {Dynamic Capacity Networks},
author = 	 {Almahairi, Amjad and Ballas, Nicolas and Cooijmans, Tim and Zheng, Yin and Larochelle, Hugo and Courville, Aaron},
booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
pages = 	 {2549--2558},
year = 	 {2016},
editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
volume = 	 {48},
series = 	 {Proceedings of Machine Learning Research},
address = 	 {New York, New York, USA},
month = 	 {20--22 Jun},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v48/almahairi16.pdf},
url = 	 {https://proceedings.mlr.press/v48/almahairi16.html},
abstract = 	 {We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN’s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.}
}

@inproceedings{Fumera2002,
  title          = {{Support Vector Machines with Embedded Reject Option}},
  author         = {Fumera, Giorgio and Roli, Fabio},
  year           = {2002},
  booktitle      = {Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines},
  publisher      = {Springer-Verlag},
  address        = {London, UK, UK},
  series         = {SVM '02},
  pages          = {68--82},
  url            = {http://dl.acm.org/citation.cfm?id=647230.719259},
  numpages       = {15},
  acmid          = {719259}
}
@article{Noukhovitch2021,
author       = {Michael Noukhovitch and
                Travis LaCroix and
                Angeliki Lazaridou and
                Aaron C. Courville},
title        = {Emergent Communication under Competition},
journal      = {CoRR},
volume       = {abs/2101.10276},
year         = {2021},
url          = {https://arxiv.org/abs/2101.10276},
eprinttype    = {arXiv},
eprint       = {2101.10276},
timestamp    = {Sat, 30 Jan 2021 18:02:51 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2101-10276.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v162-zhang22aj,
title = 	 {Building Robust Ensembles via Margin Boosting},
author =       {Zhang, Dinghuai and Zhang, Hongyang and Courville, Aaron and Bengio, Yoshua and Ravikumar, Pradeep and Suggala, Arun Sai},
booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
pages = 	 {26669--26692},
year = 	 {2022},
editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
volume = 	 {162},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {17--23 Jul},
publisher =    {PMLR},
pdf = 	 {https://proceedings.mlr.press/v162/zhang22aj/zhang22aj.pdf},
url = 	 {https://proceedings.mlr.press/v162/zhang22aj.html},
abstract = 	 {In the context of adversarial robustness, a single model does not usually have enough power to defend against all possible adversarial attacks, and as a result, has sub-optimal robustness. Consequently, an emerging line of work has focused on learning an ensemble of neural networks to defend against adversarial attacks. In this work, we take a principled approach towards building robust ensembles. We view this problem from the perspective of margin-boosting and develop an algorithm for learning an ensemble with maximum margin. Through extensive empirical evaluation on benchmark datasets, we show that our algorithm not only outperforms existing ensembling techniques, but also large models trained in an end-to-end fashion. An important byproduct of our work is a margin-maximizing cross-entropy (MCE) loss, which is a better alternative to the standard cross-entropy (CE) loss. Empirically, we show that replacing the CE loss in state-of-the-art adversarial training techniques with our MCE loss leads to significant performance improvement.}
}


@InProceedings{pmlr-v70-arpit17a,
title = 	 {A Closer Look at Memorization in Deep Networks},
author =       {Devansh Arpit and Stanis{\l}aw Jastrz{\k{e}}bski and Nicolas Ballas and David Krueger and Emmanuel Bengio and Maxinder S. Kanwal and Tegan Maharaj and Asja Fischer and Aaron Courville and Yoshua Bengio and Simon Lacoste-Julien},
booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
pages = 	 {233--242},
year = 	 {2017},
editor = 	 {Precup, Doina and Teh, Yee Whye},
volume = 	 {70},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {06--11 Aug},
publisher =    {PMLR},
pdf = 	 {http://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf},
url = 	 {https://proceedings.mlr.press/v70/arpit17a.html},
abstract = 	 {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.~real data. We also demonstrate that for appropriately tuned explicit regularization (e.g.,~dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.}
}

@inproceedings{Li2022UnderstandingRL,
title={"Understanding Robustness Lottery": A Geometric Visual Comparative Analysis of Neural Network Pruning Approaches},
author={Zhimin Li and Shusen Liu and Xin Yu and Kailkhura Bhavya and Jie Cao and Diffenderfer James Daniel and Peer-Timo Bremer and Valerio Pascucci},
year={2022},
url={https://api.semanticscholar.org/CorpusID:264451911}
}

@article{DBLP:journals/corr/abs-1811-02084,
author       = {Noam Shazeer and
                Youlong Cheng and
                Niki Parmar and
                Dustin Tran and
                Ashish Vaswani and
                Penporn Koanantakool and
                Peter Hawkins and
                HyoukJoong Lee and
                Mingsheng Hong and
                Cliff Young and
                Ryan Sepassi and
                Blake A. Hechtman},
title        = {Mesh-TensorFlow: Deep Learning for Supercomputers},
journal      = {CoRR},
volume       = {abs/1811.02084},
year         = {2018},
url          = {http://arxiv.org/abs/1811.02084},
eprinttype    = {arXiv},
eprint       = {1811.02084},
timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-1811-02084.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{frankle2020,
author       = {Jonathan Frankle and
                David J. Schwab and
                Ari S. Morcos},
title        = {The Early Phase of Neural Network Training},
journal      = {CoRR},
volume       = {abs/2002.10365},
year         = {2020},
url          = {https://arxiv.org/abs/2002.10365},
eprinttype    = {arXiv},
eprint       = {2002.10365},
timestamp    = {Tue, 03 Mar 2020 14:32:13 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2002-10365.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Goodfellow-et-al-2016,
  title={Deep Learning},
  author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher={MIT Press},
  note={\url{http://www.deeplearningbook.org}},
  year={2016}
}

@article{carole_wu_2021,
  title          = {{Sustainable AI: Environmental Implications, Challenges and Opportunities}},
  author         = {Carole{-}Jean Wu and Ramya Raghavendra and Udit Gupta and Bilge Acun and Newsha Ardalani and Kiwan Maeng and Gloria Chang and Fiona Aga Behram and James Huang and Charles Bai and Michael Gschwind and Anurag Gupta and Myle Ott and Anastasia Melnikov and Salvatore Candido and David Brooks and Geeta Chauhan and Benjamin Lee and Hsien{-}Hsin S. Lee and Bugra Akyildiz and Maximilian Balandat and Joe Spisak and Ravi Jain and Mike Rabbat and Kim Hazelwood},
  year           = {2021},
  journal        = {CoRR},
  volume         = {abs/2111.00364},
  url            = {https://arxiv.org/abs/2111.00364},
  eprinttype     = {arXiv},
  timestamp      = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-2111-00364.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{ahmadian2023intriguing,
    title={Intriguing Properties of Quantization at Scale}, 
    author={Arash Ahmadian and Saurabh Dash and Hongyu Chen and Bharat Venkitesh and Stephen Gou and Phil Blunsom and Ahmet Üstün and Sara Hooker},
    year={2023},
    eprint={2305.19268},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{blakeney2022reduce,
    title={Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation}, 
    author={Cody Blakeney and Jessica Zosa Forde and Jonathan Frankle and Ziliang Zong and Matthew L. Leavitt},
    year={2022},
    eprint={2211.00683},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Raghu2017,
  title          = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability}},
  author         = {Maithra Raghu and Justin Gilmer and Jason Yosinski and Jascha Sohl{-}Dickstein},
  year           = {2017},
  booktitle      = {NeurIPS},
  pages          = {6078--6087}
}
@article{ramachandran2017swish,
  title          = {{Swish: a self-gated activation function}},
  author         = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1710.05941},
  publisher      = {Technical report},
  volume         = {7}
}

@inproceedings{
paul2022pretraining,
title={Pre-Training on a Data Diet: Identifying Sufficient Examples for Early Training},
author={Mansheej Paul and Brett W Larsen and Surya Ganguli and Jonathan Frankle and Gintare Karolina Dziugaite},
booktitle={First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022},
year={2022},
url={https://openreview.net/forum?id=U5QRuy_LjUY}
}

@misc{kindermans2017unreliability,
    title={The (Un)reliability of saliency methods}, 
    author={Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
    year={2017},
    eprint={1711.00867},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{NEURIPS2020_6b493230,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
pages = {9459--9474},
publisher = {Curran Associates, Inc.},
title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
volume = {33},
year = {2020}
}


@misc{erdil2023algorithmic,
    title={Algorithmic progress in computer vision}, 
    author={Ege Erdil and Tamay Besiroglu},
    year={2023},
    eprint={2212.05153},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{hernandez2020,
author       = {Danny Hernandez and
                Tom B. Brown},
title        = {Measuring the Algorithmic Efficiency of Neural Networks},
journal      = {CoRR},
volume       = {abs/2005.04305},
year         = {2020},
url          = {https://arxiv.org/abs/2005.04305},
eprinttype    = {arXiv},
eprint       = {2005.04305},
timestamp    = {Thu, 14 May 2020 16:56:02 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2005-04305.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{chimoto2024critical,
    title={Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning}, 
    author={Everlyn Asiko Chimoto and Jay Gala and Orevaoghene Ahia and Julia Kreutzer and Bruce A. Bassett and Sara Hooker},
    year={2024},
    eprint={2405.19462},
    archivePrefix={arXiv},
    primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{ahia2023languages,
    title={Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models}, 
    author={Orevaoghene Ahia and Sachin Kumar and Hila Gonen and Jungo Kasai and David R. Mortensen and Noah A. Smith and Yulia Tsvetkov},
    year={2023},
    eprint={2305.13707},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{schaeffer2024predicting,
    title={Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?}, 
    author={Rylan Schaeffer and Hailey Schoelkopf and Brando Miranda and Gabriel Mukobi and Varun Madan and Adam Ibrahim and Herbie Bradley and Stella Biderman and Sanmi Koyejo},
    year={2024},
    eprint={2406.04391},
    archivePrefix={arXiv},
    primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{FXCXW2024,
author = {FXCXW},
title = {The Artificial Intelligence Law (Draft Suggested by Scholars) is here},
year = {2024},
url = {http://www.fxcxw.org.cn/dyna/content.php?id=26910},
urldate = {2024-06-22},
}

@misc{NCSC2021AICyberThreat,
author = {NCSC},
title = {The Impact of AI on the Cyber Threat},
year = {2024},
url = {https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat},
urldate = {2024-06-22}
}

@misc{OpenAI2024EarlyWarning,
author = {OpenAI},
title = {Building an Early Warning System for LLM-Aided Biological Threat Creation},
year = {2024},
url = {https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/},
urldate = {2024-06-22}
}

@book{RR-A2977-2,
author="Mouton, Christopher A. and Caleb Lucas and Ella Guest",
title="The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team Study",
address="Santa Monica, CA",
year="2024",
doi="10.7249/RRA2977-2",
publisher="RAND Corporation"
}

@misc{AISI2024AdvancedUpdate,
author = {AISI},
title = {Advanced AI Evaluations: May Update},
year = {2024},
url = {https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update},
urldate = {2024-06-22}
}

@misc{vaswani2023attention,
    title={Attention Is All You Need}, 
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2023},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{qin2023toolllm,
    title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
    author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
    year={2023},
    eprint={2307.16789},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{gemmateam2024gemma,
    title={Gemma: Open Models Based on Gemini Research and Technology}, 
    author={Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
    year={2024},
    eprint={2403.08295},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lee2024longcontext,
    title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?}, 
    author={Jinhyuk Lee and Anthony Chen and Zhuyun Dai and Dheeru Dua and Devendra Singh Sachan and Michael Boratko and Yi Luan and Sébastien M. R. Arnold and Vincent Perot and Siddharth Dalmia and Hexiang Hu and Xudong Lin and Panupong Pasupat and Aida Amini and Jeremy R. Cole and Sebastian Riedel and Iftekhar Naim and Ming-Wei Chang and Kelvin Guu},
    year={2024},
    eprint={2406.13121},
    archivePrefix={arXiv},
    primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{geminiteam2024gemini,
    title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
    author={Gemini Team and Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry and Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregory Thornton and Zhen Yang and Henryk Michalewski and Zaheer Abbas and Nathan Schucher and Ankesh Anand and Richard Ives and James Keeling and Karel Lenc and Salem Haykal and Siamak Shakeri and Pranav Shyam and Aakanksha Chowdhery and Roman Ring and Stephen Spencer and Eren Sezener and Luke Vilnis and Oscar Chang and Nobuyuki Morioka and George Tucker and Ce Zheng and Oliver Woodman and Nithya Attaluri and Tomas Kocisky and Evgenii Eltyshev and Xi Chen and Timothy Chung and Vittorio Selo and Siddhartha Brahma and Petko Georgiev and Ambrose Slone and Zhenkai Zhu and James Lottes and Siyuan Qiao and Ben Caine and Sebastian Riedel and Alex Tomala and Martin Chadwick and Juliette Love and Peter Choy and Sid Mittal and Neil Houlsby and Yunhao Tang and Matthew Lamm and Libin Bai and Qiao Zhang and Luheng He and Yong Cheng and Peter Humphreys and Yujia Li and Sergey Brin and Albin Cassirer and Yingjie Miao and Lukas Zilka and Taylor Tobin and Kelvin Xu and Lev Proleev and Daniel Sohn and Alberto Magni and Lisa Anne Hendricks and Isabel Gao and Santiago Ontanon and Oskar Bunyan and Nathan Byrd and Abhanshu Sharma and Biao Zhang and Mario Pinto and Rishika Sinha and Harsh Mehta and Dawei Jia and Sergi Caelles and Albert Webson and Alex Morris and Becca Roelofs and Yifan Ding and Robin Strudel and Xuehan Xiong and Marvin Ritter and Mostafa Dehghani and Rahma Chaabouni and Abhijit Karmarkar and Guangda Lai and Fabian Mentzer and Bibo Xu and YaGuang Li and Yujing Zhang and Tom Le Paine and Alex Goldin and Behnam Neyshabur and Kate Baumli and Anselm Levskaya and Michael Laskin and Wenhao Jia and Jack W. Rae and Kefan Xiao and Antoine He and Skye Giordano and Lakshman Yagati and Jean-Baptiste Lespiau and Paul Natsev and Sanjay Ganapathy and Fangyu Liu and Danilo Martins and Nanxin Chen and Yunhan Xu and Megan Barnes and Rhys May and Arpi Vezer and Junhyuk Oh and Ken Franko and Sophie Bridgers and Ruizhe Zhao and Boxi Wu and Basil Mustafa and Sean Sechrist and Emilio Parisotto and Thanumalayan Sankaranarayana Pillai and Chris Larkin and Chenjie Gu and Christina Sorokin and Maxim Krikun and Alexey Guseynov and Jessica Landon and Romina Datta and Alexander Pritzel and Phoebe Thacker and Fan Yang and Kevin Hui and Anja Hauth and Chih-Kuan Yeh and David Barker and Justin Mao-Jones and Sophia Austin and Hannah Sheahan and Parker Schuh and James Svensson and Rohan Jain and Vinay Ramasesh and Anton Briukhov and Da-Woon Chung and Tamara von Glehn and Christina Butterfield and Priya Jhakra and Matthew Wiethoff and Justin Frye and Jordan Grimstad and Beer Changpinyo and Charline Le Lan and Anna Bortsova and Yonghui Wu and Paul Voigtlaender and Tara Sainath and Shane Gu and Charlotte Smith and Will Hawkins and Kris Cao and James Besley and Srivatsan Srinivasan and Mark Omernick and Colin Gaffney and Gabriela Surita and Ryan Burnell and Bogdan Damoc and Junwhan Ahn and Andrew Brock and Mantas Pajarskas and Anastasia Petrushkina and Seb Noury and Lorenzo Blanco and Kevin Swersky and Arun Ahuja and Thi Avrahami and Vedant Misra and Raoul de Liedekerke and Mariko Iinuma and Alex Polozov and Sarah York and George van den Driessche and Paul Michel and Justin Chiu and Rory Blevins and Zach Gleicher and Adrià Recasens and Alban Rrustemi and Elena Gribovskaya and Aurko Roy and Wiktor Gworek and Sébastien M. R. Arnold and Lisa Lee and James Lee-Thorp and Marcello Maggioni and Enrique Piqueras and Kartikeya Badola and Sharad Vikram and Lucas Gonzalez and Anirudh Baddepudi and Evan Senter and Jacob Devlin and James Qin and Michael Azzam and Maja Trebacz and Martin Polacek and Kashyap Krishnakumar and Shuo-yiin Chang and Matthew Tung and Ivo Penchev and Rishabh Joshi and Kate Olszewska and Carrie Muir and Mateo Wirth and Ale Jakse Hartman and Josh Newlan and Sheleem Kashem and Vijay Bolina and Elahe Dabir and Joost van Amersfoort and Zafarali Ahmed and James Cobon-Kerr and Aishwarya Kamath and Arnar Mar Hrafnkelsson and Le Hou and Ian Mackinnon and Alexandre Frechette and Eric Noland and Xiance Si and Emanuel Taropa and Dong Li and Phil Crone and Anmol Gulati and Sébastien Cevey and Jonas Adler and Ada Ma and David Silver and Simon Tokumine and Richard Powell and Stephan Lee and Kiran Vodrahalli and Samer Hassan and Diana Mincu and Antoine Yang and Nir Levine and Jenny Brennan and Mingqiu Wang and Sarah Hodkinson and Jeffrey Zhao and Josh Lipschultz and Aedan Pope and Michael B. Chang and Cheng Li and Laurent El Shafey and Michela Paganini and Sholto Douglas and Bernd Bohnet and Fabio Pardo and Seth Odoom and Mihaela Rosca and Cicero Nogueira dos Santos and Kedar Soparkar and Arthur Guez and Tom Hudson and Steven Hansen and Chulayuth Asawaroengchai and Ravi Addanki and Tianhe Yu and Wojciech Stokowiec and Mina Khan and Justin Gilmer and Jaehoon Lee and Carrie Grimes Bostock and Keran Rong and Jonathan Caton and Pedram Pejman and Filip Pavetic and Geoff Brown and Vivek Sharma and Mario Lučić and Rajkumar Samuel and Josip Djolonga and Amol Mandhane and Lars Lowe Sjösund and Elena Buchatskaya and Elspeth White and Natalie Clay and Jiepu Jiang and Hyeontaek Lim and Ross Hemsley and Zeyncep Cankara and Jane Labanowski and Nicola De Cao and David Steiner and Sayed Hadi Hashemi and Jacob Austin and Anita Gergely and Tim Blyth and Joe Stanton and Kaushik Shivakumar and Aditya Siddhant and Anders Andreassen and Carlos Araya and Nikhil Sethi and Rakesh Shivanna and Steven Hand and Ankur Bapna and Ali Khodaei and Antoine Miech and Garrett Tanzer and Andy Swing and Shantanu Thakoor and Lora Aroyo and Zhufeng Pan and Zachary Nado and Jakub Sygnowski and Stephanie Winkler and Dian Yu and Mohammad Saleh and Loren Maggiore and Yamini Bansal and Xavier Garcia and Mehran Kazemi and Piyush Patil and Ishita Dasgupta and Iain Barr and Minh Giang and Thais Kagohara and Ivo Danihelka and Amit Marathe and Vladimir Feinberg and Mohamed Elhawaty and Nimesh Ghelani and Dan Horgan and Helen Miller and Lexi Walker and Richard Tanburn and Mukarram Tariq and Disha Shrivastava and Fei Xia and Qingze Wang and Chung-Cheng Chiu and Zoe Ashwood and Khuslen Baatarsukh and Sina Samangooei and Raphaël Lopez Kaufman and Fred Alcober and Axel Stjerngren and Paul Komarek and Katerina Tsihlas and Anudhyan Boral and Ramona Comanescu and Jeremy Chen and Ruibo Liu and Chris Welty and Dawn Bloxwich and Charlie Chen and Yanhua Sun and Fangxiaoyu Feng and Matthew Mauger and Xerxes Dotiwalla and Vincent Hellendoorn and Michael Sharman and Ivy Zheng and Krishna Haridasan and Gabe Barth-Maron and Craig Swanson and Dominika Rogozińska and Alek Andreev and Paul Kishan Rubenstein and Ruoxin Sang and Dan Hurt and Gamaleldin Elsayed and Renshen Wang and Dave Lacey and Anastasija Ilić and Yao Zhao and Adam Iwanicki and Alejandro Lince and Alexander Chen and Christina Lyu and Carl Lebsack and Jordan Griffith and Meenu Gaba and Paramjit Sandhu and Phil Chen and Anna Koop and Ravi Rajwar and Soheil Hassas Yeganeh and Solomon Chang and Rui Zhu and Soroush Radpour and Elnaz Davoodi and Ving Ian Lei and Yang Xu and Daniel Toyama and Constant Segal and Martin Wicke and Hanzhao Lin and Anna Bulanova and Adrià Puigdomènech Badia and Nemanja Rakićević and Pablo Sprechmann and Angelos Filos and Shaobo Hou and Víctor Campos and Nora Kassner and Devendra Sachan and Meire Fortunato and Chimezie Iwuanyanwu and Vitaly Nikolaev and Balaji Lakshminarayanan and Sadegh Jazayeri and Mani Varadarajan and Chetan Tekur and Doug Fritz and Misha Khalman and David Reitter and Kingshuk Dasgupta and Shourya Sarcar and Tina Ornduff and Javier Snaider and Fantine Huot and Johnson Jia and Rupert Kemp and Nejc Trdin and Anitha Vijayakumar and Lucy Kim and Christof Angermueller and Li Lao and Tianqi Liu and Haibin Zhang and David Engel and Somer Greene and Anaïs White and Jessica Austin and Lilly Taylor and Shereen Ashraf and Dangyi Liu and Maria Georgaki and Irene Cai and Yana Kulizhskaya and Sonam Goenka and Brennan Saeta and Ying Xu and Christian Frank and Dario de Cesare and Brona Robenek and Harry Richardson and Mahmoud Alnahlawi and Christopher Yew and Priya Ponnapalli and Marco Tagliasacchi and Alex Korchemniy and Yelin Kim and Dinghua Li and Bill Rosgen and Kyle Levin and Jeremy Wiesner and Praseem Banzal and Praveen Srinivasan and Hongkun Yu and Çağlar Ünlü and David Reid and Zora Tung and Daniel Finchelstein and Ravin Kumar and Andre Elisseeff and Jin Huang and Ming Zhang and Ricardo Aguilar and Mai Giménez and Jiawei Xia and Olivier Dousse and Willi Gierke and Damion Yates and Komal Jalan and Lu Li and Eri Latorre-Chimoto and Duc Dung Nguyen and Ken Durden and Praveen Kallakuri and Yaxin Liu and Matthew Johnson and Tomy Tsai and Alice Talbert and Jasmine Liu and Alexander Neitz and Chen Elkind and Marco Selvi and Mimi Jasarevic and Livio Baldini Soares and Albert Cui and Pidong Wang and Alek Wenjiao Wang and Xinyu Ye and Krystal Kallarackal and Lucia Loher and Hoi Lam and Josef Broder and Dan Holtmann-Rice and Nina Martin and Bramandia Ramadhana and Mrinal Shukla and Sujoy Basu and Abhi Mohan and Nick Fernando and Noah Fiedel and Kim Paterson and Hui Li and Ankush Garg and Jane Park and DongHyun Choi and Diane Wu and Sankalp Singh and Zhishuai Zhang and Amir Globerson and Lily Yu and John Carpenter and Félix de Chaumont Quitry and Carey Radebaugh and Chu-Cheng Lin and Alex Tudor and Prakash Shroff and Drew Garmon and Dayou Du and Neera Vats and Han Lu and Shariq Iqbal and Alex Yakubovich and Nilesh Tripuraneni and James Manyika and Haroon Qureshi and Nan Hua and Christel Ngani and Maria Abi Raad and Hannah Forbes and Jeff Stanway and Mukund Sundararajan and Victor Ungureanu and Colton Bishop and Yunjie Li and Balaji Venkatraman and Bo Li and Chloe Thornton and Salvatore Scellato and Nishesh Gupta and Yicheng Wang and Ian Tenney and Xihui Wu and Ashish Shenoy and Gabriel Carvajal and Diana Gage Wright and Ben Bariach and Zhuyun Xiao and Peter Hawkins and Sid Dalmia and Clement Farabet and Pedro Valenzuela and Quan Yuan and Ananth Agarwal and Mia Chen and Wooyeol Kim and Brice Hulse and Nandita Dukkipati and Adam Paszke and Andrew Bolt and Kiam Choo and Jennifer Beattie and Jennifer Prendki and Harsha Vashisht and Rebeca Santamaria-Fernandez and Luis C. Cobo and Jarek Wilkiewicz and David Madras and Ali Elqursh and Grant Uy and Kevin Ramirez and Matt Harvey and Tyler Liechty and Heiga Zen and Jeff Seibert and Clara Huiyi Hu and Andrey Khorlin and Maigo Le and Asaf Aharoni and Megan Li and Lily Wang and Sandeep Kumar and Norman Casagrande and Jay Hoover and Dalia El Badawy and David Soergel and Denis Vnukov and Matt Miecnikowski and Jiri Simsa and Praveen Kumar and Thibault Sellam and Daniel Vlasic and Samira Daruki and Nir Shabat and John Zhang and Guolong Su and Jiageng Zhang and Jeremiah Liu and Yi Sun and Evan Palmer and Alireza Ghaffarkhah and Xi Xiong and Victor Cotruta and Michael Fink and Lucas Dixon and Ashwin Sreevatsa and Adrian Goedeckemeyer and Alek Dimitriev and Mohsen Jafari and Remi Crocker and Nicholas FitzGerald and Aviral Kumar and Sanjay Ghemawat and Ivan Philips and Frederick Liu and Yannie Liang and Rachel Sterneck and Alena Repina and Marcus Wu and Laura Knight and Marin Georgiev and Hyo Lee and Harry Askham and Abhishek Chakladar and Annie Louis and Carl Crous and Hardie Cate and Dessie Petrova and Michael Quinn and Denese Owusu-Afriyie and Achintya Singhal and Nan Wei and Solomon Kim and Damien Vincent and Milad Nasr and Christopher A. Choquette-Choo and Reiko Tojo and Shawn Lu and Diego de Las Casas and Yuchung Cheng and Tolga Bolukbasi and Katherine Lee and Saaber Fatehi and Rajagopal Ananthanarayanan and Miteyan Patel and Charbel Kaed and Jing Li and Shreyas Rammohan Belle and Zhe Chen and Jaclyn Konzelmann and Siim Põder and Roopal Garg and Vinod Koverkathu and Adam Brown and Chris Dyer and Rosanne Liu and Azade Nova and Jun Xu and Alanna Walton and Alicia Parrish and Mark Epstein and Sara McCarthy and Slav Petrov and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals},
    year={2024},
    eprint={2403.05530},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{geminiteam2024gemini,
    title={Gemini: A Family of Highly Capable Multimodal Models}, 
    author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Jack Krawczyk and Cosmo Du and Ed Chi and Heng-Tze Cheng and Eric Ni and Purvi Shah and Patrick Kane and Betty Chan and Manaal Faruqui and Aliaksei Severyn and Hanzhao Lin and YaGuang Li and Yong Cheng and Abe Ittycheriah and Mahdis Mahdieh and Mia Chen and Pei Sun and Dustin Tran and Sumit Bagri and Balaji Lakshminarayanan and Jeremiah Liu and Andras Orban and Fabian Güra and Hao Zhou and Xinying Song and Aurelien Boffy and Harish Ganapathy and Steven Zheng and HyunJeong Choe and Ágoston Weisz and Tao Zhu and Yifeng Lu and Siddharth Gopal and Jarrod Kahn and Maciej Kula and Jeff Pitman and Rushin Shah and Emanuel Taropa and Majd Al Merey and Martin Baeuml and Zhifeng Chen and Laurent El Shafey and Yujing Zhang and Olcan Sercinoglu and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Gaurav Singh Tomar and Evan Senter and Martin Chadwick and Ilya Kornakov and Nithya Attaluri and Iñaki Iturrate and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Xavier Garcia and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Ravi Addanki and Antoine Miech and Annie Louis and Denis Teplyashin and Geoff Brown and Elliot Catt and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaliy Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Sidharth Mudgal and Romina Stella and Kevin Brooks and Gautam Vasudevan and Chenxi Liu and Mainak Chain and Nivedita Melinkeri and Aaron Cohen and Venus Wang and Kristie Seymore and Sergey Zubkov and Rahul Goel and Summer Yue and Sai Krishnakumaran and Brian Albert and Nate Hurley and Motoki Sano and Anhad Mohananey and Jonah Joughin and Egor Filonov and Tomasz Kępa and Yomna Eldawy and Jiawern Lim and Rahul Rishi and Shirin Badiezadegan and Taylor Bos and Jerry Chang and Sanil Jain and Sri Gayatri Sundara Padmanabhan and Subha Puttagunta and Kalpesh Krishna and Leslie Baker and Norbert Kalb and Vamsi Bedapudi and Adam Kurzrok and Shuntong Lei and Anthony Yu and Oren Litvin and Xiang Zhou and Zhichun Wu and Sam Sobell and Andrea Siciliano and Alan Papir and Robby Neale and Jonas Bragagnolo and Tej Toor and Tina Chen and Valentin Anklin and Feiran Wang and Richie Feng and Milad Gholami and Kevin Ling and Lijuan Liu and Jules Walter and Hamid Moghaddam and Arun Kishore and Jakub Adamek and Tyler Mercado and Jonathan Mallinson and Siddhinita Wandekar and Stephen Cagle and Eran Ofek and Guillermo Garrido and Clemens Lombriser and Maksim Mukha and Botu Sun and Hafeezul Rahman Mohammad and Josip Matak and Yadi Qian and Vikas Peswani and Pawel Janus and Quan Yuan and Leif Schelin and Oana David and Ankur Garg and Yifan He and Oleksii Duzhyi and Anton Älgmyr and Timothée Lottaz and Qi Li and Vikas Yadav and Luyao Xu and Alex Chinien and Rakesh Shivanna and Aleksandr Chuklin and Josie Li and Carrie Spadine and Travis Wolfe and Kareem Mohamed and Subhabrata Das and Zihang Dai and Kyle He and Daniel von Dincklage and Shyam Upadhyay and Akanksha Maurya and Luyan Chi and Sebastian Krause and Khalid Salama and Pam G Rabinovitch and Pavan Kumar Reddy M and Aarush Selvan and Mikhail Dektiarev and Golnaz Ghiasi and Erdem Guven and Himanshu Gupta and Boyi Liu and Deepak Sharma and Idan Heimlich Shtacher and Shachi Paul and Oscar Akerlund and François-Xavier Aubet and Terry Huang and Chen Zhu and Eric Zhu and Elico Teixeira and Matthew Fritze and Francesco Bertolini and Liana-Eleonora Marinescu and Martin Bölle and Dominik Paulus and Khyatti Gupta and Tejasi Latkar and Max Chang and Jason Sanders and Roopa Wilson and Xuewei Wu and Yi-Xuan Tan and Lam Nguyen Thiet and Tulsee Doshi and Sid Lall and Swaroop Mishra and Wanming Chen and Thang Luong and Seth Benjamin and Jasmine Lee and Ewa Andrejczuk and Dominik Rabiej and Vipul Ranjan and Krzysztof Styrc and Pengcheng Yin and Jon Simon and Malcolm Rose Harriott and Mudit Bansal and Alexei Robsky and Geoff Bacon and David Greene and Daniil Mirylenka and Chen Zhou and Obaid Sarvana and Abhimanyu Goyal and Samuel Andermatt and Patrick Siegler and Ben Horn and Assaf Israel and Francesco Pongetti and Chih-Wei "Louis" Chen and Marco Selvatici and Pedro Silva and Kathie Wang and Jackson Tolins and Kelvin Guu and Roey Yogev and Xiaochen Cai and Alessandro Agostini and Maulik Shah and Hung Nguyen and Noah Ó Donnaile and Sébastien Pereira and Linda Friso and Adam Stambler and Adam Kurzrok and Chenkai Kuang and Yan Romanikhin and Mark Geller and ZJ Yan and Kane Jang and Cheng-Chun Lee and Wojciech Fica and Eric Malmi and Qijun Tan and Dan Banica and Daniel Balle and Ryan Pham and Yanping Huang and Diana Avram and Hongzhi Shi and Jasjot Singh and Chris Hidey and Niharika Ahuja and Pranab Saxena and Dan Dooley and Srividya Pranavi Potharaju and Eileen O'Neill and Anand Gokulchandran and Ryan Foley and Kai Zhao and Mike Dusenberry and Yuan Liu and Pulkit Mehta and Ragha Kotikalapudi and Chalence Safranek-Shrader and Andrew Goodman and Joshua Kessinger and Eran Globen and Prateek Kolhar and Chris Gorgolewski and Ali Ibrahim and Yang Song and Ali Eichenbaum and Thomas Brovelli and Sahitya Potluri and Preethi Lahoti and Cip Baetu and Ali Ghorbani and Charles Chen and Andy Crawford and Shalini Pal and Mukund Sridhar and Petru Gurita and Asier Mujika and Igor Petrovski and Pierre-Louis Cedoz and Chenmei Li and Shiyuan Chen and Niccolò Dal Santo and Siddharth Goyal and Jitesh Punjabi and Karthik Kappaganthu and Chester Kwak and Pallavi LV and Sarmishta Velury and Himadri Choudhury and Jamie Hall and Premal Shah and Ricardo Figueira and Matt Thomas and Minjie Lu and Ting Zhou and Chintu Kumar and Thomas Jurdi and Sharat Chikkerur and Yenai Ma and Adams Yu and Soo Kwak and Victor Ähdel and Sujeevan Rajayogam and Travis Choma and Fei Liu and Aditya Barua and Colin Ji and Ji Ho Park and Vincent Hellendoorn and Alex Bailey and Taylan Bilal and Huanjie Zhou and Mehrdad Khatir and Charles Sutton and Wojciech Rzadkowski and Fiona Macintosh and Konstantin Shagin and Paul Medina and Chen Liang and Jinjing Zhou and Pararth Shah and Yingying Bi and Attila Dankovics and Shipra Banga and Sabine Lehmann and Marissa Bredesen and Zifan Lin and John Eric Hoffmann and Jonathan Lai and Raynald Chung and Kai Yang and Nihal Balani and Arthur Bražinskas and Andrei Sozanschi and Matthew Hayes and Héctor Fernández Alcalde and Peter Makarov and Will Chen and Antonio Stella and Liselotte Snijders and Michael Mandl and Ante Kärrman and Paweł Nowak and Xinyi Wu and Alex Dyck and Krishnan Vaidyanathan and Raghavender R and Jessica Mallet and Mitch Rudominer and Eric Johnston and Sushil Mittal and Akhil Udathu and Janara Christensen and Vishal Verma and Zach Irving and Andreas Santucci and Gamaleldin Elsayed and Elnaz Davoodi and Marin Georgiev and Ian Tenney and Nan Hua and Geoffrey Cideron and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Dylan Scandinaro and Heinrich Jiang and Jasper Snoek and Mukund Sundararajan and Xuezhi Wang and Zack Ontiveros and Itay Karo and Jeremy Cole and Vinu Rajashekhar and Lara Tumeh and Eyal Ben-David and Rishub Jain and Jonathan Uesato and Romina Datta and Oskar Bunyan and Shimu Wu and John Zhang and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Jane Park and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Geoffrey Irving and Edward Loper and Michael Fink and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Ivan Petrychenko and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Evan Palmer and Paul Suganthan and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Ginger Perng and Elena Allica Abellan and Mingyang Zhang and Ishita Dasgupta and Nate Kushman and Ivo Penchev and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Daniel Andor and Pedro Valenzuela and Minnie Lui and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Ken Franko and Anna Bulanova and Rémi Leblond and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Mark Omernick and Colton Bishop and Rachel Sterneck and Rohan Jain and Jiawei Xia and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Daniel J. Mankowitz and Alex Polozov and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Matthieu Geist and Ser tan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Kathy Wu and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Saaber Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Yeqing Li and Nir Levine and Ariel Stolovich and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Charlie Deck and Hyo Lee and Zonglin Li and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Sho Arora and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Lynette Webb and Sahil Dua and Dong Li and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Evgenii Eltyshev and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Christof Angermueller and Xiaowei Li and Anoop Sinha and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Denny Zhou and Komal Jalan and Dinghua Li and Blake Hechtman and Parker Schuh and Milad Nasr and Kieran Milan and Vladimir Mikulik and Juliana Franco and Tim Green and Nam Nguyen and Joe Kelley and Aroma Mahendru and Andrea Hu and Joshua Howland and Ben Vargas and Jeffrey Hui and Kshitij Bansal and Vikram Rao and Rakesh Ghiya and Emma Wang and Ke Ye and Jean Michel Sarr and Melanie Moranski Preston and Madeleine Elish and Steve Li and Aakash Kaku and Jigar Gupta and Ice Pasupat and Da-Cheng Juan and Milan Someswar and Tejvi M. and Xinyun Chen and Aida Amini and Alex Fabrikant and Eric Chu and Xuanyi Dong and Amruta Muthal and Senaka Buthpitiya and Sarthak Jauhari and Nan Hua and Urvashi Khandelwal and Ayal Hitron and Jie Ren and Larissa Rinaldi and Shahar Drath and Avigail Dabush and Nan-Jiang Jiang and Harshal Godhia and Uli Sachs and Anthony Chen and Yicheng Fan and Hagai Taitelbaum and Hila Noga and Zhuyun Dai and James Wang and Chen Liang and Jenny Hamer and Chun-Sung Ferng and Chenel Elkind and Aviel Atias and Paulina Lee and Vít Listík and Mathias Carlen and Jan van de Kerkhof and Marcin Pikus and Krunoslav Zaher and Paul Müller and Sasha Zykova and Richard Stefanec and Vitaly Gatsko and Christoph Hirnschall and Ashwin Sethi and Xingyu Federico Xu and Chetan Ahuja and Beth Tsai and Anca Stefanoiu and Bo Feng and Keshav Dhandhania and Manish Katyal and Akshay Gupta and Atharva Parulekar and Divya Pitta and Jing Zhao and Vivaan Bhatia and Yashodha Bhavnani and Omar Alhadlaq and Xiaolin Li and Peter Danenberg and Dennis Tu and Alex Pine and Vera Filippova and Abhipso Ghosh and Ben Limonchik and Bhargava Urala and Chaitanya Krishna Lanka and Derik Clive and Yi Sun and Edward Li and Hao Wu and Kevin Hongtongsak and Ianna Li and Kalind Thakkar and Kuanysh Omarov and Kushal Majmundar and Michael Alverson and Michael Kucharski and Mohak Patel and Mudit Jain and Maksim Zabelin and Paolo Pelagatti and Rohan Kohli and Saurabh Kumar and Joseph Kim and Swetha Sankar and Vineet Shah and Lakshmi Ramachandruni and Xiangkai Zeng and Ben Bariach and Laura Weidinger and Amar Subramanya and Sissie Hsiao and Demis Hassabis and Koray Kavukcuoglu and Adam Sadovsky and Quoc Le and Trevor Strohman and Yonghui Wu and Slav Petrov and Jeffrey Dean and Oriol Vinyals},
    year={2024},
    eprint={2312.11805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{davidson2023ai,
    title={AI capabilities can be significantly improved without expensive retraining}, 
    author={Tom Davidson and Jean-Stanislas Denain and Pablo Villalobos and Guillem Bas},
    year={2023},
    eprint={2312.07413},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{epoch2023aitrends,
title={Key Trends and Figures in Machine Learning},
author={{Epoch AI}},
year={2023},
url={https://epochai.org/trends},
note={Accessed: 2024-05-19}
}

@misc{openai2024gpt4,
    title={GPT-4 Technical Report}, 
    author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat et al.},
    year={2024},
    eprint={2303.08774},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@incollection{RICCI2011276,
title = {Benefits and Limitations of the Precautionary Principle},
editor = {J.O. Nriagu},
booktitle = {Encyclopedia of Environmental Health},
publisher = {Elsevier},
address = {Burlington},
pages = {276-285},
year = {2011},
isbn = {978-0-444-52272-6},
doi = {https://doi.org/10.1016/B978-0-444-52272-6.00230-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444522726002300},
author = {P.F. Ricci and J. Zhang},
keywords = {Legal causation, Precautionary principle, Risk assessment},
abstract = {Precautionary principles bridge the gap between weakly understood causes of potentially either grave or irreversible environmental damages and potentially costly policy interventions. These principles provide a moral justification for acting even though causation is unclear. This condition forces decision-makers inevitably to confront a variety of difficulties. Fundamentally, the dilemma is that the ethical choice, better safe than sorry, can be costly because an action designed to avoid a potential damage can be counterproductive for society: a seemingly precautionary action can do more harm than good. Risk–cost–benefit analysis (RCBA) resolves this dilemma by informing decision-makers about the probable net benefits of each of the actions they may consider to minimize the effects of exposure to a serious environmental hazard. This article discusses the benefits and limitations of the precautionary principle as it appears in a number of legal and nonlegal enunciations. It begins with different legal interpretations of precautionary principles in EU treaties and case laws (an EU Commission decision to ban export of beef from the United Kingdom), as well as discusses several non-EU approaches (e.g., Canada, India, and the United States). Uncertain causal reasoning is then combined, via Bayesian networks, to exemplify how the precautionary principle can be made operational.}
}

@inproceedings{Ganguli_2022,
series={FAccT ’22},
 title={Predictability and Surprise in Large Generative Models},
 url={http://dx.doi.org/10.1145/3531146.3533229},
 DOI={10.1145/3531146.3533229},
 booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
 publisher={ACM},
 author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and El Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Johnston, Scott and Jones, Andy and Joseph, Nicholas and Kernian, Jackson and Kravec, Shauna and Mann, Ben and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Christopher and Amodei, Dario and Clark, Jack},
 year={2022},
 month=jun, collection={FAccT ’22} }


@misc{singh2024aya,
    title={Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning}, 
    author={Shivalika Singh and Freddie Vargus and Daniel Dsouza and Börje F. Karlsson and Abinaya Mahendiran and Wei-Yin Ko and Herumb Shandilya and Jay Patel and Deividas Mataciunas and Laura OMahony and Mike Zhang and Ramith Hettiarachchi and Joseph Wilson and Marina Machado and Luisa Souza Moura and Dominik Krzemiński and Hakimeh Fadaei and Irem Ergün and Ifeoma Okoh and Aisha Alaagib and Oshan Mudannayake and Zaid Alyafeai and Vu Minh Chien and Sebastian Ruder and Surya Guthikonda and Emad A. Alghamdi and Sebastian Gehrmann and Niklas Muennighoff and Max Bartolo and Julia Kreutzer and Ahmet Üstün and Marzieh Fadaee and Sara Hooker},
    year={2024},
    eprint={2402.06619},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{Dalla-Torre2023.01.11.523679,
author = {Dalla-Torre, Hugo and Gonzalez, Liam and Revilla, Javier Mendoza and Carranza, Nicolas Lopez and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
title = {The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics},
elocation-id = {2023.01.11.523679},
year = {2023},
doi = {10.1101/2023.01.11.523679},
publisher = {Cold Spring Harbor Laboratory},
abstract = {Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.Competing Interest StatementThe authors have declared no competing interest.},
URL = {https://www.biorxiv.org/content/early/2023/01/15/2023.01.11.523679},
eprint = {https://www.biorxiv.org/content/early/2023/01/15/2023.01.11.523679.full.pdf},
journal = {bioRxiv}
}


@article{elnaggar2020,
author       = {Ahmed Elnaggar and
                Michael Heinzinger and
                Christian Dallago and
                Ghalia Rehawi and
                Yu Wang and
                Llion Jones and
                Tom Gibbs and
                Tamas Feher and
                Christoph Angerer and
                Martin Steinegger and
                Debsindhu Bhowmik and
                Burkhard Rost},
title        = {ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised
                Deep Learning and High Performance Computing},
journal      = {CoRR},
volume       = {abs/2007.06225},
year         = {2020},
url          = {https://arxiv.org/abs/2007.06225},
eprinttype    = {arXiv},
eprint       = {2007.06225},
timestamp    = {Wed, 28 Dec 2022 14:59:10 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2007-06225.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{FrontierModelForum,
author = {Frontier Model Forum},
title = {Issue Brief: Measuring Training Compute},
howpublished = {\url{https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/}},
year = {2023}
}

@misc{compound-ai-blog,
title={The Shift from Models to Compound AI Systems},
author={Matei Zaharia and Omar Khattab and Lingjiao Chen and Jared Quincy Davis
        and Heather Miller and Chris Potts and James Zou and Michael Carbin
        and Jonathan Frankle and Naveen Rao and Ali Ghodsi},
howpublished={\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}},
year={2024}
}


@misc{li2024agents,
    title={More Agents Is All You Need}, 
    author={Junyou Li and Qin Zhang and Yangbin Yu and Qiang Fu and Deheng Ye},
    year={2024},
    eprint={2402.05120},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}



@inbook{inbook,
author = {Zhang, Hua},
year = {2017},
month = {01},
pages = {},
title = {The History of Microwave Heating}
}

@misc{fx_cw,
title = {Artificial Intelligence Law (Draft of Scholars' Suggestions)},
URL = {http://www.fxcxw.org.cn/dyna/content.php?id=26910},
note    = {Accessed: 2023-08-22}
}

@misc{sukhbaatar2024branchtrainmix,
    title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM}, 
    author={Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozière and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li},
    year={2024},
    eprint={2403.07816},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
shi2023language-mgsm,
title={Language models are multilingual chain-of-thought reasoners},
author={Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=fR3wGCk-IXp}
}


@misc{gemmareport,
    title={Gemma: Open Models Based on Gemini Research and Technology}, 
    author={Gemma-Team},
    year={2024},
}

@misc{jiang2023mistral,
    title={Mistral 7B}, 
    author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
    year={2023},
    eprint={2310.06825},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@online{DatabricksBlog2023DollyV2,
  author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year      = {2023},
  url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate   = {2023-06-30}
}

@article{bandarkar2023belebele,
    title={The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants}, 
    author={Lucas Bandarkar and Davis Liang and Benjamin Muller and Mikel Artetxe and Satya Narayan Shukla and Donald Husa and Naman Goyal and Abhinandan Krishnan and Luke Zettlemoyer and Madian Khabsa},
    year={2023},
    journal={arXiv preprint arXiv:2308.16884}
}

@article{team2024gemma,
title={Gemma: Open models based on gemini research and technology},
author={Gemini Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
journal={arXiv preprint arXiv:2403.08295},
year={2024}
}

@article{jiang2023mistral,
title={Mistral 7B},
author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
journal={arXiv preprint arXiv:2310.06825},
year={2023}
}

@article{khanuja2021muril,
    title={MuRIL: Multilingual Representations for Indian Languages}, 
    author={Simran Khanuja and Diksha Bansal and Sarvesh Mehtani and Savya Khosla and Atreyee Dey and Balaji Gopalan and Dilip Kumar Margam and Pooja Aggarwal and Rajiv Teja Nagipogu and Shachi Dave and Shruti Gupta and Subhash Chandra Bose Gali and Vish Subramanian and Partha Talukdar},
    year={2021},
    eprint={2103.10730},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
  oladipo-etal-2023-better,
  title = "Better Quality Pre-training Data and T5 Models for {A}frican Languages",
  author = "Oladipo, Akintunde  and
    Adeyemi, Mofetoluwa  and
    Ahia, Orevaoghene  and
    Owodunni, Abraham  and
    Ogundepo, Odunayo  and
    Adelani, David  and
    Lin, Jimmy",
  editor = "Bouamor, Houda  and
    Pino, Juan  and
    Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.emnlp-main.11",
  pages = "158--168",
  abstract = "In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. To compile this dataset, we undertake a rigorous examination of current data sources for thirteen languages within one of the most extensive multilingual web crawls, mC4, and extract cleaner data through meticulous auditing and improved web crawling strategies. Subsequently, we pretrain a new T5-based model on this dataset and evaluate its performance on multiple downstream tasks. Our model demonstrates better downstream effectiveness over existing pretrained models across four NLP tasks, underscoring the critical role data quality plays in pretraining language models in low-resource scenarios. Specifically, on cross-lingual QA evaluation, our new model is more than twice as effective as multilingual T5. All code, data and models are publicly available at https://github.com/castorini/AfriTeVa-keji.",
}



@article{Treviso2023,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  volume = {abs/https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}


@article{gale2019state,
    title={The State of Sparsity in Deep Neural Networks}, 
    author={Trevor Gale and Erich Elsen and Sara Hooker},
    year={2019},
    eprint={1902.09574},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Nakamura2023,
author = {Nakamura, Gabriel and Soares, Bruno and Pillar, Valério and Diniz-Filho, José and Duarte, Leandro},
year = {2023},
month = {08},
pages = {},
title = {Three pathways to better recognize the expertise of Global South researchers},
journal = {npj Biodiversity},
doi = {10.1038/s44185-023-00021-7}
}

@article{Park2023PapersAP,
title={Papers and patents are becoming less disruptive over time},
author={Michael Park and Erin Leahey and Russell J. Funk},
journal={Nature},
year={2023},
volume={613},
pages={138-144},
url={https://api.semanticscholar.org/CorpusID:255466666}
}

@article{zeng2021pangu,
title        = {PanGu-$\alpha$: Large-scale autoregressive pretrained Chinese language models with auto-parallel computation},
author       = {Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
year         = 2021,
journal      = {arXiv preprint arXiv:2104.12369}
}


@article{srivastava2022beyond,
title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
journal={arXiv preprint arXiv:2206.04615},
year={2022}
}

@article{paul2023deep,
    title={Deep Learning on a Data Diet: Finding Important Examples Early in Training}, 
    author={Mansheej Paul and Surya Ganguli and Gintare Karolina Dziugaite},
    year={2023},
    eprint={2107.07075},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{wenzek2019ccnet,
    title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data}, 
    author={Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzmán and Armand Joulin and Edouard Grave},
    year={2019},
    eprint={1911.00359},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{chen2024monolingual,
    title={Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca}, 
    author={Pinzhen Chen and Shaoxiong Ji and Nikolay Bogoychev and Andrey Kutuzov and Barry Haddow and Kenneth Heafield},
    year={2024},
    eprint={2309.08958},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{akiki2022bigscience,
title={BigScience: A case study in the social construction of a multilingual large language model},
author={Akiki, Christopher and Pistilli, Giada and Mieskes, Margot and Gall{\'e}, Matthias and Wolf, Thomas and Ili{\'c}, Suzana and Jernite, Yacine},
journal={arXiv preprint arXiv:2212.04960},
year={2022}
}

@article{laurenccon2022bigscience,
title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
journal={Advances in Neural Information Processing Systems},
volume={35},
pages={31809--31826},
year={2022}
}

@article{workshop2022bloom,
title={Bloom: A 176b-parameter open-access multilingual language model},
author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
journal={arXiv preprint arXiv:2211.05100},
year={2022}
}

@article{kwiatkowski-etal-2019-natural,
  title = "Natural Questions: A Benchmark for Question Answering Research",
  author = "Kwiatkowski, Tom  and
    Palomaki, Jennimaria  and
    Redfield, Olivia  and
    Collins, Michael  and
    Parikh, Ankur  and
    Alberti, Chris  and
    Epstein, Danielle  and
    Polosukhin, Illia  and
    Devlin, Jacob  and
    Lee, Kenton  and
    Toutanova, Kristina  and
    Jones, Llion  and
    Kelcey, Matthew  and
    Chang, Ming-Wei  and
    Dai, Andrew M.  and
    Uszkoreit, Jakob  and
    Le, Quoc  and
    Petrov, Slav",
  editor = "Lee, Lillian  and
    Johnson, Mark  and
    Roark, Brian  and
    Nenkova, Ani",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "7",
  year = "2019",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/Q19-1026",
  doi = "10.1162/tacl_a_00276",
  pages = "452--466",
  abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@article{frantar-sparsegpt,
title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
author={Elias Frantar and Dan Alistarh},
year={2023},
journal={arXiv preprint arXiv:2301.00774}
}

@article{frantar2022gptq,
title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
journal={arXiv preprint arXiv:2210.17323},
year={2022}
}



@article{dettmers2022llm,
title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
journal={arXiv preprint arXiv:2208.07339},
year={2022}
}


@article{mesham2021low,
title={Low-resource language modelling of South African languages},
author={Mesham, Stuart and Hayward, Luc and Shapiro, Jared and Buys, Jan},
journal={arXiv preprint arXiv:2104.00772},
year={2021}
}

@article{Merak,
title={Merak-7B: The LLM for Bahasa Indonesia},
author={Muhammad Ichsan},
publisher={Hugging Face},
journal={Hugging Face Repository},
year={2023}
}
@misc{zolkepli2024mallam,
    title={MaLLaM -- Malaysia Large Language Model}, 
    author={Husein Zolkepli and Aisyah Razak and Kamarul Adha and Ariff Nazhan},
    year={2024},
    eprint={2401.14680},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{PhoGPT,
title     = {{PhoGPT: Generative Pre-training for Vietnamese}},
author    = {Dat Quoc Nguyen and Linh The Nguyen and Chi Tran and Dung Ngoc Nguyen and Dinh Phung and Hung Bui},
journal   = {arXiv preprint},
volume    = {arXiv:2311.02945},
year      = {2023}
}

@misc{sea_lion_2023,
title={SEA-LION (Southeast Asian Languages In One Network): A Family of Large Language Models for Southeast Asia},
author={AI Singapore},
year={2023},
howpublished={\url{https://github.com/aisingapore/sealion}}
}

@misc{open-llm-leaderboard,
author = {Edward Beeching and Clémentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
title = {Open LLM Leaderboard},
year = {2023},
publisher = {Hugging Face},
howpublished = "\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}"
}

@misc{eval-harness,
author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
title        = {A framework for few-shot language model evaluation},
month        = 12,
year         = 2023,
publisher    = {Zenodo},
version      = {v0.4.0},
doi          = {10.5281/zenodo.10256836},
url          = {https://zenodo.org/records/10256836}
}

@article{jiang2024mixtral,
title={Mixtral of experts},
author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
journal={arXiv preprint arXiv:2401.04088},
year={2024}
}

@article{shi2022language,
title={Language models are multilingual chain-of-thought reasoners},
author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
journal={arXiv preprint arXiv:2210.03057},
year={2022}
}

@article{cobbe2021training,
title={Training verifiers to solve math word problems},
author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
journal={arXiv preprint arXiv:2110.14168},
year={2021}
}

@misc{ethnologue,
title = {Ethnologue},
howpublished = {\url{https://www.ethnologue.com/insights/how-many-languages/}},
note = {Accessed: 2023-06-17},
year={2023},
}

@article{gluvariants,
author       = {Noam Shazeer},
title        = {{GLU} Variants Improve Transformer},
journal      = {CoRR},
volume       = {abs/2002.05202},
year         = {2020},
url          = {https://arxiv.org/abs/2002.05202},
eprinttype    = {arXiv},
eprint       = {2002.05202},
timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{fax,
    title={Scalable Training of Language Models using JAX pjit and TPUv4}, 
    author={Joanna Yoo and Kuba Perlin and Siddhartha Rao Kamalakara and João G. M. Araújo},
    year={2022},
    eprint={2204.06514},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{megatron,
    title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
    author={Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro},
    year={2020},
    eprint={1909.08053},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{alibi,
author       = {Ofir Press and
                Noah A. Smith and
                Mike Lewis},
title        = {Train Short, Test Long: Attention with Linear Biases Enables Input
                Length Extrapolation},
journal      = {CoRR},
volume       = {abs/2108.12409},
year         = {2021},
url          = {https://arxiv.org/abs/2108.12409},
eprinttype    = {arXiv},
eprint       = {2108.12409},
timestamp    = {Thu, 02 Sep 2021 14:42:29 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rope,
author       = {Jianlin Su and
                Yu Lu and
                Shengfeng Pan and
                Bo Wen and
                Yunfeng Liu},
title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
journal      = {CoRR},
volume       = {abs/2104.09864},
year         = {2021},
url          = {https://arxiv.org/abs/2104.09864},
eprinttype    = {arXiv},
eprint       = {2104.09864},
timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-2104-09864.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{tpuv4,
    title={TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings}, 
    author={Norman P. Jouppi and George Kurian and Sheng Li and Peter Ma and Rahul Nagarajan and Lifeng Nai and Nishant Patil and Suvinay Subramanian and Andy Swing and Brian Towles and Cliff Young and Xiang Zhou and Zongwei Zhou and David Patterson},
    year={2023},
    eprint={2304.01433},
    archivePrefix={arXiv},
    primaryClass={cs.AR}
}

@misc{ko2023fairensemble,
    title={FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling}, 
    author={Wei-Yin Ko and Daniel D'souza and Karina Nguyen and Randall Balestriero and Sara Hooker},
    year={2023},
    eprint={2303.00586},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@book{benedictow2004black,
title={The Black Death, 1346-1353: The Complete History},
author={Benedictow, O.J.},
isbn={9781843832140},
lccn={2003024313},
url={https://books.google.com/books?id=KjLHAOE7irsC},
year={2004},
publisher={Boydell Press}
}


@book{greatfire,
title={The Great Fire of London: In That Apocalyptic Year, 1666},
author={Peter, John},
year={2002},
publisher={John Wiley}
}

@book{diamond1997guns,
title={Guns, Germs, and Steel: The Fates of Human Societies},
author={Diamond, Jared},
year={1997},
publisher={W.W. Norton \& Company}
}

@misc{le2012building,
    title={Building high-level features using large scale unsupervised learning}, 
    author={Quoc V. Le and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Greg S. Corrado and Jeff Dean and Andrew Y. Ng},
    year={2012},
    eprint={1112.6209},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{szegedy2014going,
    title={Going Deeper with Convolutions}, 
    author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
    year={2014},
    eprint={1409.4842},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{lyman2003much,
title={How much information?},
author={Lyman, Peter and Varian, Hal R},
year={2003},
url={http://www.sims.berkeley.edu/how-much-info2003}
}


@article{Goldberg1991,
author = {Goldberg, David},
title = {What every computer scientist should know about floating-point arithmetic},
year = {1991},
issue_date = {March 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/103162.103163},
doi = {10.1145/103162.103163},
abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
journal = {ACM Comput. Surv.},
month = {mar},
pages = {5–48},
numpages = {44},
keywords = {NaN, denormalized number, exception, floating-point, floating-point standard, gradual underflow, guard digit, overflow, relative error, rounding error, rounding mode, ulp, underflow}
}



@article{lin2023,
author = {Zeming Lin  and Halil Akin  and Roshan Rao  and Brian Hie  and Zhongkai Zhu  and Wenting Lu  and Nikita Smetanin  and Robert Verkuil  and Ori Kabeli  and Yaniv Shmueli  and Allan dos Santos Costa  and Maryam Fazel-Zarandi  and Tom Sercu  and Salvatore Candido  and Alexander Rives },
title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
journal = {Science},
volume = {379},
number = {6637},
pages = {1123-1130},
year = {2023},
doi = {10.1126/science.ade2574},
URL = {https://www.science.org/doi/abs/10.1126/science.ade2574},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ade2574},
abstract = {Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for \&gt;617 million metagenomic protein sequences, including \&gt;225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins. Machine learning methods for protein structure prediction have taken advantage of the evolutionary information present in multiple sequence alignments to derive accurate structural information, but predicting structure accurately from a single sequence is much more difficult. Lin et al. trained transformer protein language models with up to 15 billion parameters on experimental and high-quality predicted structures and found that information about atomic-level structure emerged in the model as it was scaled up. They created ESMFold, a sequence-to-structure predictor that is nearly as accurate as alignment-based methods and considerably faster. The increased speed permitted the generation of a database, the ESM Metagenomic Atlas, containing more than 600 million metagenomic proteins. —MAF A protein language model enables structure prediction and analysis of more than 600 million metagenomic proteins.}}


@misc{chen2024xtrimopglm,
    title={xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein}, 
    author={Bo Chen and Xingyi Cheng and Pan Li and Yangli-ao Geng and Jing Gong and Shen Li and Zhilei Bei and Xu Tan and Boyan Wang and Xin Zeng and Chiming Liu and Aohan Zeng and Yuxiao Dong and Jie Tang and Le Song},
    year={2024},
    eprint={2401.06199},
    archivePrefix={arXiv},
    primaryClass={q-bio.QM}
}

@misc{epoch2023tradingoffcomputeintrainingandinference,
title={Trading Off Compute in Training and Inference},
author={Pablo Villalobos and David Atkinson},
year={2023},
url={https://epochai.org/blog/trading-off-compute-in-training-and-inference},
note={Accessed: 2024-05-28}
}

@misc{xue2023adaptive,
    title={Adaptive Computation with Elastic Input Sequence}, 
    author={Fuzhao Xue and Valerii Likhosherstov and Anurag Arnab and Neil Houlsby and Mostafa Dehghani and Yang You},
    year={2023},
    eprint={2301.13195},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{udandarao2024zeroshot,
    title={No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance}, 
    author={Vishaal Udandarao and Ameya Prabhu and Adhiraj Ghosh and Yash Sharma and Philip H. S. Torr and Adel Bibi and Samuel Albanie and Matthias Bethge},
    year={2024},
    eprint={2404.04125},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{albalak2024survey,
    title={A Survey on Data Selection for Language Models}, 
    author={Alon Albalak and Yanai Elazar and Sang Michael Xie and Shayne Longpre and Nathan Lambert and Xinyi Wang and Niklas Muennighoff and Bairu Hou and Liangming Pan and Haewon Jeong and Colin Raffel and Shiyu Chang and Tatsunori Hashimoto and William Yang Wang},
    year={2024},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{epoch2023pcdtrends,
title="Parameter, Compute and Data Trends in Machine Learning",
author={{Epoch AI}},
year=2024,
url={https://epochai.org/data/epochdb/visualization},
note={Accessed: 2024-05-27}
}

@misc{gemma_2024,
  title={Gemma},
  url={https://www.kaggle.com/m/3301},
  DOI={10.34740/KAGGLE/M/3301},
  publisher={Kaggle},
  author={Gemma Team},
  year={2024}
}

@inproceedings{Ganguli_2022, series={FAccT ’22},
 title={Predictability and Surprise in Large Generative Models},
 url={http://dx.doi.org/10.1145/3531146.3533229},
 DOI={10.1145/3531146.3533229},
 booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
 publisher={ACM},
 author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and El Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Johnston, Scott and Jones, Andy and Joseph, Nicholas and Kernian, Jackson and Kravec, Shauna and Mann, Ben and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Christopher and Amodei, Dario and Clark, Jack},
 year={2022},
 month=jun, collection={FAccT ’22} }

@misc{cohere_c4ai_command_r_plus,
  title = {C4AI Command R+},
url={https://huggingface.co/CohereForAI/c4ai-command-r-plus},
author= {Cohere and Cohere For AI Team},
year={2024},
  note = {Accessed: 2024-06-30}
}

@misc{georgetown_cset_ai_law_draft,
  title = {Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)},
  url = {https://cset.georgetown.edu/publication/china-ai-law-draft/},
author={Zhang Linghan and Yang Jianjun and Cheng Ying and Zhao Jingwu and Han Xuzhi and Zheng Zhifeng and Xu Xiaoben},
year={2024}
}

@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{musameh2017determinants,
title={Determinants of day--night difference in blood pressure, a comparison with determinants of daytime and night-time blood pressure},
author={Musameh, Mohammad and Nelson, Christopher and Gracey, Jonathan and Davies, Jessica and Davies, Richard and Francis, Denise and Hughes, Adrian and Lip, Gregory Y H and Mcnamara, Helen and Mccarthy, Alison and others},
journal={Journal of human hypertension},
volume={31},
number={1},
pages={43--48},
year={2017},
publisher={Nature Publishing Group}
}

@misc{almazrouei2023falconseriesopenlanguage,
    title={The Falcon Series of Open Language Models}, 
    author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
    year={2023},
    eprint={2311.16867},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2311.16867}, 
}

@misc{workshop2023bloom176bparameteropenaccessmultilingual,
    title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
    author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić et al.},
    year={2023},
    eprint={2211.05100},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2211.05100}, 
}

@misc{aryabumi2024aya,
    title={Aya 23: Open Weight Releases to Further Multilingual Progress}, 
    author={Viraat Aryabumi and John Dang and Dwarak Talupuru and Saurabh Dash and David Cairuz and Hangyu Lin and Bharat Venkitesh and Madeline Smith and Kelly Marchisio and Sebastian Ruder and Acyr Locatelli and Julia Kreutzer and Nick Frosst and Phil Blunsom and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
    year={2024},
    eprint={2405.15032},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lee2022deduplicating,
    title={Deduplicating Training Data Makes Language Models Better}, 
    author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},
    year={2022},
    eprint={2107.06499},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{10.1162/tacl_a_00577,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}

@misc{swiss_federal_statistical_office_risk_poverty,
  title = {Risk of poverty},
  url = {https://www.bfs.admin.ch/bfs/en/home/statistics/economic-social-situation-population/economic-and-social-situequation-of-the-population/poverty-deprivation/risk-poverty.html},
  author = {Swiss Federal Statistical Office},
  year = {2024},
  urldate = {2024-07-05}
}

@misc{kossen2024semanticentropyprobesrobust,
    title={Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs}, 
    author={Jannik Kossen and Jiatong Han and Muhammed Razzak and Lisa Schut and Shreshth Malik and Yarin Gal},
    year={2024},
    eprint={2406.15927},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.15927}, 
}

@misc{marchisio2024does,
  title={How Does Quantization Affect Multilingual LLMs?},
  author={Kelly Marchisio and Saurabh Dash and Hongyu Chen and Dennis Aumiller and Ahmet Üstün and Sara Hooker and Sebastian Ruder},
  year={2024},
  eprint={2407.03211},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{fang2024domainagnostic,
    title={Domain-Agnostic Molecular Generation with Chemical Feedback}, 
    author={Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
    year={2024},
    eprint={2301.11259},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{oleary2016,
title={Reference sequence ({RefSeq}) database at {NCBI}: current status, taxonomic expansion, and functional annotation},
author={O'Leary, Nuala A and Wright, Farrell W and Brister, James R and Shumway, Martin and Mejia, Javier and Woollard, Peter and Yao, Jun and Li, Yi and Padhye, Amol and Vasani, Nisha and Amazon Web Services and et al.},
journal={Nucleic Acids Research},
volume={44},
number={D1},
pages={D733--D745},
year={2016},
publisher={Oxford University Press}
}

@article{uniprot2008,
title={The universal protein resource ({UniProt})},
author={UniProt Consortium},
journal={Nucleic Acids Research},
volume={36},
number={Database issue},
pages={D190--D195},
year={2008},
publisher={Oxford University Press}
}


@misc{epoch2024biologicalsequencemodelsinthecontextoftheaidirectives,
title={Biological Sequence Models in the Context of the AI Directives},
author={Nicole Maug and Aidan O'Gara and Tamay Besiroglu},
year={2024},
url={https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives},
note={Accessed: 2024-05-19}
}

@misc{birhane2023hate,
    title={On Hate Scaling Laws For Data-Swamps}, 
    author={Abeba Birhane and Vinay Prabhu and Sang Han and Vishnu Naresh Boddeti},
    year={2023},
    eprint={2306.13141},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@misc{epoch2022estimatingtrainingcompute,
title={Estimating Training Compute of Deep Learning Models},
author={Jaime Sevilla and Lennart Heim and Marius Hobbhahn and Tamay Besiroglu and Anson Ho and Pablo Villalobos},
year={2022},
url={https://epochai.org/blog/estimating-training-compute},
note={Accessed: 2024-05-19}
}

@article{quetelet_1817,
author = {Quetelet, A.},
title = {Statement of the Sizes of men in different Counties of Scotland, taken from the Local Militia},
journal = {Edinburgh Medical and Surgical Journal},
volume = {13},
year = {1817},
pages = {260--264}
}

@article{shen2014,
author = {Shen, Ye and Li, Chao and Heimonen, Aura and Meurman, Jukka and Nunn, Martha and Miller, Donald and Van Dyke, Thomas and Bollu, Prashanti and Kaaja, Risto and Janket, Sok-Ja},
year = {2014},
month = {01},
pages = {7-13},
title = {A pilot study on maternal oral health and birth weight of twins},
volume = {04},
journal = {Open Journal of Epidemiology},
doi = {10.4236/ojepi.2014.41002}
}

@article{quer2020,
author = {Quer, Giorgio and Gouda, Pishoy and Galarnyk, Michael and Topol, Eric and Steinhubl, Steven},
year = {2020},
month = {02},
pages = {e0227709},
title = {Inter- And intraindividual variability in daily resting heart rate and its associations with age, sex, sleep, BMI, and time of year: Retrospective, longitudinal cohort study of 92,457 adults},
volume = {15},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0227709}
}

@misc{raji2020closingaiaccountabilitygap,
    title={Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing}, 
    author={Inioluwa Deborah Raji and Andrew Smart and Rebecca N. White and Margaret Mitchell and Timnit Gebru and Ben Hutchinson and Jamila Smith-Loud and Daniel Theron and Parker Barnes},
    year={2020},
    eprint={2001.00973},
    archivePrefix={arXiv},
    primaryClass={cs.CY},
    url={https://arxiv.org/abs/2001.00973}, 
}

@misc{jatho2023concretesafetymlproblems,
    title={Concrete Safety for ML Problems: System Safety for ML Development and Assessment}, 
    author={Edgar W. Jatho and Logan O. Mailloux and Eugene D. Williams and Patrick McClure and Joshua A. Kroll},
    year={2023},
    eprint={2302.02972},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2302.02972}, 
}

@article{paleyes2022,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {114},
numpages = {29},
keywords = {Machine learning applications, sofware deployment}
}

@misc{pozzobon2024many,
    title={From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models}, 
    author={Luiza Pozzobon and Patrick Lewis and Sara Hooker and Beyza Ermis},
    year={2024},
    eprint={2403.03893},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{NEURIPS2019_fe4b8556,
author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
booktitle = {Advances in Neural Information Processing Systems},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {},
publisher = {Curran Associates, Inc.},
title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf},
volume = {32},
year = {2019}
}



@misc{pozzobon2023challenges,
    title={On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research}, 
    author={Luiza Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
    year={2023},
    eprint={2304.12397},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{Linnainmaa1976TaylorEO,
  title          = {{Taylor expansion of the accumulated rounding error}},
  author         = {Seppo Linnainmaa},
  year           = {1976},
  journal        = {BIT Numerical Mathematics},
  volume         = {16},
  pages          = {146--160}
}




@misc{lush2002,
  title          = {{Technical report: Lush reference manual, code available at http://lush.sourceforge.net}},
  author         = {Yann Lecun and Leon Bottou},
  year           = {2002},
  language       = {English (US)},
  type           = {Other}
}
@book{gilder2000telecosm,
  title          = {{Telecosm: How Infinite Bandwidth Will Revolutionize Our World}},
  author         = {Gilder, G.},
  year           = {2000},
  publisher      = {Free Press},
  url            = {https://books.google.com/books?id=Kzo-KTxdwcEC}
}


@misc{ko2023fairensemble,
    title={FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling}, 
    author={Wei-Yin Ko and Daniel D'souza and Karina Nguyen and Randall Balestriero and Sara Hooker},
    year={2023},
    eprint={2303.00586},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



@misc{wang2022robust,
    title={Robust Distillation for Worst-class Performance}, 
    author={Serena Wang and Harikrishna Narasimhan and Yichen Zhou and Sara Hooker and Michal Lukasik and Aditya Krishna Menon},
    year={2022},
    eprint={2206.06479},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{HOOKER2021100241,
title = {Moving beyond “algorithmic bias is a data problem”},
journal = {Patterns},
volume = {2},
number = {4},
pages = {100241},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100241},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000611},
author = {Sara Hooker},
abstract = {A surprisingly sticky belief is that a machine learning model merely reflects existing algorithmic bias in the dataset and does not itself contribute to harm. Why, despite clear evidence to the contrary, does the myth of the impartial model still hold allure for so many within our research community? Algorithms are not impartial, and some design choices are better than others. Recognizing how model design impacts harm opens up new mitigation techniques that are less burdensome than comprehensive data collection.}
}

@article{ABRAMO201951,
title = {Diversification versus specialization in scientific research: Which strategy pays off?},
journal = {Technovation},
volume = {82-83},
pages = {51-57},
year = {2019},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166497217303462},
author = {Giovanni Abramo and Ciriaco Andrea D'Angelo and Flavia {Di Costa}},
keywords = {Research strategy, Research evaluation, Research collaboration, Bibliometrics},
abstract = {The current work addresses a theme previously unexplored in the literature: that of whether the results arising from research activity in fields other than the scientist's primary field have greater value than the others. Operationally, the authors proceed by identifying: the scientific production of each researcher under observation; field classification of the publications; the field containing the greatest number of the researcher's publications; attribution of value of each publication. The results show that diversification at the aggregate level does not pay off, although there are some exceptions at the level of individual disciplines. The implications at policy level are notable. Since the incentive systems of research organizations are based on the impact of scientific output, the scientists concerned could resist engaging in multidisciplinary projects.}
}

@article{pascual_telling_2021,
  title          = {{Telling BERT's full story: from Local Attention to Global Aggregation}},
  author         = {Pascual, Damian and Brunner, Gino and Wattenhofer, Roger},
  year           = {2021},
  month          = jan,
  journal        = {arXiv:2004.05916 [cs]},
  url            = {http://arxiv.org/abs/2004.05916},
  urldate        = {2021-01-23},
  note           = {arXiv: 2004.05916}
}

@misc{tensorflow2015,
  title          = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}},
  author         = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  year           = {2015},
  month          = jan,
  url            = {https://www.tensorflow.org/},
  note           = {Software available from tensorflow.org}
}
@article{zhang_ternarybert_2020,
  title          = {{TernaryBERT: Distillation-aware Ultra-low Bit BERT}},
  author         = {Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  year           = {2020},
  month          = oct,
  journal        = {arXiv:2009.12812 [cs, eess]},
  url            = {http://arxiv.org/abs/2009.12812},
  urldate        = {2021-01-28},
  note           = {arXiv: 2009.12812}
}
@article{kitty_paper2017,
  title          = {{The (Un)reliability of saliency methods}},
  author         = {Kindermans, P.-J. and Hooker, S. and Adebayo, J. and Alber, M. and Sch{\"u}tt, K.~T. and D{\"a}hne, S. and Erhan, D. and Kim, B.},
  year           = {2017},
  month          = nov,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv171100867K},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@book{Raymond1990,
  title          = {{The Age of Intelligent Machines}},
  author         = {Kurzweil, Raymond},
  year           = {1990},
  publisher      = {MIT Press},
  address        = {Cambridge, MA, USA}
}
@article{dwayne2001,
  title          = {{The Anna Karenina Principle Applied to Ecological Risk Assessments of Multiple Stressors}},
  author         = {Dwayne Moore},
  year           = {2001},
  journal        = {Human and Ecological Risk Assessment: An International Journal},
  volume         = {7},
  number         = {2},
  pages          = {231--237}
}
@article{marcus2014,
  title          = {{The atoms of neural computation}},
  author         = {Gary Marcus and Adam Marblestone and Tom Dean},
  year           = {2014},
  journal        = {Science},
  volume         = {346},
  pages          = {551--552},
  note           = {Computational Neuroscience}
}
@inproceedings{5597285,
  title          = {{The Balanced Accuracy and Its Posterior Distribution}},
  author         = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
  year           = {2010},
  booktitle      = {2010 20th International Conference on Pattern Recognition},
  pages          = {3121--3124}
}

@misc{lee2021,
doi = {10.48550/ARXIV.2107.06499},

url = {https://arxiv.org/abs/2107.06499},

author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},

keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Deduplicating Training Data Makes Language Models Better},

publisher = {arXiv},

year = {2021},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{hernandez2022,
doi = {10.48550/ARXIV.2205.10487},

url = {https://arxiv.org/abs/2205.10487},

author = {Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Henighan, Tom and Hume, Tristan and Johnston, Scott and Mann, Ben and Olah, Chris and Olsson, Catherine and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Scaling Laws and Interpretability of Learning from Repeated Data},

publisher = {arXiv},

year = {2022},

copyright = {Creative Commons Attribution 4.0 International}
}


@misc{hoffmann2022,
doi = {10.48550/ARXIV.2203.15556},

url = {https://arxiv.org/abs/2203.15556},

author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},

keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Training Compute-Optimal Large Language Models},

publisher = {arXiv},

year = {2022},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{sorscher2022,
doi = {10.48550/ARXIV.2206.14486},

url = {https://arxiv.org/abs/2206.14486},

author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Beyond neural scaling laws: beating power law scaling via data pruning},

publisher = {arXiv},

year = {2022},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{thebitterlesson2019,
  title          = {{The Bitter Lesson}},
  author         = {Rich Sutton},
  year           = {2019},
  url            = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}
@inproceedings{Colwell2013,
  title          = {{The chip design game at the end of Moore's law}},
  author         = {Colwell, R.},
  year           = {2013},
  booktitle      = {2013 IEEE Hot Chips 25 Symposium (HCS)},
  pages          = {1--16}
}


@misc{piaget1954,
  title          = {{The construction of reality in the child}},
  author         = {Jean Piaget},
  year           = {1954}
}
@misc{Thompson2018TheDO,
  title          = {{The Decline of Computers As a General Purpose Technology: Why Deep Learning and the End of Moore’s Law are Fragmenting Computing}},
  author         = {Neil Thompson and Svenja Spanuth},
  year           = {2018}
}

@article{tpuv2,
  title          = {{The Design Process for Google's Training Chips: TPUv2 and TPUv3}},
  author         = {Thomas Norrie and Nishant Patil and Doe Hyun Yoon and George Kurian and Sheng Li and James Laudon and Cliff Young and Norman P. Jouppi and David A. Patterson},
  year           = {2021},
  journal        = {{IEEE} Micro},
  volume         = {41},
  number         = {2},
  pages          = {56--63},
  url            = {https://doi.org/10.1109/MM.2021.3058217},
  timestamp      = {Thu, 29 Apr 2021 15:11:30 +0200},
  biburl         = {https://dblp.org/rec/journals/micro/NorriePYKLLYJP21.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}


@misc{Hennessy2019,
  title          = {{The End of Moore's Law, CPUs (as we know them), and the Rise of Domain Specific Architectures}},
  author         = {John Hennessy},
  year           = {2019},
  url            = {https://www.kisacoresearch.com/sites/default/files/presentations/09.00_-_alphabet_-_john_hennessy.pdf}
}
@misc{2019Feldman,
  title          = {{The era of general purpose computers is ending}},
  author         = {Michael Feldman},
  year           = {2019},
  url            = {https://bit.ly/3hP8XJh}
}
@inproceedings{lu2017expressive,
  title          = {{The expressive power of neural networks: A view from the width}},
  author         = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year           = {2017},
  booktitle      = {Advances in neural information processing systems},
  pages          = {6231--6239}
}
@article{morgan1983,
  title          = {{The fifth generation: Artificial intelligence and Japan's computer challenge to the world, by Edward A. Feigenbaum and Pamela McCorduck. Reading, MA: Addison-Wesley, 1983, 275 pp. Price: \$15.35}},
  author         = {Morgan, M. Granger},
  year           = {1983},
  journal        = {Journal of Policy Analysis and Management},
  volume         = {3},
  number         = {1},
  pages          = {156--156},
  url            = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3324061}
}

@article{Shalf2020TheFO,
  title          = {{The future of computing beyond Moore’s Law}},
  author         = {John Shalf},
  year           = {2020},
  journal        = {Philosophical Transactions of the Royal Society A},
  volume         = {378}
}

@book{stein2004handbook,
  title          = {{The Handbook of Multisensory Processes}},
  author         = {Stein, G.C.C.S.B.E. and Calvert, G. and Spence, C. and Spence, D.E.P.C. and Stein, B.E. and Stein, P.C.B.E.},
  year           = {2004},
  publisher      = {MIT Press},
  series         = {A Bradford book},
  url            = {https://books.google.com/books?id=CZS\_yDoFV7AC},
  lccn           = {2004042612}
}
@article{2020arXiv200906489H,
  title          = {{The Hardware Lottery}},
  author         = {Hooker, Sara},
  year           = {2020},
  month          = sep,
  journal        = {arXiv e-prints},
  pages          = {arXiv:2009.06489},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200906489H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{hooker2020hardware,
  title          = {{The Hardware Lottery}},
  author         = {Sara Hooker},
  year           = {2020}
}
@book{posselt1888jacquard,
  title          = {{The Jacquard Machine Analyzed and Explained: The Preparation of Jacquard Cards and Practical Hints to Learners of Jacquard Designing}},
  author         = {Posselt, E.A.},
  year           = {1888},
  publisher      = {E.A. Posselt},
  series         = {Posselt's textile library},
  url            = {https://books.google.com/books?id=-6FtmgEACAAJ}
}
@techreport{Asanovic2006,
  title          = {{The Landscape of Parallel Computing Research: A View from Berkeley}},
  author         = {Asanović, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A.},
  year           = {2006},
  month          = {Dec},
  number         = {UCB/EECS-2006-183},
  url            = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
  institution    = {EECS Department, University of California, Berkeley}
}
@article{papernot2015,
  title          = {{The Limitations of Deep Learning in Adversarial Settings}},
  author         = {Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and {Berkay Celik}, Z. and Swami, A.},
  year           = {2015},
  month          = nov,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2015arXiv151107528P},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{chen_lottery_2020,
  title          = {{The Lottery Ticket Hypothesis for Pre-trained BERT Networks}},
  author         = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  year           = {2020},
  month          = oct,
  journal        = {arXiv:2007.12223 [cs, stat]},
  url            = {http://arxiv.org/abs/2007.12223},
  urldate        = {2021-01-22},
  note           = {arXiv: 2007.12223}
}
@article{frankle2018the,
  title          = {{The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks}},
  author         = {Frankle, J. and Carbin, M.},
  year           = {2018},
  month          = mar,
  journal        = {ArXiv e-prints \url{arXiv preprint arXiv:1803.03635}},
  adsurl         = {http://adsabs.harvard.edu/abs/2018arXiv180303635F},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{frankle2018lottery,
  title          = {{The lottery ticket hypothesis: Finding sparse, trainable neural networks}},
  author         = {Frankle, Jonathan and Carbin, Michael},
  year           = {2018},
  journal        = {}
}
@article{lottery-ticket-hypothesis,
  title          = {{The Lottery Ticket Hypothesis: Training Pruned Neural Networks}},
  author         = {Jonathan Frankle and Michael Carbin},
  year           = {2018},
  journal        = {CoRR},
  volume         = {abs/1803.03635},
  url            = {http://arxiv.org/abs/1803.03635}
}
@inproceedings{ahia-etal-2021-low-resource,
  title          = {{The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation}},
  author         = {Ahia, Orevaoghene  and Kreutzer, Julia  and Hooker, Sara},
  year           = {2021},
  month          = nov,
  booktitle      = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  publisher      = {Association for Computational Linguistics},
  address        = {Punta Cana, Dominican Republic},
  pages          = {3316--3333},
  url            = {https://aclanthology.org/2021.findings-emnlp.282}
}
@unpublished{anonymous2021the,
  title          = {{The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation}},
  author         = {Anonymous},
  year           = {2021},
  journal        = {OpenReview Preprint},
  note           = {anonymous preprint under review}
}
@techreport{mitchell80,
  title          = {{The Need for Biases in Learning Generalizations}},
  author         = {T. M. Mitchell},
  year           = {1980},
  address        = {New Brunswick, MA},
  institution    = {Computer Science Department, Rutgers University}
}
@misc{Lee2018,
  title          = {{The next step in Facebook AI hardware infrastructure}},
  author         = {Lee, Kevin  and Wang,Xiaodong},
  year           = {2018},
  url            = {https://bit.ly/3bgZFDn}
}
@article{rosenblatt1958perceptron,
  title          = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
  author         = {Rosenblatt, Frank},
  year           = {1958},
  journal        = {Psychological review},
  publisher      = {American Psychological Association},
  volume         = {65},
  number         = {6},
  pages          = {386}
}
@inproceedings{eldan2016power,
  title          = {{The power of depth for feedforward neural networks}},
  author         = {Eldan, Ronen and Shamir, Ohad},
  year           = {2016},
  booktitle      = {Conference on learning theory},
  pages          = {907--940}
}
@misc{zhu2021rich,
  title          = {{The Rich Get Richer: Disparate Impact of Semi-Supervised Learning}},
  author         = {Zhaowei Zhu and Tianyi Luo and Yang Liu},
  year           = {2021}
}
@article{MORGAN1992248,
  title          = {{The ring array processor: A multiprocessing peripheral for connectionist applications}},
  author         = {Nelson Morgan and James Beck and Phil Kohn and Jeff Bilmes and Eric Allman and Joachim Beer},
  year           = {1992},
  journal        = {Journal of Parallel and Distributed Computing},
  volume         = {14},
  number         = {3},
  pages          = {248--259},
  issn           = {0743-7315},
  url            = {http://www.sciencedirect.com/science/article/pii/074373159290067W}
}
@misc{1995thinkingmachines,
  title          = {{The Rise and Fall of Thinking Machines}},
  author         = {Gary Taubes},
  year           = {1995},
  url            = {https://www.inc.com/magazine/19950915/2622.html}
}
@article{balduzzi2017shattered,
  title          = {{The shattered gradients problem: If resnets are the answer, then what is the question?}},
  author         = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  year           = {2017},
  journal        = {arXiv preprint arXiv:1702.08591}
}
@article{gale2019state,
  title          = {{The State of Sparsity in Deep Neural Networks}},
  author         = {Trevor Gale and Erich Elsen and Sara Hooker},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1902.09574},
  url            = {http://arxiv.org/abs/1902.09574},
  timestamp      = {Tue, 21 May 2019 18:03:40 +0200},
  biburl         = {https://dblp.org/rec/bib/journals/corr/abs-1902-09574},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@book{Kuhn1962,
  title          = {{The Structure of Scientific Revolutions}},
  author         = {Kuhn, Thomas S.},
  year           = {1962},
  publisher      = {University of Chicago Press},
  address        = {Chicago},
  added-at       = {2011-09-20T15:32:20.000+0200},
  biburl         = {https://www.bibsonomy.org/bibtex/231d37d5bf096e1cf5e893934336464d3/voj},
  timestamp      = {2013-01-07T08:31:59.000+0100}
}
@book{Aho:72,
  title          = {{The Theory of Parsing, Translation and Compiling}},
  author         = {Alfred V. Aho and Jeffrey D. Ullman},
  year           = {1972},
  publisher      = {Prentice-Hall},
  address        = {Englewood Cliffs, NJ},
  volume         = {1}
}
@inproceedings{liu2022the,
  title          = {{The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training}},
  author         = {Shiwei Liu and Tianlong Chen and Xiaohan Chen and Li Shen and Decebal Constantin Mocanu and Zhangyang Wang and Mykola Pechenizkiy},
  year           = {2022},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=VBZJ_3tz-t}
}
@article{Wexler2019,
  title          = {{The What-If Tool: Interactive Probing of Machine Learning Models}},
  author         = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Viegas, Fernanda and Wilson, Jimbo},
  year           = {2019},
  journal        = {IEEE Transactions on Visualization and Computer Graphics},
  publisher      = {Institute of Electrical and Electronics Engineers (IEEE)},
  pages          = {1–1},
  issn           = {2160-9306},
  url            = {http://dx.doi.org/10.1109/TVCG.2019.2934619}
}
@article{Heeger1773,
  title          = {{Theory of cortical function}},
  author         = {Heeger, David J.},
  year           = {2017},
  journal        = {Proceedings of the National Academy of Sciences},
  publisher      = {National Academy of Sciences},
  volume         = {114},
  number         = {8},
  pages          = {1773--1782},
  issn           = {0027-8424},
  url            = {https://www.pnas.org/content/114/8/1773}
}
@article{Leisersoneaam9744,
  title          = {{There\textquoterights plenty of room at the Top: What will drive computer performance after Moore\textquoterights law?}},
  author         = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
  year           = {2020},
  journal        = {Science},
  publisher      = {American Association for the Advancement of Science},
  volume         = {368},
  number         = {6495},
  issn           = {0036-8075},
  url            = {https://science.sciencemag.org/content/368/6495/eaam9744},
}
@article{Budd_2021,
 title={A survey on active learning and human-in-the-loop deep learning for medical image analysis},
 volume={71},
 ISSN={1361-8415},
 url={http://dx.doi.org/10.1016/j.media.2021.102062},
 DOI={10.1016/j.media.2021.102062},
 journal={Medical Image Analysis},
 publisher={Elsevier BV},
 author={Budd, Samuel and Robinson, Emma C. and Kainz, Bernhard},
 year={2021},
 month={Jul},
 pages={102062}
}


@inproceedings{thinet,
  title          = {{ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression}},
  author         = {Jian-Hao Luo and Jianxin Wu and Weiyao Lin},
  year           = {2017},
  booktitle      = {IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017},
  pages          = {5068--5076}
}

@article{zhu2017prune,
  title          = {{To prune, or not to prune: exploring the efficacy of pruning for model compression}},
  author         = {Zhu, M. and Gupta, S.},
  year           = {2017},
  month          = oct,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv171001878Z},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Aha1992ToleratingNI,
  title          = {{Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms}},
  author         = {David W. Aha},
  year           = {1992},
  journal        = {International Journal of Man-Machine Studies},
  volume         = {36},
  pages          = {267--287}
}
@article{zhang2016,
  title          = {{Top-down Neural Attention by Excitation Backprop}},
  author         = {Zhang, J. and Lin, Z. and Brandt, J. and Shen, X. and Sclaroff, S.},
  year           = {2016},
  month          = aug,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2016arXiv160800507Z},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{Torch2002,
  title          = {{Torch: A Modular Machine Learning Software Library}},
  author         = {Collobert, Ronan and Bengio, Samy and Marithoz, Johnny},
  year           = {2002},
  month          = {11}
}
@article{Marblestone2016,
  title          = {{Toward an Integration of Deep Learning and Neuroscience}},
  author         = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
  year           = {2016},
  journal        = {Frontiers in Computational Neuroscience},
  volume         = {10},
  pages          = {94},
  issn           = {1662-5188},
  url            = {https://www.frontiersin.org/article/10.3389/fncom.2016.00094}
}

@article{Wang2019,
  title          = {{Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation}},
  author         = {Zeyu Wang and Klint Qinami and Yannis Karakozis and Kyle Genova and Prem Nair and Kenji Hata and Olga Russakovsky},
  year           = {2019},
  journal        = {CoRR},
  volume         = {abs/1911.11834},
  url            = {http://arxiv.org/abs/1911.11834},
  eprinttype     = {arXiv},
  timestamp      = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl         = {https://dblp.org/rec/journals/corr/abs-1911-11834.bib},
  bibsource      = {dblp computer science bibliography, https://dblp.org}
}
@misc{li2020train,
  title          = {{Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}},
  author         = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
  year           = {2020},
  journal        = {ICML},
  url            = {https://arxiv.org/abs/2002.11794}
}

@article{2017Lee,
  title          = {{Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}},
  author         = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  year           = {2017},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1711.09325},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2017arXiv171109325L},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{lee2018training,
  title          = {{Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}},
  author         = {Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
  year           = {2018},
  booktitle      = {International Conference on Learning Representations},
  url            = {https://openreview.net/forum?id=ryiAv2xAZ}
}

@inproceedings{zoph-etal-2016-transfer,
  title          = {{Transfer Learning for Low-Resource Neural Machine Translation}},
  author         = {Zoph, Barret  and Yuret, Deniz  and May, Jonathan  and Knight, Kevin},
  year           = {2016},
  month          = nov,
  booktitle      = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  publisher      = {Association for Computational Linguistics},
  address        = {Austin, Texas},
  pages          = {1568--1575},
  url            = {https://aclanthology.org/D16-1163}
}
@misc{cotter2018twoplayer,
  title          = {{Two-Player Games for Efficient Non-Convex Constrained Optimization}},
  author         = {Andrew Cotter and Heinrich Jiang and Karthik Sridharan},
  year           = {2018}
}
@inproceedings{koh2017,
  title          = {{Understanding Black-box Predictions via Influence Functions}},
  author         = {Pang Wei Koh and Percy Liang},
  year           = {2017},
  month          = {06--11 Aug},
  booktitle      = {Proceedings of the 34th International Conference on Machine Learning},
  publisher      = {PMLR},
  address        = {International Convention Centre, Sydney, Australia},
  series         = {Proceedings of Machine Learning Research},
  volume         = {70},
  pages          = {1885--1894},
  editor         = {Doina Precup and Yee Whye Teh},
  pdf            = {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf}
}
@book{1985understandingcomputers,
  title          = {{Understanding computers: software}},
  author         = {Time},
  year           = {1985},
  publisher      = {Time},
  address        = {Virginia}
}
@article{2016Zhang,
  title          = {{Understanding deep learning requires rethinking generalization}},
  author         = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year           = {2016},
  month          = {Nov},
  journal        = {arXiv e-prints},
  pages          = {arXiv:1611.03530},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2016arXiv161103530Z},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{2020matrix,
  title          = {{Understanding Matrix Capsules with EM Routing.}},
  url            = {https://jhui.github.io/2017/11/14/ Matrix-Capsules-with-EM-routing-Capsule-Network},
  accessed       = {2020-04-20}
}
@inproceedings{glorot2010understanding,
  title          = {{Understanding the difficulty of training deep feedforward neural networks}},
  author         = {Glorot, Xavier and Bengio, Yoshua},
  year           = {2010},
  booktitle      = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages          = {249--256}
}
@inproceedings{Fatahalian2004,
  title          = {{Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication}},
  author         = {Fatahalian, K. and Sugerman, J. and Hanrahan, P.},
  year           = {2004},
  booktitle      = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
  location       = {Grenoble, France},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {HWWS ’04},
  pages          = {133–137},
  url            = {https://doi.org/10.1145/1058129.1058148},
  numpages       = {5}
}
@inproceedings{kay2015unequal,
  title          = {{Unequal representation and gender stereotypes in image search results for occupations}},
  author         = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A},
  year           = {2015},
  booktitle      = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
  pages          = {3819--3828}
}
@article{hochreiter1991untersuchungen,
  title          = {{Untersuchungen zu dynamischen neuronalen Netzen}},
  author         = {Hochreiter, Sepp},
  year           = {1991},
  journal        = {Diploma, Technische Universit{\"a}t M{\"u}nchen},
  volume         = {91},
  number         = {1}
}
@inproceedings{1575717,
  title          = {{Using GPUs for machine learning algorithms}},
  author         = {Steinkraus, D. and Buck, I. and Simard, P. Y.},
  year           = {2005},
  booktitle      = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
  pages          = {1115--1120 Vol. 2}
}
@misc{2019cross,
  title          = {{Validating quantum computers using randomized model circuits}},
  author         = {Cross, Andrew W. and Bishop, Lev S. and Sheldon, Sarah and Nation, Paul D. and Gambetta, Jay M.},
  year           = {2019},
  month          = sep,
  volume         = {100},
  number         = {3}
}
@incollection{Kingma2015Variational,
  title          = {{Variational Dropout and the Local Reparameterization Trick}},
  author         = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
  year           = {2015},
  booktitle      = {Advances in Neural Information Processing Systems 28},
  publisher      = {Curran Associates, Inc.},
  pages          = {2575--2583},
  url            = {http://papers.NeurIPS.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
  editor         = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett}
}
@article{2017Molchanov,
  title          = {{Variational Dropout Sparsifies Deep Neural Networks}},
  author         = {Molchanov, D. and Ashukha, A. and Vetrov, D.},
  year           = {2017},
  month          = jan,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{molchanov2017variational,
  title          = {{Variational Dropout Sparsifies Deep Neural Networks}},
  author         = {Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov},
  year           = {2017},
  booktitle      = {Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017},
  pages          = {2498--2507}
}
@article{2014Simonyan,
  title          = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  author         = {Simonyan, K. and Zisserman, A.},
  year           = {2014},
  month          = sep,
  journal        = {ArXiv e-prints},
  adsurl         = {http://adsabs.harvard.edu/abs/2014arXiv1409.1556S},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{Zeiler2014,
  title          = {{Visualizing and understanding convolutional networks}},
  author         = {Zeiler, Matthew D and Fergus, Rob},
  year           = {2014},
  booktitle      = {European Conference on Computer Vision},
  pages          = {818--833},
  date-added     = {2016-10-12 11:14:28 +0000},
  date-modified  = {2016-10-12 11:16:57 +0000},
  organization   = {Springer}
}
@inproceedings{wavenet,
  title          = {{WaveNet: A Generative Model for Raw Audio}},
  author         = {A{\"{a}}ron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew W. Senior and Koray Kavukcuoglu},
  year           = {2016},
  booktitle      = {The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 September 2016},
  pages          = {125}
}
@misc{søgaard2021need,
  title          = {{We Need to Talk About Random Splits}},
  author         = {Anders Søgaard and Sebastian Ebert and Jasmijn Bastings and Katja Filippova},
  year           = {2021}
}
@article{2019arXiv191105248H,
  title          = {{What Do Compressed Deep Neural Networks Forget?}},
  author         = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
  year           = {2019},
  month          = nov,
  journal        = {arXiv e-prints},
  pages          = {arXiv:1911.05248},
  url            = {https://arxiv.org/abs/1911.05248},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2019arXiv191105248H},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{hooker2020compressed,
  title          = {{What Do Compressed Deep Neural Networks Forget?}},
  author         = {Sara Hooker and Aaron Courville and Gregory Clark and Yann Dauphin and Andrea Frome},
  year           = {2020}
}
@article{clark_what_2019,
  title          = {{What Does BERT Look At? An Analysis of BERT's Attention}},
  author         = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year           = {2019},
  month          = jun,
  journal        = {arXiv:1906.04341 [cs]},
  url            = {http://arxiv.org/abs/1906.04341},
  urldate        = {2021-01-22},
  note           = {arXiv: 1906.04341}
}
@misc{karpathy2014,
  title          = {{What I learned from competing against a ConvNet on ImageNet}},
  author         = {Karpathy, Andrej},
  year           = {2014},
  note           = {Accessed: 2019-07-07},
  howpublished   = {\url{http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/}}
}

@misc{2018Sato,
  title          = {{What makes TPUs fine-tuned for deep learning?}},
  author         = {Kaz Sato},
  year           = {2018},
  url            = {https://bit.ly/2ER3bIu}
}
@article{Feldman2020,
  title          = {{What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation}},
  author         = {Feldman, Vitaly and Zhang, Chiyuan},
  year           = {2020},
  month          = aug,
  journal        = {arXiv e-prints},
  booktitle      = {Advances in Neural Information Processing Systems},
  publisher      = {Curran Associates, Inc.},
  volume         = {33},
  pages          = {arXiv:2008.03703},
  url            = {https://proceedings.neurips.cc/paper/2020/file/1e14bfe2714193e7af5abc64ecbd6b46-Paper.pdf},
  adsurl         = {https://ui.adsabs.harvard.edu/abs/2020arXiv200803703F},
  adsnote        = {Provided by the SAO/NASA Astrophysics Data System},
  editor         = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}

@inproceedings{facct_mckane,
  title          = {{What We Can't Measure, We Can't Understand: Challenges to Demographic Data Procurement in the Pursuit of Fairness}},
  author         = {Andrus, McKane and Spitzer, Elena and Brown, Jeffrey and Xiang, Alice},
  year           = {2021},
  booktitle      = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  location       = {Virtual Event, Canada},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  series         = {FAccT '21},
  pages          = {249–260},
  url            = {https://doi.org/10.1145/3442188.3445888},
  numpages       = {12}
}
@article{Mernik2005,
  title          = {{When and How to Develop Domain-Specific Languages}},
  author         = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
  year           = {2005},
  month          = dec,
  journal        = {ACM Comput. Surv.},
  publisher      = {Association for Computing Machinery},
  address        = {New York, NY, USA},
  volume         = {37},
  number         = {4},
  pages          = {316–344},
  issn           = {0360-0300},
  url            = {https://doi.org/10.1145/1118890.1118892},
  issue_date     = {December 2005},
  numpages       = {29}
}

@misc{hu2021does,
  title          = {{When does loss-based prioritization fail?}},
  author         = {Niel Teng Hu and Xinyu Hu and Rosanne Liu and Sara Hooker and Jason Yosinski},
  year           = {2021}
}

@article{Moravec98whenwill,
  title          = {{When will computer hardware match the human brain}},
  author         = {Hans Moravec},
  year           = {1998},
  journal        = {Journal of Transhumanism},
  volume         = {1}
}
@misc{Dettmers2020,
  title          = {{Which GPU for deep learning?}},
  author         = {Tim Dettmers},
  year           = {2020},
  url            = {https://bit.ly/35qq8xe}
}


@article{rasooli-tetrault-2015,
  title          = {{Yara Parser: A Fast and Accurate Dependency Parser}},
  author         = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  year           = {2015},
  journal        = {Computing Research Repository},
  volume         = {arXiv:1503.06733},
  url            = {http://arxiv.org/abs/1503.06733},
  note           = {version 2}
}
@inproceedings{banon-etal-2020-paracrawl,
  title = {{ParaCrawl: Web-Scale Acquisition of Parallel Corpora}},
  author = {Ba{\~n}{\'o}n, Marta and Chen, Pinzhen and Haddow, Barry and Heafield, Kenneth and Hoang, Hieu and Espl{\`a}-Gomis, Miquel and Forcada, Mikel L. and Kamran, Amir and Kirefu, Faheem and Koehn, Philipp and Ortiz Rojas, Sergio and Pla Sempere, Leopoldo and Ram{\'\i}rez-S{\'a}nchez, Gema and Sarr{\'\i}as, Elsa and Strelec, Marek and Thompson, Brian and Waites, William and Wiggins, Dion and Zaragoza, Jaume},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month = jul,
  year = {2020},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2020.acl-main.417},
  pages = {4555--4567},
}
@inproceedings{card-etal-2020-little,
  title = {{With Little Power Comes Great Responsibility}},
  author = {Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and Jia, Robin and Mahowald, Kyle and Jurafsky, Dan},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month = nov,
  year = {2020},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2020.emnlp-main.745},
  doi = {10.18653/v1/2020.emnlp-main.745},
  pages = {9263--9274},
}
@inproceedings{sogaard-etal-2021-need,
  title = {{We Need To Talk About Random Splits}},
  author = {S{\o}gaard, Anders and Ebert, Sebastian and Bastings, Jasmijn and Filippova, Katja},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month = apr,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.eacl-main.156},
  doi = {10.18653/v1/2021.eacl-main.156},
  pages = {1823--1832},
}
@inproceedings{sennrich-etal-2016-neural,
  title = {{Neural Machine Translation of Rare Words with Subword Units}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month = aug,
  year = {2016},
  address = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/P16-1162},
  doi = {10.18653/v1/P16-1162},
  pages = {1715--1725},
}
@inproceedings{cieri-etal-2016-selection,
  title = {{Selection Criteria for Low Resource Language Programs}},
  author = {Cieri, Christopher and Maxwell, Mike and Strassel, Stephanie and Tracey, Jennifer},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
  month = may,
  year = {2016},
  address = {Portoro{\v{z}}, Slovenia},
  publisher = {European Language Resources Association (ELRA)},
  url = {https://aclanthology.org/L16-1720},
  pages = {4543--4549},
}

@misc{hu2024predictingemergentabilitiesinfinite,
    title={Predicting Emergent Abilities with Infinite Resolution Evaluation}, 
    author={Shengding Hu and Xin Liu and Xu Han and Xinrong Zhang and Chaoqun He and Weilin Zhao and Yankai Lin and Ning Ding and Zebin Ou and Guoyang Zeng and Zhiyuan Liu and Maosong Sun},
    year={2024},
    eprint={2310.03262},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2310.03262}, 
}

@article{kaminski_regulating_2023,
title = {Regulating {the} {Risks} of {AI}},
volume = {103},
url = {https://ssrn.com/abstract=4195066},
doi = {10.2139/ssrn.4195066},
number = {1347},
journal = {Boston University Law Review},
author = {Kaminski, Margot E.},
year = {2023},
note = {U of Colorado Law Legal Studies Research Paper No. 22-21}
}

@article{Schwartz:2020,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green {AI}},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
journal = {Communications of the ACM (CACM)},
month = nov,
pages = {54-63},
numpages = {10}
}


@article{10.1162/tacl_a_00577,
  author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
  title = "{Efficient Methods for Natural Language Processing: A Survey}",
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826-860},
  year = {2023},
  month = {07},
  abstract = "{Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.}",
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00577},
  url = {https://doi.org/10.1162/tacl\_a\_00577},
  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00577/2143614/tacl\_a\_00577.pdf},
}


@article{derczynski2020power,
title={{Power Consumption Variation over Activation Functions}},
author={Derczynski, Leon},
journal={arXiv preprint arXiv:2006.07237v1},
url={https://arxiv.org/abs/2006.07237v1},
year={2020}
}

@inproceedings{Strubell:2019,
  title = "{Energy and Policy Considerations for Deep Learning in {NLP}}",
  author = "Strubell, Emma  and
    Ganesh, Ananya  and
    McCallum, Andrew",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1355",
  doi = "10.18653/v1/P19-1355",
  pages = "3645--3650",
}

@misc{patterson2021carbonemissionslargeneural,
    title={Carbon Emissions and Large Neural Network Training}, 
    author={David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
    year={2021},
    eprint={2104.10350},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2104.10350}, 
}


@inproceedings{
reuel2024position,
title={Position: Technical Research and Talent is Needed for Effective {AI} Governance},
author={Anka Reuel and Lisa Soder and Benjamin Bucknall and Trond Arne Undheim},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=Be2B6f0ps1}
}

@article{salganik_predicting_2023,
author = {Salganik, Matthew J.},
year = {2023},
title = {Predicting the future of society},
journal = {Nature Human Behaviour},
volume = {7},
pages = {478--479},
doi = {10.1038/s41562-023-01535-7},
url = {https://www.nature.com/articles/s41562-023-01535-7},
}

@article{frydman2007,
author = {Frydman, Carola and Molloy, Raven},
year = {2007},
month = {01},
pages = {},
title = {Historical Trends in Executive Compensation, 1936-2003},
volume = {67},
journal = {Journal of Economic History}
}

@misc{qin2024federatedfullparametertuningbillionsized,
    title={Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes}, 
    author={Zhen Qin and Daoyuan Chen and Bingchen Qian and Bolin Ding and Yaliang Li and Shuiguang Deng},
    year={2024},
    eprint={2312.06353},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2312.06353}, 
}

@misc{yuan2023decentralizedtrainingfoundationmodels,
    title={Decentralized Training of Foundation Models in Heterogeneous Environments}, 
    author={Binhang Yuan and Yongjun He and Jared Quincy Davis and Tianyi Zhang and Tri Dao and Beidi Chen and Percy Liang and Christopher Re and Ce Zhang},
    year={2023},
    eprint={2206.01288},
    archivePrefix={arXiv},
    primaryClass={cs.DC},
    url={https://arxiv.org/abs/2206.01288}, 
}

@misc{ho2024algorithmicprogresslanguagemodels,
    title={Algorithmic progress in language models}, 
    author={Anson Ho and Tamay Besiroglu and Ege Erdil and David Owen and Robi Rahman and Zifan Carl Guo and David Atkinson and Neil Thompson and Jaime Sevilla},
    year={2024},
    eprint={2403.05812},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2403.05812}, 
}

@misc{ruis2023goldilockspragmaticunderstandingfinetuning,
    title={The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs}, 
    author={Laura Ruis and Akbir Khan and Stella Biderman and Sara Hooker and Tim Rocktäschel and Edward Grefenstette},
    year={2023},
    eprint={2210.14986},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2210.14986}, 
}

@misc{saparov2023testinggeneraldeductivereasoning,
    title={Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples}, 
    author={Abulhair Saparov and Richard Yuanzhe Pang and Vishakh Padmakumar and Nitish Joshi and Seyed Mehran Kazemi and Najoung Kim and He He},
    year={2023},
    eprint={2305.15269},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2305.15269}, 
}

@misc{bowman2023things,
    title={Eight Things to Know about Large Language Models}, 
    author={Samuel R. Bowman},
    year={2023},
    eprint={2304.00612},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{mitchell2023comparinghumansgpt4gpt4v,
    title={Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks}, 
    author={Melanie Mitchell and Alessandro B. Palmarini and Arseny Moskvichev},
    year={2023},
    eprint={2311.09247},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2311.09247}, 
}

@misc{anwar2024foundational,
    title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models}, 
    author={Usman Anwar and Abulhair Saparov and Javier Rando and Daniel Paleka and Miles Turpin and Peter Hase and Ekdeep Singh Lubana and Erik Jenner and Stephen Casper and Oliver Sourbut and Benjamin L. Edelman and Zhaowei Zhang and Mario Günther and Anton Korinek and Jose Hernandez-Orallo and Lewis Hammond and Eric Bigelow and Alexander Pan and Lauro Langosco and Tomasz Korbak and Heidi Zhang and Ruiqi Zhong and Seán Ó hÉigeartaigh and Gabriel Recchia and Giulio Corsi and Alan Chan and Markus Anderljung and Lilian Edwards and Yoshua Bengio and Danqi Chen and Samuel Albanie and Tegan Maharaj and Jakob Foerster and Florian Tramer and He He and Atoosa Kasirzadeh and Yejin Choi and David Krueger},
    year={2024},
    eprint={2404.09932},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

\usepackage{url}
@misc{christiano2023RLHF,
    title={Deep reinforcement learning from human preferences}, 
    author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
    year={2017},
    eprint={1706.03741},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{khalifa2021distributional,
    title={A Distributional Approach to Controlled Text Generation}, 
    author={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},
    year={2021},
    eprint={2012.11635},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{NEURIPS2022_67496dfa,
author = {Korbak, Tomasz and Elsahar, Hady and Kruszewski, Germ\'{a}n and Dymetman, Marc},
booktitle = {Advances in Neural Information Processing Systems},
editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
pages = {16203--16220},
publisher = {Curran Associates, Inc.},
title = {On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting},
url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf},
volume = {35},
year = {2022}
}

@misc{li2023remax,
    title={ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models}, 
    author={Ziniu Li and Tian Xu and Yushun Zhang and Zhihang Lin and Yang Yu and Ruoyu Sun and Zhi-Quan Luo},
    year={2023},
    eprint={2310.10505},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rafailov2023DPO,
    title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
    year={2023},
    eprint={2305.18290},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{LearningToSummarizeHF,
    title={Learning to summarize from human feedback}, 
    author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
    year={2020},
    eprint={2009.01325},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{askell2021general,
    title={A General Language Assistant as a Laboratory for Alignment}, 
    author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
    year={2021},
    eprint={2112.00861},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{ouyang2022LLMRLHF,
    title={Training language models to follow instructions with human feedback}, 
    author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    year={2022},
    eprint={2203.02155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{williams1992simple,
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
author={Williams, Ronald J.},
journal={Machine Learning},
volume={8},
number={3-4},
pages={229-256},
year={1992},
publisher={Kluwer Academic Publishers}
}
@misc{schulman2017trust,
    title={Trust Region Policy Optimization}, 
    author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
    year={2017},
    eprint={1502.05477},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{tunstall2023zephyr,
    title={Zephyr: Direct Distillation of LM Alignment}, 
    author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
    year={2023},
    eprint={2310.16944},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{nakano2022webgpt,
    title={WebGPT: Browser-assisted question-answering with human feedback}, 
    author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
    year={2022},
    eprint={2112.09332},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{ziegler2020finetuning,
    title={Fine-Tuning Language Models from Human Preferences}, 
    author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
    year={2020},
    eprint={1909.08593},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{zhao2023slichf,
    title={SLiC-HF: Sequence Likelihood Calibration with Human Feedback}, 
    author={Yao Zhao and Rishabh Joshi and Tianqi Liu and Misha Khalman and Mohammad Saleh and Peter J. Liu},
    year={2023},
    eprint={2305.10425},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{dong2023raft,
    title={RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment}, 
    author={Hanze Dong and Wei Xiong and Deepanshu Goyal and Yihan Zhang and Winnie Chow and Rui Pan and Shizhe Diao and Jipeng Zhang and Kashun Shum and Tong Zhang},
    year={2023},
    eprint={2304.06767},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{liu2023statistical,
    title={Statistical Rejection Sampling Improves Preference Optimization}, 
    author={Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J. Liu and Jialu Liu},
    year={2023},
    eprint={2309.06657},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{Radford2018ImprovingLU,
title={Improving Language Understanding by Generative Pre-Training},
author={Alec Radford and Karthik Narasimhan},
year={2018},
url={https://api.semanticscholar.org/CorpusID:49313245}
}

@misc{schulman2017proximal,
    title={Proximal Policy Optimization Algorithms}, 
    author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year={2017},
    eprint={1707.06347},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{sutton2020reinforcement,
title={Reinforcement learning: An introduction},
author={Sutton, Richard S and Barto, Andrew G},
year={2020},
publisher={MIT press}
}


@misc{jaques2019way,
    title={Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog}, 
    author={Natasha Jaques and Asma Ghandeharioun and Judy Hanwen Shen and Craig Ferguson and Agata Lapedriza and Noah Jones and Shixiang Gu and Rosalind Picard},
    year={2019},
    eprint={1907.00456},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{stiennon2022learningRLHF,
    title={Learning to summarize from human feedback}, 
    author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
    year={2022},
    eprint={2009.01325},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{schulman2018GAE,
    title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
    author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
    year={2018},
    eprint={1506.02438},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{schulman2017trust,
    title={Trust Region Policy Optimization}, 
    author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
    year={2017},
    eprint={1502.05477},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{Kool2019Buy4R,
title={Buy 4 REINFORCE Samples, Get a Baseline for Free!},
author={Wouter Kool and Herke van Hoof and Max Welling},
booktitle={DeepRLStructPred@ICLR},
year={2019},
url={https://api.semanticscholar.org/CorpusID:198489118}
}
@misc{biderman2023pythia,
    title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
    author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
    year={2023},
    eprint={2304.01373},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{bai2022AnthropicHH,
    title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
    author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
    year={2022},
    eprint={2204.05862},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{touvron2023llama2,
    title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
    author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
    year={2023},
    eprint={2307.09288},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{touvron2023llama,
    title={LLaMA: Open and Efficient Foundation Language Models}, 
    author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
    year={2023},
    eprint={2302.13971},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{anil2023palm,
    title={PaLM 2 Technical Report}, 
    author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
    year={2023},
    eprint={2305.10403},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{brown2020languageGPT3,
    title={Language Models are Few-Shot Learners}, 
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{openai2023GPT4,
    title={GPT-4 Technical Report}, 
    author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
    year={2023},
    eprint={2303.08774},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{shi2022gradientstein,
    title={Gradient Estimation with Discrete Stein Operators}, 
    author={Jiaxin Shi and Yuhao Zhou and Jessica Hwang and Michalis K. Titsias and Lester Mackey},
    year={2022},
    eprint={2202.09497},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@misc{dimitriev2021carms,
    title={CARMS: Categorical-Antithetic-REINFORCE Multi-Sample Gradient Estimator}, 
    author={Alek Dimitriev and Mingyuan Zhou},
    year={2021},
    eprint={2110.14002},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{titsias2022doublegrad,
    title={Double Control Variates for Gradient Estimation in Discrete Latent Variable Models}, 
    author={Michalis K. Titsias and Jiaxin Shi},
    year={2022},
    eprint={2111.05300},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{kingma2022autoencoding,
    title={Auto-Encoding Variational Bayes}, 
    author={Diederik P Kingma and Max Welling},
    year={2022},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{sun2023exploringLoRAPPO,
    title={Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF}, 
    author={Simeng Sun and Dhawal Gupta and Mohit Iyyer},
    year={2023},
    eprint={2309.09055},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{xue2023parameterefficientRLHF,
    title={Parameter-Efficient Tuning Helps Language Model Alignment}, 
    author={Tianci Xue and Ziqi Wang and Heng Ji},
    year={2023},
    eprint={2310.00819},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lee2023rlaif,
    title={RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback}, 
    author={Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},
    year={2023},
    eprint={2309.00267},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{bai2022constitutional,
    title={Constitutional AI: Harmlessness from AI Feedback}, 
    author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
    year={2022},
    eprint={2212.08073},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lin2024scaling,
  title={Scaling Laws Behind Code Understanding Model},
  author={Jiayi Lin and Hande Dong and Yutao Xie and Lei Zhang},
  year={2024},
  eprint={2402.12813},
  archivePrefix={arXiv},
  primaryClass={cs.SE}
}

@misc{sorscher2023neural,
    title={Beyond neural scaling laws: beating power law scaling via data pruning}, 
    author={Ben Sorscher and Robert Geirhos and Shashank Shekhar and Surya Ganguli and Ari S. Morcos},
    year={2023},
    eprint={2206.14486},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hooker2019compressed,
  title={What Do Compressed Deep Neural Networks Forget?},
  author={Sara Hooker and Aaron Courville and Gregory Clark and Yann Dauphin and Andrea Frome},
  year={2019},
  eprint={1911.05248},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{yuan2023rrhf,
    title={RRHF: Rank Responses to Align Language Models with Human Feedback without tears}, 
    author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
    year={2023},
    eprint={2304.05302},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{azar2023IPO,
    title={A General Theoretical Paradigm to Understand Learning from Human Preferences}, 
    author={Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and Rémi Munos},
    year={2023},
    eprint={2310.12036},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{wang2023reverseDPO,
    title={Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints}, 
    author={Chaoqi Wang and Yibo Jiang and Chenghao Yang and Han Liu and Yuxin Chen},
    year={2023},
    eprint={2309.16240},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Loshchilov2016cosine,
title={SGDR: Stochastic Gradient Descent with Restarts},
author={Ilya Loshchilov and Frank Hutter},
journal={ArXiv},
year={2016},
volume={abs/1608.03983},
url={https://api.semanticscholar.org/CorpusID:15884797}
}

@misc{kendall2017uncertaintiesBNN,
    title={What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}, 
    author={Alex Kendall and Yarin Gal},
    year={2017},
    eprint={1703.04977},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{collier2021correlatedBNN,
    title={Correlated Input-Dependent Label Noise in Large-Scale Image Classification}, 
    author={Mark Collier and Basil Mustafa and Efi Kokiopoulou and Rodolphe Jenatton and Jesse Berent},
    year={2021},
    eprint={2105.10305},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{dubois2024alpacafarm,
    title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
    author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
    year={2024},
    eprint={2305.14387},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{kirk2024understandingRLHF,
    title={Understanding the Effects of RLHF on LLM Generalisation and Diversity}, 
    author={Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu},
    year={2024},
    eprint={2310.06452},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{li-etal-2016-diversity,
  title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
  author = "Li, Jiwei  and
    Galley, Michel  and
    Brockett, Chris  and
    Gao, Jianfeng  and
    Dolan, Bill",
  editor = "Knight, Kevin  and
    Nenkova, Ani  and
    Rambow, Owen",
  booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2016",
  address = "San Diego, California",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N16-1014",
  doi = "10.18653/v1/N16-1014",
  pages = "110--119",
}


@inproceedings{reimers-gurevych-2019-sentence,
  title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
  author = "Reimers, Nils  and
    Gurevych, Iryna",
  editor = "Inui, Kentaro  and
    Jiang, Jing  and
    Ng, Vincent  and
    Wan, Xiaojun",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month = nov,
  year = "2019",
  address = "Hong Kong, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D19-1410",
  doi = "10.18653/v1/D19-1410",
  pages = "3982--3992",
  abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}


@inproceedings{kreutzer-etal-2017-bandit,
  title = "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning",
  author = "Kreutzer, Julia  and
    Sokolov, Artem  and
    Riezler, Stefan",
  editor = "Barzilay, Regina  and
    Kan, Min-Yen",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2017",
  address = "Vancouver, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P17-1138",
  doi = "10.18653/v1/P17-1138",
  pages = "1503--1513",
  abstract = "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",
}


@inproceedings{edunov-etal-2018-classicalseq,
  title = "Classical Structured Prediction Losses for Sequence to Sequence Learning",
  author = "Edunov, Sergey  and
    Ott, Myle  and
    Auli, Michael  and
    Grangier, David  and
    Ranzato, Marc{'}Aurelio",
  editor = "Walker, Marilyn  and
    Ji, Heng  and
    Stent, Amanda",
  booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
  month = jun,
  year = "2018",
  address = "New Orleans, Louisiana",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N18-1033",
  doi = "10.18653/v1/N18-1033",
  pages = "355--364",
  abstract = "There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT{'}14 German-English translation as well as Gigaword abstractive summarization. On the large WMT{'}14 English-French task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.",
}


@misc{rennie2017selfcritical,
    title={Self-critical Sequence Training for Image Captioning}, 
    author={Steven J. Rennie and Etienne Marcheret and Youssef Mroueh and Jarret Ross and Vaibhava Goel},
    year={2017},
    eprint={1612.00563},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rennie2017selfcritical,
    title={Self-critical Sequence Training for Image Captioning}, 
    author={Steven J. Rennie and Etienne Marcheret and Youssef Mroueh and Jarret Ross and Vaibhava Goel},
    year={2017},
    eprint={1612.00563},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{mnih2016A3C,
    title={Asynchronous Methods for Deep Reinforcement Learning}, 
    author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
    year={2016},
    eprint={1602.01783},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{lawrence-riezler-2018-improving,
  title = "Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback",
  author = "Lawrence, Carolin  and
    Riezler, Stefan",
  editor = "Gurevych, Iryna  and
    Miyao, Yusuke",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2018",
  address = "Melbourne, Australia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P18-1169",
  doi = "10.18653/v1/P18-1169",
  pages = "1820--1830",
  abstract = "Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.",
}

@inproceedings{sutton2000policygrad,
author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
booktitle = {Advances in Neural Information Processing Systems},
editor = {S. Solla and T. Leen and K. M\"{u}ller},
pages = {},
publisher = {MIT Press},
title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
volume = {12},
year = {1999}
}

@inproceedings{nguyen-etal-2017-reinforcement,
  title = "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback",
  author = "Nguyen, Khanh  and
    Daum{\'e} III, Hal  and
    Boyd-Graber, Jordan",
  editor = "Palmer, Martha  and
    Hwa, Rebecca  and
    Riedel, Sebastian",
  booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  month = sep,
  year = "2017",
  address = "Copenhagen, Denmark",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D17-1153",
  doi = "10.18653/v1/D17-1153",
  pages = "1464--1474",
  abstract = "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.",
}


@misc{mnih2014neural,
    title={Neural Variational Inference and Learning in Belief Networks}, 
    author={Andriy Mnih and Karol Gregor},
    year={2014},
    eprint={1402.0030},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{engstrom2020implementation,
    title={Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO}, 
    author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
    year={2020},
    eprint={2005.12729},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{choshen2020weaknesses,
    title={On the Weaknesses of Reinforcement Learning for Neural Machine Translation}, 
    author={Leshem Choshen and Lior Fox and Zohar Aizenbud and Omri Abend},
    year={2020},
    eprint={1907.01752},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Kiegeland_2021_revisitingweakness,
 title={Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation},
 url={http://dx.doi.org/10.18653/v1/2021.naacl-main.133},
 DOI={10.18653/v1/2021.naacl-main.133},
 booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 publisher={Association for Computational Linguistics},
 author={Kiegeland, Samuel and Kreutzer, Julia},
 year={2021} }

@misc{shimabucoro2024llmseellmdo,
    title={LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives}, 
    author={Luísa Shimabucoro and Sebastian Ruder and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
    year={2024},
    eprint={2407.01490},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2407.01490}, 
}

@misc{dang2024rlhfspeaklanguagesunlocking,
    title={RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs}, 
    author={John Dang and Arash Ahmadian and Kelly Marchisio and Julia Kreutzer and Ahmet Üstün and Sara Hooker},
    year={2024},
    eprint={2407.02552},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2407.02552}, 
}

@misc{UKGovernment2021,
author = {{UK Government}},
title = {{International scientific report on the safety of advanced AI}},
year = {2021},
url = {https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai},
urldate = {2023-07-04}
}

@inproceedings{wu-etal-2018-study,
  title = "A Study of Reinforcement Learning for Neural Machine Translation",
  author = "Wu, Lijun  and
    Tian, Fei  and
    Qin, Tao  and
    Lai, Jianhuang  and
    Liu, Tie-Yan",
  editor = "Riloff, Ellen  and
    Chiang, David  and
    Hockenmaier, Julia  and
    Tsujii, Jun{'}ichi",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  month = oct # "-" # nov,
  year = "2018",
  address = "Brussels, Belgium",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D18-1397",
  doi = "10.18653/v1/D18-1397",
  pages = "3612--3621",
  abstract = "Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",
}
@inproceedings{kreutzer-etal-2021-offline,
  title = "Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks",
  author = "Kreutzer, Julia  and
    Riezler, Stefan  and
    Lawrence, Carolin",
  editor = "Kozareva, Zornitsa  and
    Ravi, Sujith  and
    Vlachos, Andreas  and
    Agrawal, Priyanka  and
    Martins, Andr{\'e}",
  booktitle = "Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)",
  month = aug,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.spnlp-1.4",
  doi = "10.18653/v1/2021.spnlp-1.4",
  pages = "37--43",
  abstract = "Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.",
}



@misc{workshop2023bloom,
    title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
    author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić et al.},
    year={2023},
    eprint={2211.05100},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{ammanabrolu2020graph,
    title={Graph Constrained Reinforcement Learning for Natural Language Action Spaces}, 
    author={Prithviraj Ammanabrolu and Matthew Hausknecht},
    year={2020},
    eprint={2001.08837},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{martin-etal-2022-learning,
  title = "Learning Natural Language Generation with Truncated Reinforcement Learning",
  author = "Martin, Alice  and
    Quispe, Guillaume  and
    Ollion, Charles  and
    Le Corff, Sylvain  and
    Strub, Florian  and
    Pietquin, Olivier",
  editor = "Carpuat, Marine  and
    de Marneffe, Marie-Catherine  and
    Meza Ruiz, Ivan Vladimir",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jul,
  year = "2022",
  address = "Seattle, United States",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.naacl-main.2",
  doi = "10.18653/v1/2022.naacl-main.2",
  pages = "12--37",
  abstract = "This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original approach to train conditional languagemodels without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pretrained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual question generation tasks, for which we report positive results over performance and language metrics, which we then corroborate with a human evaluation. To our knowledge, it is the first approach that successfully learns a language generation policy without pre-training, using only reinforcement learning.",
}

@inproceedings{ammanabrolu-etal-2022-aligning,
  title = "Aligning to Social Norms and Values in Interactive Narratives",
  author = "Ammanabrolu, Prithviraj  and
    Jiang, Liwei  and
    Sap, Maarten  and
    Hajishirzi, Hannaneh  and
    Choi, Yejin",
  editor = "Carpuat, Marine  and
    de Marneffe, Marie-Catherine  and
    Meza Ruiz, Ivan Vladimir",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jul,
  year = "2022",
  address = "Seattle, United States",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.naacl-main.439",
  doi = "10.18653/v1/2022.naacl-main.439",
  pages = "5994--6017",
  abstract = "We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games{---}environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms{---}causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people{---}in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4{%} while reducing the frequency of socially harmful behaviors by 25{%} compared to strong contemporary value alignment approaches.",
}

@misc{ranzato2015sequence,
    title={Sequence Level Training with Recurrent Neural Networks}, 
    author={Marc'Aurelio Ranzato and Sumit Chopra and Michael Auli and Wojciech Zaremba},
    year={2016},
    eprint={1511.06732},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{bahdanau2017actorcritic,
    title={An Actor-Critic Algorithm for Sequence Prediction}, 
    author={Dzmitry Bahdanau and Philemon Brakel and Kelvin Xu and Anirudh Goyal and Ryan Lowe and Joelle Pineau and Aaron Courville and Yoshua Bengio},
    year={2017},
    eprint={1607.07086},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{kearns-bias,
    title={“Bias-Variance” Error Bounds for Temporal Difference Updates}, 
    author={Michael Kearns and Satinder Singh},
    howpublished={\url{https://www.cis.upenn.edu/~mkearns/papers/tdlambda.pdf}}
}

@misc{nguyen2017reinforcement,
    title={Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback}, 
    author={Khanh Nguyen and Hal Daumé III au2 and Jordan Boyd-Graber},
    year={2017},
    eprint={1707.07402},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{ding2017cold,
title={Cold-start reinforcement learning with softmax policy gradient},
author={Ding, Nan and Soricut, Radu},
journal={Advances in Neural Information Processing Systems},
volume={30},
year={2017}
}

@inproceedings{kreutzer-etal-2018-neural,
  title = "Can Neural Machine Translation be Improved with User Feedback?",
  author = "Kreutzer, Julia  and
    Khadivi, Shahram  and
    Matusov, Evgeny  and
    Riezler, Stefan",
  editor = "Bangalore, Srinivas  and
    Chu-Carroll, Jennifer  and
    Li, Yunyao",
  booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",
  month = jun,
  year = "2018",
  address = "New Orleans - Louisiana",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N18-3012",
  doi = "10.18653/v1/N18-3012",
  pages = "92--105",
  abstract = "We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay e-commerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments{---}five-star ratings of translation quality{---}and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.",
}

@article{gulcehre2023reinforced,
title={Reinforced self-training (rest) for language modeling},
author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
journal={arXiv preprint arXiv:2308.08998},
year={2023}
}


@misc{sutton-TDlearning,
    title={Learning to Predict by the Methods
of Temporal Differences}, 
    author={Richard Sutton},
    howpublished={\url{http://incompleteideas.net/papers/sutton-88-with-erratum.pdf}},
  year={1989}
}

@misc{gao2022scaling,
    title={Scaling Laws for Reward Model Overoptimization}, 
    author={Leo Gao and John Schulman and Jacob Hilton},
    year={2022},
    eprint={2210.10760},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ahmadian2024basics,
    title={Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs}, 
    author={Arash Ahmadian and Chris Cremer and Matthias Gallé and Marzieh Fadaee and Julia Kreutzer and Olivier Pietquin and Ahmet Üstün and Sara Hooker},
    year={2024},
    eprint={2402.14740},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{srivastava2023imitation,
    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
    author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid et al.},
    year={2023},
    eprint={2206.04615},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{besiroglu2024chinchilla,
    title={Chinchilla Scaling: A replication attempt}, 
    author={Tamay Besiroglu and Ege Erdil and Matthew Barnett and Josh You},
    year={2024},
    eprint={2404.10102},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{Terry-McElrath2011,
author = {Terry-McElrath, Yvonne M. and Emery, Sherry and Szczypka, Gery and Johnston, Lloyd D.},
year = {2011},
title = {Potential exposure to anti-drug advertising and drug-related attitudes, beliefs, and behaviors among United States youth, 1995-2006},
journal = {Addictive Behaviors},
volume = {36},
number = {1-2},
pages = {116--124},
publisher = {Elsevier {BV}},
doi = {10.1016/j.addbeh.2010.09.005}
}

@article{liang2023,
author = {Liang, Jinfeng and Huang, Yi and Yin, Li and Sadeghi, Fatemeh and Yang, Yanping and Xiao, Xue and Adami, Hans-Olov and Ye, Weimin and Zhang, Zhe and Fang, Fang},
year = {2023},
month = {05},
pages = {},
title = {Cancer risk following surgical removal of tonsils and adenoids — a population-based, sibling-controlled cohort study in Sweden},
volume = {21},
journal = {BMC Medicine},
doi = {10.1186/s12916-023-02902-x}
}

@inproceedings{joshi-etal-2020-state,
  title = {{The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World}},
  author = {Joshi, Pratik  and Santy, Sebastin  and Budhiraja, Amar  and Bali, Kalika  and Choudhury, Monojit},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month = jul,
  year = {2020},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2020.acl-main.560},
  doi = {10.18653/v1/2020.acl-main.560},
  pages = {6282--6293},
}

@article{shi2022language,
title={Language models are multilingual chain-of-thought reasoners},
author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
journal={arXiv preprint arXiv:2210.03057},
year={2022}
}

@article{cobbe2021training,
title={Training verifiers to solve math word problems},
author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
journal={arXiv preprint arXiv:2110.14168},
year={2021}
}

@misc{ethnologue,
title = {Ethnologue},
howpublished = {\url{https://www.ethnologue.com/insights/how-many-languages/}},
note = {Accessed: 2023-06-17},
year={2023},
}

@misc{denil2014predicting,
    title={Predicting Parameters in Deep Learning}, 
    author={Misha Denil and Babak Shakibi and Laurent Dinh and Marc'Aurelio Ranzato and Nando de Freitas},
    year={2014},
    eprint={1306.0543},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{von2000computer,
title={The Computer and the Brain},
author={Von Neumann, J. and Churchland, P.M. and Churchland, P.S.},
isbn={9780300084733},
lccn={00026937},
series={The Silliman Memorial Lectures Series},
url={https://books.google.com/books?id=Q30MqJjRv1gC},
year={2000},
publisher={Yale University Press}
}

@misc{thebitterlesson2019,
title={The Bitter Lesson},
author={Rich Sutton},
year={2019},
url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
}

@Inbook{Howe1994,
author="Howe, Denis B.
and Asanovi{\'{c}}, Krste",
title="SPACE: Symbolic Processing in Associative Computing Elements",
bookTitle="VLSI for Neural Networks and Artificial Intelligence",
year="1994",
publisher="Springer US",
address="Boston, MA",
pages="243--252",
isbn="978-1-4899-1331-9",
doi="10.1007/978-1-4899-1331-9_24",
url="https://doi.org/10.1007/978-1-4899-1331-9_24"
}


@article{singh_article,
author = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year = {2015},
month = {01},
pages = {Pages 76-85},
title = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume = {Volume 58},
journal = {Communications of the ACM},
doi = {10.1145/2688498.2688499}
}    

@misc{Gallistel2009,
author = {Gallistel, Charles and King, Adam},
year = {2009},
month = {04},
pages = {288-298},
title = {Memory and the Computational Brain: Why Cognitive Science Will Transform Neuroscience},
doi = {10.1002/9781444310498.refs}
}

@misc{MCCLOSKEY1989109,
title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
editor = "Gordon H. Bower",
series = "Psychology of Learning and Motivation",
publisher = "Academic Press",
volume = "24",
pages = "109 - 165",
year = "1989",
issn = "0079-7421",
doi = "https://doi.org/10.1016/S0079-7421(08)60536-8",
author = "Michael McCloskey and Neal J. Cohen",
}

@article{Mcclelland1995,
author = {Mcclelland, James and Mcnaughton, Bruce and O’Reilly, Randall},
year = {1995},
month = {08},
pages = {419-57},
title = {Why There are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
volume = {102},
journal = {Psychological review},
doi = {10.1037/0033-295X.102.3.419}
}


@ARTICLE{Marblestone2016,

AUTHOR={Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},   
 
TITLE={Toward an Integration of Deep Learning and Neuroscience},      

JOURNAL={Frontiers in Computational Neuroscience},      

VOLUME={10},      

PAGES={94},     

YEAR={2016},      
  
URL={https://www.frontiersin.org/article/10.3389/fncom.2016.00094},       

DOI={10.3389/fncom.2016.00094},      

ISSN={1662-5188},   
}


@book{Kuhn1962,
added-at = {2011-09-20T15:32:20.000+0200},
address = {Chicago},
author = {Kuhn, Thomas S.},
biburl = {https://www.bibsonomy.org/bibtex/231d37d5bf096e1cf5e893934336464d3/voj},
interhash = {329d7457ec19e3c93e0737215924fdfc},
intrahash = {31d37d5bf096e1cf5e893934336464d3},
keywords = {paradigm},
publisher = {University of Chicago Press},
timestamp = {2013-01-07T08:31:59.000+0100},
title = {The Structure of Scientific Revolutions},
year = 1962
}

@inproceedings{Lindsey1994,
author = "Lindsey, Clark S. and Lindblad, Thomas",
title = "{Review of hardware neural networks: A User's perspective}",
booktitle = "{3rd Workshop on Neural Networks: From Biology to High-energy Physics}",
reportNumber = "TRITA-FYS-9012",
pages = "0215--224",
month = "9",
year = "1994"
}

@article{MISRA2010239,
title = "Artificial neural networks in hardware: A survey of two decades of progress",
journal = "Neurocomputing",
volume = "74",
number = "1",
pages = "239 - 255",
year = "2010",
note = "Artificial Brains",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2010.03.021",
url = "http://www.sciencedirect.com/science/article/pii/S092523121000216X",
author = "Janardan Misra and Indranil Saha",
}

@ARTICLE{holi210171,
author={J. L. {Holi} and J. -. {Hwang}},
journal={IEEE Transactions on Computers}, 
title={Finite precision error analysis of neural network hardware implementations}, 
year={1993},
volume={42},
number={3},
pages={281-290},}

@article{ciresan2011,
author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year = {2011},
month = {07},
pages = {1237-1242},
title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210}
}

@InProceedings{Elsen_2020_CVPR,
author = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
title = {Fast Sparse ConvNets},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@INPROCEEDINGS{1575717,
author={D. {Steinkraus} and I. {Buck} and P. Y. {Simard}},
booktitle={Eighth International Conference on Document Analysis and Recognition (ICDAR'05)}, 
title={Using GPUs for machine learning algorithms}, 
year={2005},
volume={},
number={},
pages={1115-1120 Vol. 2},}


@inproceedings{10.1145/2555243.2557966,
author = {Olukotun, Kunle},
title = {Beyond Parallel Programming with Domain Specific Languages},
year = {2014},
isbn = {9781450326568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2555243.2557966},
doi = {10.1145/2555243.2557966},
abstract = {Today, almost all computer architectures are parallel and heterogeneous; a combination of multiple CPUs, GPUs and specialized processors. This creates a challenging problem for application developers who want to develop high performance programs without the effort required to use low-level, architecture specific parallel programming models (e.g. OpenMP for CMPs, CUDA for GPUs, MPI for clusters). Domain-specific languages (DSLs) are a promising solution to this problem because they can provide an avenue for high-level application-specific abstractions with implicit parallelism to be mapped directly to low level architecture-specific programming models; providing both high programmer productivity and high execution performance.In this talk I will describe an approach to building high performance DSLs, which is based on DSL embedding in a general purpose programming language, metaprogramming and a DSL infrastructure called Delite. I will describe how we transform DSL programs into efficient first-order low-level code using domain specific optimization, parallelism and locality optimization with parallel patterns, and architecture-specific code generation. All optimizations and transformations are implemented in Delite: an extensible DSL compiler infrastucture that significantly reduces the effort required to develop new DSLs. Delite DSLs for machine learning, data querying, graph analysis, and scientific computing all achieve performance competitive with manually parallelized C++ code.},
booktitle = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {179–180},
numpages = {2},
keywords = {domain specific languages},
location = {Orlando, Florida, USA},
series = {PPoPP '14}
}

@article{Mernik2005,
author = {Mernik, Marjan and Heering, Jan and Sloane, Anthony M.},
title = {When and How to Develop Domain-Specific Languages},
year = {2005},
issue_date = {December 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1118890.1118892},
doi = {10.1145/1118890.1118892},
journal = {ACM Comput. Surv.},
month = dec,
pages = {316–344},
numpages = {29},
keywords = {language development system, domain analysis, application language, Domain-specific language}
}

@INPROCEEDINGS{7459430,
author={G. {Fursin} and A. {Lokhmotov} and E. {Plowman}},
booktitle={2016 Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
title={Collective Knowledge: Towards R D sustainability}, 
year={2016},
volume={},
number={},
pages={864-869},}

@ARTICLE{Lee2011,
author={H. {Lee} and K. {Brown} and A. {Sujeeth} and H. {Chafi} and T. {Rompf} and M. {Odersky} and K. {Olukotun}},
journal={IEEE Micro}, 
title={Implementing Domain-Specific Languages for Heterogeneous Parallel Computing}, 
year={2011},
volume={31},
number={5},
pages={42-53},}

@ARTICLE{Cong2011,
author={J. {Cong} and V. {Sarkar} and G. {Reinman} and A. {Bui}},
journal={IEEE Design   Test of Computers}, 
title={Customizable Domain-Specific Computing}, 
year={2011},
volume={28},
number={2},
pages={6-15},}


@article{Olukotun2014,
author = {Olukotun, Kunle},
title = {Beyond Parallel Programming with Domain Specific Languages},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2692916.2557966},
doi = {10.1145/2692916.2557966},
journal = {SIGPLAN Not.},
month = feb,
pages = {179–180},
numpages = {2},
keywords = {domain specific languages}
}

@book{Hauck2007,
author = {Hauck, Scott and DeHon, Andre},
title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation},
year = {2007},
isbn = {9780080556017},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA}}
}

@article{Florenta2014,
author = {Teodoridis, Florenta},
year = {2014},
month = {01},
pages = {},
title = {Generalists, Specialists, and the Direction of Inventive Activity},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2541383}
}

@techreport{Asanovic2006,
  Author = {Asanović, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A.},
  Title = {The Landscape of Parallel Computing Research: A View from Berkeley},
  Institution = {EECS Department, University of California, Berkeley},
  Year = {2006},
  Month = {Dec},
  URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
  Number = {UCB/EECS-2006-183},
}

@article{CLINTWHALEY20013,
title = "Automated empirical optimizations of software and the ATLAS project",
journal = "Parallel Computing",
volume = "27",
number = "1",
pages = "3 - 35",
year = "2001",
note = "New Trends in High Performance Computing",
issn = "0167-8191",
doi = "https://doi.org/10.1016/S0167-8191(00)00087-9",
url = "http://www.sciencedirect.com/science/article/pii/S0167819100000879",
author = "R. {Clint Whaley} and Antoine Petitet and Jack J. Dongarra",
keywords = "ATLAS, BLAS, Portable performance, AEOS",
}

@ARTICLE{8476161,
author={J. {Dongarra} and M. {Gates} and J. {Kurzak} and P. {Luszczek} and Y. M. {Tsai}},
journal={Proceedings of the IEEE}, 
title={Autotuning Numerical Dense Linear Algebra for Batched Computation With GPU Hardware Accelerators}, 
year={2018},
volume={106},
number={11},
pages={2040-2055},}

@INPROCEEDINGS{Colwell2013,
author={R. {Colwell}},
booktitle={2013 IEEE Hot Chips 25 Symposium (HCS)}, 
title={The chip design game at the end of Moore's law}, 
year={2013},
volume={},
number={},
pages={1-16},}

@misc{HotelSoftwarePF,
title={Software Productivity for Extreme-Scale Science},
author={H. Hotel and H. Johansen and D. Bernholdt and M. H{\'e}roux and R. Hornung},
year={2014}
}

@misc{Xu2010,
author = {Xu, Harry and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Sevitsky, Gary},
year = {2010},
month = {01},
pages = {421-426},
title = {Software bloat analysis: Finding, removing, and preventing performance problems in modern large-scale object-oriented applications},
journal = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
doi = {10.1145/1882362.1882448}
}


@article{larus2008spending,
author = {Larus, James},
title = {Spending Moore's Dividend},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/1506409.1506425},
doi = {10.1145/1506409.1506425},
abstract = {Multicore computers shift the burden of software performance from chip designers and processor architects to software developers.},
journal = {Commun. ACM},
month = may,
pages = {62–69},
numpages = {8}
}

@misc{Thompson2018TheDO,
title={The Decline of Computers As a General Purpose Technology: Why Deep Learning and the End of Moore’s Law are Fragmenting Computing},
author={Neil Thompson and Svenja Spanuth},
year={2018}
}

@INPROCEEDINGS{8192487,
author={R. {Prabhakar} and Y. {Zhang} and D. {Koeplinger} and M. {Feldman} and T. {Zhao} and S. {Hadjis} and A. {Pedram} and C. {Kozyrakis} and K. {Olukotun}},
booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
title={Plasticine: A reconfigurable architecture for parallel patterns}, 
year={2017},
volume={},
number={},
pages={389-402},}

@article{Shalf2020TheFO,
title={The future of computing beyond Moore’s Law},
author={John Shalf},
journal={Philosophical Transactions of the Royal Society A},
year={2020},
volume={378}
}


@ARTICLE{2007legg,
     author = {{Legg}, Shane and {Hutter}, Marcus},
      title = "{A Collection of Definitions of Intelligence}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Artificial Intelligence},
       year = 2007,
      month = jun,
        eid = {arXiv:0706.3639},
      pages = {arXiv:0706.3639},
archivePrefix = {arXiv},
     eprint = {0706.3639},
primaryClass = {cs.AI},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2007arXiv0706.3639L},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article {Leisersoneaam9744,
author = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
title = {There{\textquoteright}s plenty of room at the Top: What will drive computer performance after Moore{\textquoteright}s law?},
volume = {368},
number = {6495},
elocation-id = {eaam9744},
year = {2020},
doi = {10.1126/science.aam9744},
publisher = {American Association for the Advancement of Science},
issn = {0036-8075},
URL = {https://science.sciencemag.org/content/368/6495/eaam9744},
eprint = {https://science.sciencemag.org/content/368/6495/eaam9744.full.pdf},
journal = {Science}
}


@inproceedings{Ansel2014,
author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
title = {OpenTuner: An Extensible Framework for Program Autotuning},
year = {2014},
isbn = {9781450328098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628071.2628092},
doi = {10.1145/2628071.2628092},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages = {303–316},
numpages = {14},
keywords = {optimization, autotuner},
location = {Edmonton, AB, Canada},
series = {PACT '14}
}

@misc{wigger2020,
title={OpenAI launches an API to commercialize its research},
author={Kyle Wiggers},
year={2020},
url = {https://bit.ly/31NAJQB},
}


@misc{2018Amodei,
author = {Amodei,Dario and Hernandez, Danny and Sastry,Girish and Clark, Jack and Brockman, Greg and Sutskever, Ilya},
title = {AI and Compute},
url = {https://openai.com/blog/ai-and-compute/},
year = {2018},
}

@misc{strubell2019energy,
title={Energy and Policy Considerations for Deep Learning in NLP},
author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
year={2019},
eprint={1906.02243},
archivePrefix={arXiv},
primaryClass={cs.CL}
}



@misc{2018vacuum,
author = {Computer History Archives Project},
title = {Computer History 1949 - 1960 Early Vacuum Tube Computers Overview},
url = {https://www.youtube.com/watch?v=WnNm_uJYWhA},
year = {2018},
}

@misc{2019EdgeTpu,
author = {Gupta, Suyog and Tan, Mingxing},
title = {EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML},
url = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html},
year = {2019},
}

@book{Raymond1990,
author = {Kurzweil, Raymond},
title = {The Age of Intelligent Machines},
year = {1990},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
}

@ARTICLE{Moravec98whenwill,
author = {Hans Moravec},
title = {When will computer hardware match the human brain},
journal = {Journal of Transhumanism},
year = {1998},
volume = {1}
}

@book{diamond98,
title={Guns, Germs, and Steel: The Fates of Human Societies},
author={Diamond, J.M. and Diamond, P.G.J. and Bernard Hames Collection},
isbn={9780393317558},
lccn={96037068},
series={National bestseller / W.W. Norton \& Company},
url={https://books.google.com/books?id=1lBu\_bqSsSMC},
year={1999},
publisher={W.W. Norton}
}

@misc{gale2019state,
title={The State of Sparsity in Deep Neural Networks},
author={Trevor Gale and Erich Elsen and Sara Hooker},
year={2019},
eprint={1902.09574},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@InProceedings{malsburg_1986,
author="Van Der Malsburg, C.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="245--248",
abstract="Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.",
isbn="978-3-642-70911-1"
}

@ARTICLE{129422,
author={E. {Sackinger} and B. E. {Boser} and J. {Bromley} and Y. {LeCun} and L. D. {Jackel}},
journal={IEEE Transactions on Neural Networks},
title={Application of the ANNA neural network chip to high-speed character recognition},
year={1992},
volume={3},
number={3},
pages={498-505},}

@article{moore1965,
author = {Moore, Gordon},
journal = {Electronics},
month = {April},
number = {8},
title = {Cramming more components onto integrated circuits},
url = {https://www.cs.utexas.edu/~fussell/courses/cs352h/papers/moore.pdf},
volume = {38},
year = {1965},
}

@misc{NIPS2017_6975,
title = {Dynamic Routing Between Capsules},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {3856--3866},
year = {2017},
url = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},                         }                         }


@ARTICLE{claudiu2010,
author = {{Claudiu Ciresan}, Dan and {Meier}, Ueli and {Gambardella}, Luca Maria and
{Schmidhuber}, Juergen},
title = "{Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition}",
journal = {arXiv e-prints},
keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
year = 2010,
month = mar,
eid = {arXiv:1003.0358},
pages = {arXiv:1003.0358},
archivePrefix = {arXiv},
eprint = {1003.0358},
primaryClass = {cs.NE},
adsurl = {https://ui.adsabs.harvard.edu/abs/2010arXiv1003.0358C},
adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{morgan1983,
author = {Morgan, M. Granger},
title = {The fifth generation: Artificial intelligence and Japan's computer challenge to the world, by Edward A. Feigenbaum and Pamela McCorduck. Reading, MA: Addison-Wesley, 1983, 275 pp. Price: \$15.35},
journal = {Journal of Policy Analysis and Management},
volume = {3},
number = {1},
pages = {156-156},
doi = {10.2307/3324061},
url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3324061},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.2307/3324061},
year = {1983}
}

@book{rumelhart1987,
editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
year = {1986},
isbn = {026268053X},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

@article{spelke2007,
author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
title = {Core knowledge},
journal = {Developmental Science},
volume = {10},
number = {1},
pages = {89-96},
doi = {10.1111/j.1467-7687.2007.00569.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00569.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-7687.2007.00569.x},
abstract = {Abstract Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
year = {2007}
}


@article{Singh2015,
author = {Singh, Dr-Virender and Perdigones, Alicia and Garcia, Jose and Cañas, Ignacio and Mazarrón, Fernando},
year = {2015},
month = {01},
pages = {Pages 76-85},
title = {Analyzing Worldwide Research in Hardware Architecture, 1997-2011},
volume = {Volume 58},
journal = {Communications of the ACM},
doi = {10.1145/2688498.2688499}
}

@INPROCEEDINGS{2014Horowitz, author={M. {Horowitz}},
booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
title={1.1 Computing's energy problem (and what we can do about it)},
year={2014}, volume={}, number={}, pages={10-14},}

@article{2014Mark1,
title = {Grace Hopper, computing pioneer},
journal = {The Harvard Gazette},
year = {2014},
url = {https://news.harvard.edu/gazette/story/2014/12/grace-hopper-computing-pioneer/},
author = {Walter Isaacson},
}

@article{DBLP:journals/corr/abs-2112-00861,
author       = {Amanda Askell and
                Yuntao Bai and
                Anna Chen and
                Dawn Drain and
                Deep Ganguli and
                Tom Henighan and
                Andy Jones and
                Nicholas Joseph and
                Benjamin Mann and
                Nova DasSarma and
                Nelson Elhage and
                Zac Hatfield{-}Dodds and
                Danny Hernandez and
                Jackson Kernion and
                Kamal Ndousse and
                Catherine Olsson and
                Dario Amodei and
                Tom B. Brown and
                Jack Clark and
                Sam McCandlish and
                Chris Olah and
                Jared Kaplan},
title        = {A General Language Assistant as a Laboratory for Alignment},
journal      = {CoRR},
volume       = {abs/2112.00861},
year         = {2021},
url          = {https://arxiv.org/abs/2112.00861},
eprinttype    = {arXiv},
eprint       = {2112.00861},
timestamp    = {Tue, 07 Dec 2021 12:15:54 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-2112-00861.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{LLMsMoreCovertlyRacist,
author = {Karen Hao},
title = {LLMs become more “covertly racist” with human intervention},
year = {2024},
url = {https://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-intervention/},
journal = {MIT Technology Review}
}

@misc{LargeLanguageModels,
author = {Kyle Wiggers},
title = {Large language models exhibit significant Western cultural bias, study finds},
year = {2023},
url = {https://venturebeat.com/ai/large-language-models-exhibit-significant-western-cultural-bias-study-finds/},
journal = {VentureBeat}
}

@misc{nakamura2024auroram,
    title={Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order}, 
    author={Taishi Nakamura and Mayank Mishra and Simone Tedeschi and Yekun Chai and Jason T Stillerman and Felix Friedrich and Prateek Yadav and Tanmay Laud and Vu Minh Chien and Terry Yue Zhuo and Diganta Misra and Ben Bogin and Xuan-Son Vu and Marzena Karpinska and Arnav Varma Dantuluri and Wojciech Kusa and Tommaso Furlanello and Rio Yokota and Niklas Muennighoff and Suhas Pai and Tosin Adewumi and Veronika Laippala and Xiaozhe Yao and Adalberto Junior and Alpay Ariyak and Aleksandr Drozd and Jordan Clive and Kshitij Gupta and Liangyu Chen and Qi Sun and Ken Tsui and Noah Persaud and Nour Fahmy and Tianlong Chen and Mohit Bansal and Nicolo Monti and Tai Dang and Ziyang Luo and Tien-Tung Bui and Roberto Navigli and Virendra Mehta and Matthew Blumberg and Victor May and Huu Nguyen and Sampo Pyysalo},
    year={2024},
    eprint={2404.00399},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{aakanksha2024multilingualalignmentprismaligning,
    title={The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm}, 
    author={Aakanksha and Arash Ahmadian and Beyza Ermis and Seraphina Goldfarb-Tarrant and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
    year={2024},
    eprint={2406.18682},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.18682}, 
}

@article{WINOGRAD1980209,
title = {What does it mean to understand language?},
journal = {Cognitive Science},
volume = {4},
number = {3},
pages = {209-241},
year = {1980},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(80)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0364021380800036},
author = {Terry Winograd}
}

@misc{aipgwg2024,
author = {AI Policy Group at the Institute for Advanced Study},
title = {Response to NTIA RFC on Open Foundation Models with Available Model Weights},
year = {2024},
month = mar,
howpublished = {\url{https://www.ias.edu/sites/default/files/AIPGWG-Response_NTIA-RFC-on-Open-Foundation-AI-w-Available-Model-Weights_Updt_Mar2024.pdf}},
note = {Accessed: 2024-06-30}
}

@misc{eu_ai_act,
author = {{European Union}},
title = {EU Artificial Intelligence Act},
url = {https://artificialintelligenceact.eu/the-act/},
note = {Accessed: 2024-06-30},
year={2024}
}

@misc{gehman2020realtoxicitypromptsevaluatingneuraltoxic,
    title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models}, 
    author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
    year={2020},
    eprint={2009.11462},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2009.11462}, 
}

@misc{ganguli2022red,
    title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned}, 
    author={Deep Ganguli and Liane Lovitt and Jackson Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Ben Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zac Hatfield-Dodds and Tom Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom Brown and Nicholas Joseph and Sam McCandlish and Chris Olah and Jared Kaplan and Jack Clark},
    year={2022},
    eprint={2209.07858},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}



@inproceedings{pozzobon-etal-2023-goodtriever,
  title = "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models",
  author = "Pozzobon, Luiza  and
    Ermis, Beyza  and
    Lewis, Patrick  and
    Hooker, Sara",
  editor = "Bouamor, Houda  and
    Pino, Juan  and
    Bali, Kalika",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-emnlp.339",
  doi = "10.18653/v1/2023.findings-emnlp.339",
  pages = "5108--5125",
  abstract = "Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language{'}s evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43{%} relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild.",
}

@misc{buchanan2021truth,
title={Truth, Lies, and Automation: How Language Models Could Change Disinformation},
author={Buchanan, Ben and Lohn, Andrew and Musser, Micah and Sedova, Katerina},
year={2021},
month={May},
publisher={Center for Security and Emerging Technology},
doi={10.51593/2021CA003}
}

@misc{musser2023cost,
    title={A Cost Analysis of Generative Language Models and Influence Operations}, 
    author={Micah Musser},
    year={2023},
    eprint={2308.03740},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@misc{goldstein2023generative,
    title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations}, 
    author={Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
    year={2023},
    eprint={2301.04246},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@article{zellers2019,
author       = {Rowan Zellers and
                Ari Holtzman and
                Hannah Rashkin and
                Yonatan Bisk and
                Ali Farhadi and
                Franziska Roesner and
                Yejin Choi},
title        = {Defending Against Neural Fake News},
journal      = {CoRR},
volume       = {abs/1905.12616},
year         = {2019},
url          = {http://arxiv.org/abs/1905.12616},
eprinttype    = {arXiv},
eprint       = {1905.12616},
timestamp    = {Sat, 29 Apr 2023 10:09:28 +0200},
biburl       = {https://dblp.org/rec/journals/corr/abs-1905-12616.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ZhouZLPC23,
title = {Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions},
author = {Jiawei Zhou and Yixuan Zhang and Qianni Luo and Andrea G. Parker and Munmun De Choudhury},
year = {2023},
doi = {10.1145/3544548.3581318},
url = {https://doi.org/10.1145/3544548.3581318},
researchr = {https://researchr.org/publication/ZhouZLPC23},
cites = {0},
citedby = {0},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023},
editor = {Albrecht Schmidt 0001 and Kaisa Väänänen and Tesh Goyal and Per Ola Kristensson and Anicia Peters and Stefanie Mueller 0001 and Julie R. Williamson and Max L. Wilson 0001},
publisher = {ACM},
isbn = {978-1-4503-9421-5},
}

@misc{economist_ai_hallucinations,
  title = {Why AI models make stuff up, and how hallucinations can be controlled},
  author = {The Economist},
  url = {https://www.economist.com/science-and-technology/2023/02/28/ai-models-make-stuff-up-how-can-hallucinations-be-controlled},
  year = {2023},
  urldate = {2023-02-28}
}

@misc{european_commission_ai_act,
  title = {Questions and Answers: The EU's new legal framework for Artificial Intelligence},
  author = {European Commission},
  url = {https://ec.europa.eu/commission/presscorner/detail/en/qanda_21_1683},
  year = {2023},
  urldate = {2021-04-21}
}

@misc{stanford_hallucinating_law,
  title = {Hallucinating the Law: Legal Mistakes in Large Language Models Are Pervasive},
  author = {Stanford HAI},
  url = {https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive},
  year = {2023},
  urldate = {Accessed: 2023-10-19}
}

@misc{whitehouse_eo,
  title = {Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence},
url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/},
  author = {{The White House}},
  year = {2023}
}

@misc{1986Rosenblatt,
author={Van Der Malsburg, C},
title={Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
year={1986},
publisher={Springer Berlin Heidelberg},
pages={245--248},
}

@misc{dettmers2023qloraefficientfinetuningquantized,
    title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
    author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
    year={2023},
    eprint={2305.14314},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2305.14314}, 
}

@misc{LeCun1989,
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
title = {Backpropagation Applied to Handwritten Zip Code Recognition},
year = {1989},
publisher = {MIT Press},
volume = {1},
number = {4},
url = {https://doi.org/10.1162/neco.1989.1.4.541},
pages = {541–551},
}

@book{gilder2000telecosm,
title={Telecosm: How Infinite Bandwidth Will Revolutionize Our World},
author={Gilder, G.},
isbn={9780743215947},
url={https://books.google.com/books?id=Kzo-KTxdwcEC},
year={2000},
publisher={Free Press}
}


@article{dwayne2001,
author = {Dwayne Moore},
title = {The Anna Karenina Principle Applied to Ecological Risk Assessments of Multiple Stressors},
journal = {Human and Ecological Risk Assessment: An International Journal},
volume = {7},
number = {2},
pages = {231-237},
year  = {2001},
doi = {10.1080/20018091094349},
}

@misc{Torch2002,
author = {Collobert, Ronan and Bengio, Samy and Marithoz, Johnny},
year = {2002},
month = {11},
title = {Torch: A Modular Machine Learning Software Library}
}

@misc{lush2002,
title = "Technical report: Lush reference manual, code available at http://lush.sourceforge.net",
author = "Yann Lecun and Leon Bottou",
year = "2002",
language = "English (US)",
type = "Other",
}

@book{posselt1888jacquard,
title={The Jacquard Machine Analyzed and Explained: The Preparation of Jacquard Cards and Practical Hints to Learners of Jacquard Designing},
author={Posselt, E.A.},
series={Posselt's textile library},
url={https://books.google.com/books?id=-6FtmgEACAAJ},
year={1888},
publisher={E.A. Posselt}
}


@ARTICLE{1993PASJ,
     author = {{Ito}, Tomoyoshi and {Makino}, Junichiro and {Fukushige}, Toshiyuki and
       {Ebisuzaki}, Toshikazu and {Okumura}, Sachiko K. and
       {Sugimoto}, Daiichiro},
      title = "{A Special-Purpose Computer forN-Body Simulations: GRAPE-2A}",
    journal = {\pasj},
   keywords = {MANY-BODY SYSTEM, SIMULATION, SPECIAL-PURPOSE COMPUTER, STELLAR DYNAMICS},
       year = 1993,
      month = jun,
     volume = {45},
      pages = {339-347},
     adsurl = {https://ui.adsabs.harvard.edu/abs/1993PASJ...45..339I},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{bruce1991,
author = {Collier, Bruce},
title = {Little Engines That Could’ve: The Calculating Machines of Charles Babbage},
year = {1991},
isbn = {0824000439},
publisher = {Garland Publishing, Inc.},
address = {USA}
}


@inproceedings{Fatahalian2004,
author = {Fatahalian, K. and Sugerman, J. and Hanrahan, P.},
title = {Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication},
year = {2004},
isbn = {3905673150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1058129.1058148},
doi = {10.1145/1058129.1058148},
booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
pages = {133–137},
numpages = {5},
location = {Grenoble, France},
series = {HWWS ’04}
}

@article{1963steinbuch,
author={K, Steinbuch and U. Piske},
journal={IEEE Transactions on Electronic Computers},
title={Learning matrices and their applications},
year={1963}, volume={EC-12}, number={6}, pages={846-862},}

@book{warden2019tinyml,
title={TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers},
author={Warden, P. and Situnayake, D.},
isbn={9781492052043},
url={https://books.google.com/books?id=sB3mxQEACAAJ},
year={2019},
publisher={O'Reilly Media, Incorporated}
}

@misc{2019Feldman,
title ={The era of general purpose computers is ending},
year = {2019},
url = {https://bit.ly/3hP8XJh},
author = {Michael Feldman},
}

@book{Hinton1989,
author = {Hinton, Geoffrey E. and Anderson, J. A.},
title = {Parallel Models of Associative Memory},
year = {1989},
isbn = {080580269X},
publisher = {L. Erlbaum Associates Inc.},
address = {USA}
}

@article{Linnainmaa1976TaylorEO,
title={Taylor expansion of the accumulated rounding error},
author={Seppo Linnainmaa},
journal={BIT Numerical Mathematics},
year={1976},
volume={16},
pages={146-160}
}

@inproceedings{Barham2019,
author = {Barham, Paul and Isard, Michael},
title = {Machine Learning Systems Are Stuck in a Rut},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321441},
doi = {10.1145/3317550.3321441},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {177–183},
numpages = {7},
location = {Bertinoro, Italy},
series = {HotOS ’19}
}

@book{tolstoy2016anna,
title={Anna Karenina},
author={Tolstoy, L. and Bartlett, R.},
isbn={9780198748847},
lccn={2015943753},
series={Oxford world's classics},
url={https://books.google.com/books?id=1DooDwAAQBAJ},
year={2016},
publisher={Oxford University Press}
}

@article{FUKUSHIMA1982455,
title = "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position",
journal = "Pattern Recognition",
volume = "15",
number = "6",
pages = "455 - 469",
year = "1982",
issn = "0031-3203",
url = "http://www.sciencedirect.com/science/article/pii/0031320382900243",
author = "Kunihiko Fukushima and Sei Miyake",
}


@article{Jouppi2017,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080246},
doi = {10.1145/3140659.3080246},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {1–12},
numpages = {12},
keywords = {LSTM, CNN, RNN, TPU, DNN, accelerator, neural network, deep learning, MLP, domain-specific architecture, TensorFlow, GPU}
}


@misc{1995thinkingmachines,
title = {The Rise and Fall of Thinking Machines},
author = {Gary Taubes},
year = {1995},
url = {https://www.inc.com/magazine/19950915/2622.html},
}

@inbook{1988rumelhart,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title = {Learning Representations by Back-Propagating Errors},
year = {1988},
publisher = {MIT Press},
booktitle = {Neurocomputing: Foundations of Research},
pages = {696–699},
numpages = {4}
}

@INPROCEEDINGS{7298594,
author={C. {Szegedy} and  {Wei Liu} and  {Yangqing Jia} and P. {Sermanet} and S. {Reed} and D. {Anguelov} and D. {Erhan} and V. {Vanhoucke} and A. {Rabinovich}},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Going deeper with convolutions}, 
year={2015},
volume={},
number={},
pages={1-9},}

@article{Dean202011TD,
title={1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
author={Jeffrey Dean},
journal={2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
year={2020},
pages={8-14}
}

@ARTICLE{2020Mirhoseini,
     author = {{Mirhoseini}, Azalia and {Goldie}, Anna and {Yazgan}, Mustafa and
       {Jiang}, Joe and {Songhori}, Ebrahim and {Wang}, Shen and
       {Lee}, Young-Joon and {Johnson}, Eric and {Pathak}, Omkar and
       {Bae}, Sungmin and {Nazi}, Azade and {Pak}, Jiwoo and {Tong}, Andy and
       {Srinivasa}, Kavya and {Hang}, William and {Tuncer}, Emre and
       {Babu}, Anand and {Le}, Quoc V. and {Laudon}, James and {Ho}, Richard and
       {Carpenter}, Roger and {Dean}, Jeff},
      title = "{Chip Placement with Deep Reinforcement Learning}",
    journal = {arXiv e-prints},
   keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
       year = 2020,
      month = apr,
        eid = {arXiv:2004.10746},
      pages = {arXiv:2004.10746},
archivePrefix = {arXiv},
     eprint = {2004.10746},
primaryClass = {cs.LG},
     adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200410746M},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article {Bi10464,
author = {Bi, Guo-qiang and Poo, Mu-ming},
title = {Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type},
volume = {18},
number = {24},
pages = {10464--10472},
year = {1998},
doi = {10.1523/JNEUROSCI.18-24-10464.1998},
publisher = {Society for Neuroscience},
abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb{\textquoteright}s rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
issn = {0270-6474},
URL = {https://www.jneurosci.org/content/18/24/10464},
eprint = {https://www.jneurosci.org/content/18/24/10464.full.pdf},
journal = {Journal of Neuroscience}
}


@article{Barnett2002WhenAW,
title={When and where do we apply what we learn? A taxonomy for far transfer.},
author={Susan M. Barnett and S. Ceci},
journal={Psychological bulletin},
year={2002},
volume={128 4},
pages={
        612-37
      }
}

@INPROCEEDINGS{Reddi2020,
author={V. J. {Reddi} and C. {Cheng} and D. {Kanter} and P. {Mattson} and G. {Schmuelling} and C. {Wu} and B. {Anderson} and M. {Breughe} and M. {Charlebois} and W. {Chou} and R. {Chukka} and C. {Coleman} and S. {Davis} and P. {Deng} and G. {Diamos} and J. {Duke} and D. {Fick} and J. S. {Gardner} and I. {Hubara} and S. {Idgunji} and T. B. {Jablin} and J. {Jiao} and T. S. {John} and P. {Kanwar} and D. {Lee} and J. {Liao} and A. {Lokhmotov} and F. {Massa} and P. {Meng} and P. {Micikevicius} and C. {Osborne} and G. {Pekhimenko} and A. T. R. {Rajan} and D. {Sequeira} and A. {Sirasao} and F. {Sun} and H. {Tang} and M. {Thomson} and F. {Wei} and E. {Wu} and L. {Xu} and K. {Yamada} and B. {Yu} and G. {Yuan} and A. {Zhong} and P. {Zhang} and Y. {Zhou}},
booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
title={MLPerf Inference Benchmark}, 
year={2020},
volume={},
number={},
pages={446-459},}

@inproceedings{Zhuang2021RandomnessIN,
title={Randomness In Neural Network Training: Characterizing The Impact of Tooling},
author={Donglin Zhuang and Xingyao Zhang and Shuaiwen Leon Song and Sara Hooker},
year={2021}
}

@book{cortada2019ibm,
title={IBM: The Rise and Fall and Reinvention of a Global Icon},
author={Cortada, J.W.},
isbn={9780262039444},
lccn={2018023090},
series={History of Computing},
url={https://books.google.com/books?id=aLeHDwAAQBAJ},
year={2019},
publisher={MIT Press}
}

@inproceedings{Baldwin2017ExplainingTV,
title={Explaining the Vertical-to-Horizontal Transition in the Computer Industry},
author={C. Baldwin},
year={2017}
}

@misc{2020cortexm,
author = {ARM},
title = {Enhancing AI Performance for IoT Endpoint Devices},
url = {https://www.arm.com/company/news/2020/02/new-ai-technology-from-arm},
year = {2020},
}


@ARTICLE{1050511,
author={R. H. {Dennard} and F. H. {Gaensslen} and H. {Yu} and V. L. {Rideout} and E. {Bassous} and A. R. {LeBlanc}},
journal={IEEE Journal of Solid-State Circuits}, 
title={Design of ion-implanted MOSFET's with very small physical dimensions}, 
year={1974},
volume={9},
number={5},
pages={256-268},}

@inbook{2003Gitelman,
title = "How Users Define New Media: A History of the Amusement Phonograph",
author = "Lisa Gitelman",
year = "2003",
language = "English (US)",
editor = "Thorburn, {David } and Jenkins, {Henry }",
booktitle = "Rethinking Media Change",
publisher = "MIT Press",
}

</script> 

<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
